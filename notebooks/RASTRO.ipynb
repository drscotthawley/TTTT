{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7481ae5",
   "metadata": {},
   "source": [
    "# Creating the RASTRO Dataset\n",
    "The new dataset called RASTRO (\"Reduced Accessible Songs for Teaching, Rhythmically Oversimplified\") is a time-quantized version of the MIDI portion of Google Magenta's MAESTRO dataset of piano recordings. \n",
    "It is a list of PyTorch tensors, one tensor for each song. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c197cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -N https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip\n",
    "#!unzip -n -qq maestro-v3.0.0-midi.zip\n",
    "\n",
    "source_dataset = 'maestro'\n",
    "\n",
    "!rm -rf midi_files\n",
    "if source_dataset == 'groove':\n",
    "    !wget -N https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip\n",
    "    !unzip -n -qq groove-v1.0.0-midionly.zip\n",
    "    !ln -s groove midi_files\n",
    "else:\n",
    "    !wget -N https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip\n",
    "    !unzip -n -qq maestro-v3.0.0-midi.zip\n",
    "    !ln -s maestro-v3.0.0 midi_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aeac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from glob import glob\n",
    "import mido \n",
    "import pathlib\n",
    "import pretty_midi\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import multiprocessing as mp\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c9ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('midi_files')\n",
    "filenames = glob(str(data_dir/'**/*.mid*'), recursive=True)\n",
    "print('Number of files:', len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9beaaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# routines for processing midi files\n",
    "def time_convert(time_s, bpm, pm, time_units='ticks'):\n",
    "    if time_units == 'beats':\n",
    "        bps = bpm/60\n",
    "        beats = time_s * bps\n",
    "        #print(\"time_s, bps, beats = \",time_s, bps, beats)\n",
    "        return beats\n",
    "    elif time_units == 'ticks':  # 500000 ticks per beat\n",
    "        return pm.time_to_tick(time_s)\n",
    "    return time_s  # leave it in seconds    \n",
    "\n",
    "def midi_file_to_tensor_old(midi_file,\n",
    "                        time_units='ticks', # beats, ticks, s\n",
    "                        info=False,  # return info about the track\n",
    "                       ):\n",
    "    pm = pretty_midi.PrettyMIDI(midi_file) # read in the whole file. this is incredibly slow\n",
    "    bpm = pm.estimate_tempo()\n",
    "    mid = mido.MidiFile(midi_file)\n",
    "    tpb = mid.ticks_per_beat\n",
    "    #tps = 60000.0 / (bpm * tpb) \n",
    "    spt = mido.tick2second(1, tpb, 500000 )\n",
    "    # Sort the notes first by start time (then by pitch if two notes start at the same time)\n",
    "    sorted_notes = sorted(pm.instruments[0].notes, key=lambda note: (note.start, note.pitch))\n",
    "    notes = torch.empty( (len(sorted_notes), 3), dtype=torch.float32 ) # allocate storage\n",
    "    \n",
    "    prev_start = sorted_notes[0].start\n",
    "    for i, note in enumerate(sorted_notes):\n",
    "        notes[i] = note.pitch\n",
    "        notes[i, 1] = note.start - prev_start  # step, time since last note\n",
    "        notes[i, 2] = note.end - note.start    # duration\n",
    "        prev_start = note.start\n",
    "\n",
    "        notes[i, 1] = time_convert(notes[i, 1], bpm, pm, time_units=time_units)\n",
    "        notes[i, 2] = time_convert(notes[i, 2], bpm, pm, time_units=time_units)\n",
    "\n",
    "    #notes[:,1:] = notes[:,1:]//(tpb/16) # severely quantize in time <-- save for later\n",
    "    if info:\n",
    "        return notes, {'bpm': bpm, 'ticks_per_beat':tpb, 'seconds_per_tick':spt}\n",
    "    else:\n",
    "        return notes\n",
    "\n",
    "def midi_file_to_tensor_ugh(midi_file,\n",
    "                        to_bpm=None, # e.g. 120. None=no change\n",
    "                        divs_per_beat=4, # quantize to 16th-note-time-steps  \n",
    "                        info=False,  # return info about the track\n",
    "                       ):\n",
    "    pm = pretty_midi.PrettyMIDI(midi_file) # read in the whole file. this is incredibly slow\n",
    "    # Sort the notes first by start time (then by pitch if two notes start at the same time)\n",
    "    sorted_notes = sorted(pm.instruments[0].notes, key=lambda note: (note.start, note.pitch))\n",
    "    notes = torch.empty( (len(sorted_notes), 3), dtype=torch.float32 ) # allocate storage\n",
    "\n",
    "    return_info = {}\n",
    "    time_mult=1.0\n",
    "    if to_bpm is not None: \n",
    "        bpm = pm.estimate_tempo()\n",
    "        time_mult = bpm / to_bpm\n",
    "        return_info = {'orig_bpm':bpm, 'time_mult':time_mult}\n",
    "    \n",
    "    prev_start = sorted_notes[0].start\n",
    "    for i, note in enumerate(sorted_notes):\n",
    "        notes[i] = note.pitch\n",
    "        notes[i, 1] = note.start - prev_start  # step, time since last note\n",
    "        notes[i, 2] = note.end - note.start    # duration\n",
    "        prev_start = note.start\n",
    "\n",
    "        if to_bpm is not None:\n",
    "            notes[i, 1:] = notes[i, 1:] * time_mult # rescale timing for tempo change\n",
    "\n",
    "    #notes[:,1:] = notes[:,1:]//(tpb/16) # severely quantize in time <-- save quantization for later\n",
    "    if info:\n",
    "        return notes, return_info\n",
    "    else:\n",
    "        return notes\n",
    "\n",
    "\n",
    "def midi_file_to_tensor(midi_file,\n",
    "                        time_units='ticks', # beats, ticks, s\n",
    "                        info=False,  # return info about the track\n",
    "                       ):\n",
    "    pm = pretty_midi.PrettyMIDI(midi_file) # read in the whole file. this is incredibly slow\n",
    "    bpm = pm.estimate_tempo()\n",
    "    mid = mido.MidiFile(midi_file)\n",
    "    tpb = mid.ticks_per_beat\n",
    "    tps = 60000.0 / (bpm * tpb) \n",
    "    spt = mido.tick2second(1, tpb, 500000 )\n",
    "    # Sort the notes first by start time (then by pitch if two notes start at the same time)\n",
    "    sorted_notes = sorted(pm.instruments[0].notes, key=lambda note: (note.start, note.pitch))\n",
    "    notes = torch.empty( (len(sorted_notes), 3), dtype=torch.float32 ) # allocate storage\n",
    "    \n",
    "    prev_start = sorted_notes[0].start\n",
    "    for i, note in enumerate(sorted_notes):\n",
    "        notes[i] = note.pitch\n",
    "        notes[i, 1] = note.start - prev_start  # step, time since last note\n",
    "        notes[i, 2] = note.end - note.start    # duration\n",
    "        prev_start = note.start\n",
    "\n",
    "        #notes[i, 1] = time_convert(notes[i, 1], bpm, pm, time_units=time_units)\n",
    "        #notes[i, 2] = time_convert(notes[i, 2], bpm, pm, time_units=time_units)\n",
    "\n",
    "    #notes[:,1:] = notes[:,1:]//(tpb/16) # severely quantize in time <-- save for later\n",
    "    if info:\n",
    "        return notes, {'bpm': bpm, 'ticks_per_beat':tpb, 'seconds_per_tick':spt}\n",
    "    else:\n",
    "        return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9675196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes, info = midi_file_to_tensor(filenames[0], info=True)  \n",
    "print(\"info = \",info)\n",
    "pitches = notes[:,0].type(torch.long)  # just the pitch info\n",
    "print(\"notes.shape, pitches.shape =\",notes.shape, pitches.shape)\n",
    "print(\"notes[:,1] min, max = \", notes[:,1].min(), notes[:,1].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b32ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_quantize(notes_tensor,  # a single song\n",
    "                  time_res=0.008, # resolution in seconds.  8ms is from Google \"This Time With Feeling\" paper\n",
    "                  t_max=1.0, # again, from Google paper. This will give us from 0 to 1 second. Anything beyond that gets clipped\n",
    "                 ):\n",
    "    nt2 = notes_tensor.contiguous().clone()\n",
    "    if False:\n",
    "        bucket_vals = torch.arange(0, t_max, time_res)\n",
    "        boundaries = torch.arange(time_res/2, t_max - time_res/2, time_res)\n",
    "        inds = torch.bucketize(nt2[:,1:], boundaries)\n",
    "        nt2[:,1:] = bucket_vals[inds]\n",
    "    else:\n",
    "        nt2[:,1:] = torch.clamp(torch.floor(nt2[:,1:]/time_res)*time_res, 0.0, t_max)\n",
    "    return nt2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994563dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_quant = time_quantize(notes)\n",
    "print(\"notes_quant[:,1] min, max = \", notes_quant[:,1].min(), notes_quant[:,1].max())\n",
    "print(\"notes_quant[:,2] min, max = \", notes_quant[:,2].min(), notes_quant[:,2].max())\n",
    "print(\"Number of unique steps, durations = \", len(notes_quant[:,1].unique()), len(notes_quant[:,2].unique()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69e60c",
   "metadata": {},
   "source": [
    "# Read all files into a list of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28664b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_to_tensor_list(filenames): \n",
    "    tensor_list = process_map(midi_file_to_tensor, filenames, max_workers=mp.cpu_count(), chunksize=1)\n",
    "    return tensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6dd9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_list = files_to_tensor_list(filenames)\n",
    "print(f\"\\n{len(notes_list)} files read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b955ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save that for next time\n",
    "torch.save(notes_list, f'{source_dataset}_tensorlist.pt') # save for next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c19d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notes_list = torch.load('maestro3_tensorlist_120bpm.pt')  # load from previous computation\n",
    "len(notes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a74e711",
   "metadata": {},
   "source": [
    "For easier analysis, put all notes into one big long tensor called \"`all_notes`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b777b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tl_to_notes(tensor_list, shuffle=False, delimit=True):\n",
    "  \"list of tensors (of arbitrary length, for each song) converted to one big long tensor of notes all running togehter\"\n",
    "  if shuffle:random.shuffle(tensor_list)\n",
    "  if delimit:\n",
    "    delimiter = torch.zeros(3)  # use all zeros to show ends of songs\n",
    "    tensor_list = [element for item in tensor_list for element in (item, delimiter)]\n",
    "  return torch.vstack(tensor_list)\n",
    "\n",
    "all_notes = tl_to_notes(notes_list, shuffle=False) # just grab one file, for testing overfitting\n",
    "all_notes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb2bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# routines for displaying midi / notes\n",
    "\n",
    "def notes_arr_to_df(notes_arr) -> pd.DataFrame:\n",
    "    columns = ['pitch','step','duration']\n",
    "    df = pd.DataFrame(notes_arr, columns=columns)\n",
    "    df[\"start\"] = \"\"\n",
    "    df[\"end\"] = \"\"\n",
    "\n",
    "    prev_start = 0\n",
    "    #for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    for i, row in df.iterrows():\n",
    "        start = prev_start + float(row['step'])\n",
    "        df.at[i, 'start'] = start\n",
    "        df.at[i, 'end'] = start + float(row['duration'])\n",
    "        prev_start = start\n",
    "    return df\n",
    "\n",
    "def df_to_midi(\n",
    "        notes_df: pd.DataFrame,\n",
    "        out_file: str = '',  # output file to save to, if any\n",
    "        instrument_name: str = 'Acoustic Grand Piano', # whatever you want to call this instrument\n",
    "        velocity: int = 100,  # note loudness\n",
    "    ) -> pretty_midi.PrettyMIDI:\n",
    "    \"converts a dataframe to valid midi\"\n",
    "\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.Instrument(\n",
    "        program=pretty_midi.instrument_name_to_program(\n",
    "            instrument_name))\n",
    "\n",
    "    prev_start = 0\n",
    "    for i, note in notes_df.iterrows(): # this is a serial operation, not sure how to parallelize\n",
    "        start = float(prev_start + note['step'])\n",
    "        end = float(start + note['duration'])\n",
    "        note = pretty_midi.Note(\n",
    "            velocity=velocity,\n",
    "            pitch=int(note['pitch']),\n",
    "            start=start,\n",
    "            end=end,\n",
    "        )\n",
    "        instrument.notes.append(note)\n",
    "        prev_start = start\n",
    "\n",
    "    pm.instruments.append(instrument)\n",
    "    if out_file: pm.write(out_file)\n",
    "    return pm\n",
    "\n",
    "def plot_piano_roll(notes_df: pd.DataFrame, count: Optional[int] = None, vocab_size=128):\n",
    "    \"produce a piano roll plot\"\n",
    "    if count:\n",
    "        title = f'First {count} notes'\n",
    "    else:\n",
    "        title = f'Whole track'\n",
    "        count = len(notes_df['pitch'])\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plot_pitch = np.stack([notes_df['pitch'], notes_df['pitch']], axis=0)\n",
    "    plot_start_stop = np.stack([notes_df['start'], notes_df['end']], axis=0)\n",
    "    plt.plot(\n",
    "        plot_start_stop[:, :count], plot_pitch[:, :count], color=\"b\", marker=\".\")\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Pitch')\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim([0, vocab_size])\n",
    "    _ = plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def midi_to_audio(pm: pretty_midi.PrettyMIDI, seconds=30, sr=16000):\n",
    "    \"midi to audio, playable in notebook\"\n",
    "    waveform = pm.fluidsynth(fs=float(sr))\n",
    "    # Take a sample of the generated waveform to mitigate kernel resets\n",
    "    try: \n",
    "        waveform_short = waveform[:seconds*sr]\n",
    "    except:\n",
    "        waveform_short = waveform\n",
    "    return display(Audio(waveform_short, rate=sr))\n",
    "\n",
    "def pitches_to_midi(pitch_list, seconds=30):\n",
    "    notes_tensor = torch.zeros((len(pitch_list), 3)) + 0.25\n",
    "    for i, p in enumerate(pitch_list):\n",
    "        notes_tensor[i,0] = p\n",
    "    notes_df = notes_arr_to_df(notes_tensor.cpu().detach().numpy())\n",
    "    midi = df_to_midi(notes_df)\n",
    "    plot_piano_roll(notes_df)\n",
    "    audio_display = midi_to_audio(midi, seconds=seconds)\n",
    "    return audio_display\n",
    "\n",
    "def notes_to_midi(notes_tensor, seconds=30, time_rescale=2/((120/60))):\n",
    "    notes_tensor = notes_tensor.clone() # just to avoid weird overwrites of memory\n",
    "    #notes_tensor = notes_tensor * (notes_tensor>0)  # negative numbers clipped to zero\n",
    "    if notes_tensor.min() < 0.0:\n",
    "      print(\"WARNING: You have negative pitches, steps or durations. Setting them to zero\")\n",
    "      notes_tensor = notes_tensor * (notes_tensor >= 0)\n",
    "    if time_rescale is not None :\n",
    "        notes_tensor[:,1:] = notes_tensor[:,1:] *time_rescale\n",
    "    notes_df = notes_arr_to_df(notes_tensor.cpu().detach().numpy())\n",
    "    midi = df_to_midi(notes_df)\n",
    "    plot_piano_roll(notes_df)\n",
    "    audio_display = midi_to_audio(midi, seconds=seconds)\n",
    "    return audio_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d8b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start, playlen = 920000, 70\n",
    "orig1 = all_notes[start:start+playlen]\n",
    "#notes_to_midi(orig1, time_rescale=info['seconds_per_tick'])\n",
    "notes_to_midi(orig1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e48c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantization\n",
    "grid_resolution = 1/8\n",
    "quant_notes = all_notes.clone()\n",
    "quant_notes[:,1] = torch.round(all_notes[:,1]/grid_resolution)\n",
    "quant_notes[:,2] = torch.ceil(all_notes[:,2]/grid_resolution)  # ceil to avoid zero duration notes\n",
    "\n",
    "\n",
    "\n",
    "print(\"steps: min max, unique = \",quant_notes[:,1].min(), quant_notes[:,1].max(), len(quant_notes[:,1].unique()))\n",
    "print(\"dur: min max, unique =\",quant_notes[:,2].min(), quant_notes[:,2].max(), len(quant_notes[:,2].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312167da",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_rescale = 2/(120/60) * grid_resolution\n",
    "time_rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc74af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notes_to_midi(quant_notes[start:start+playlen], time_rescale=info['seconds_per_tick']*grid_resolution)\n",
    "notes_to_midi(quant_notes[start:start+playlen], time_rescale = 2/(120/60) * grid_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e494dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_vals = quant_notes[:,1].unique().sort()[0]\n",
    "step_cap = step_vals[len(step_vals)//2]\n",
    "step_cap = quant_notes[:,1].unique().median()//2\n",
    "dur_cap = int(quant_notes[:,2].unique().median()/ 2 )\n",
    "step_cap, dur_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf2e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_notes[:,1] = torch.clamp( quant_notes[:,1], 0, step_cap) \n",
    "quant_notes[:,2] = torch.clamp( quant_notes[:,2], 1, dur_cap) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16767a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(quant_notes[:,1].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cce30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notes_to_midi(quant_notes[start:start+playlen], time_rescale=info['seconds_per_tick']*grid_resolution)\n",
    "notes_to_midi(quant_notes[start:start+playlen], time_rescale = 2/(120/60) * grid_resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b5f06a",
   "metadata": {},
   "source": [
    "That looks good.  Let's \"burn it in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8208cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a \"notes list\" based on original notes list\n",
    "quant_notes_list = []\n",
    "for notes in notes_list: \n",
    "    quant_notes = notes.clone()\n",
    "    quant_notes[:,1] = torch.round(notes[:,1]/grid_resolution)\n",
    "    quant_notes[:,2] = torch.ceil(notes[:,2]/grid_resolution)  # ceil to avoid zero duration notes\n",
    "    \n",
    "    quant_notes[:,1] = torch.clamp( quant_notes[:,1], 0, step_cap) \n",
    "    quant_notes[:,2] = torch.clamp( quant_notes[:,2], 0, dur_cap) \n",
    "    quant_notes_list.append(quant_notes.type(torch.torch.int16))\n",
    "len(quant_notes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9585b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(quant_notes_list, 'rastro-120bpm_16th_tensor_list.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b8cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/time_rescale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b1b07",
   "metadata": {},
   "source": [
    "# JS  Bach chorales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a74fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = \"JSB-Chorales-dataset/Jsb16thSeparated.json\"\n",
    "with open(path) as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c789581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(data_dict['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d278d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "song = np.array(data_dict['train'][1])\n",
    "song[0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14933a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def song_to_piano_roll(song):\n",
    "    frames = song.shape[0]\n",
    "    pr = np.zeros((128,frames))\n",
    "    for frame, quad in enumerate(song): \n",
    "        for note in quad: \n",
    "            pr[note,frame] = 64 # velocity? \n",
    "    return pr\n",
    "\n",
    "piano_roll = song_to_piano_roll(song)\n",
    "piano_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3152cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qq librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e934c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi as pm \n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "#import librosa\n",
    "\n",
    "\n",
    "def piano_roll_to_pretty_midi(piano_roll, fs=8*(25/24), program=0):\n",
    "    '''Convert a Piano Roll array into a PrettyMidi object\n",
    "     with a single instrument.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    piano_roll : np.ndarray, shape=(128,frames), dtype=int\n",
    "        Piano roll of one instrument\n",
    "    fs : int\n",
    "        Sampling frequency of the columns, i.e. each column is spaced apart\n",
    "        by ``1./fs`` seconds.\n",
    "    program : int\n",
    "        The program number of the instrument.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    midi_object : pretty_midi.PrettyMIDI\n",
    "        A pretty_midi.PrettyMIDI class instance describing\n",
    "        the piano roll.\n",
    "\n",
    "    '''\n",
    "    notes, frames = piano_roll.shape\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.Instrument(program=program)\n",
    "\n",
    "    # pad 1 column of zeros so we can acknowledge inital and ending events\n",
    "    piano_roll = np.pad(piano_roll, [(0, 0), (1, 1)], 'constant')\n",
    "\n",
    "    # use changes in velocities to find note on / note off events\n",
    "    velocity_changes = np.nonzero(np.diff(piano_roll).T)\n",
    "\n",
    "    # keep track on velocities and note on times\n",
    "    prev_velocities = np.zeros(notes, dtype=int)\n",
    "    note_on_time = np.zeros(notes)\n",
    "\n",
    "    for time, note in zip(*velocity_changes):\n",
    "        # use time + 1 because of padding above\n",
    "        velocity = piano_roll[note, time + 1]\n",
    "        time = time / fs\n",
    "        if velocity > 0:\n",
    "            if prev_velocities[note] == 0:\n",
    "                note_on_time[note] = time\n",
    "                prev_velocities[note] = velocity\n",
    "        else:\n",
    "            pm_note = pretty_midi.Note(\n",
    "                velocity=prev_velocities[note],\n",
    "                pitch=note,\n",
    "                start=note_on_time[note],\n",
    "                end=time)\n",
    "            instrument.notes.append(pm_note)\n",
    "            prev_velocities[note] = 0\n",
    "    pm.instruments.append(instrument)\n",
    "    return pm\n",
    "\n",
    "\n",
    "fs = 8*(25/24)\n",
    "print(\"fs, 1/fs = \",fs, 1/fs)\n",
    "pm2 = piano_roll_to_pretty_midi(piano_roll, fs=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0944832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pm_to_tensor(pm):\n",
    "    # Sort the notes first by start time (then by pitch if two notes start at the same time)\n",
    "    sorted_notes = sorted(pm.instruments[0].notes, key=lambda note: (note.start, note.pitch))\n",
    "    notes = torch.empty( (len(sorted_notes), 3), dtype=torch.float32 ) # allocate storage\n",
    "    \n",
    "    prev_start = sorted_notes[0].start\n",
    "    for i, note in enumerate(sorted_notes):\n",
    "        notes[i] = note.pitch\n",
    "        notes[i, 1] = note.start - prev_start  # step, time since last note\n",
    "        notes[i, 2] = note.end - note.start    # duration\n",
    "        prev_start = note.start\n",
    "    return notes\n",
    "\n",
    "notes2 = pm_to_tensor(pm2)\n",
    "notes2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3054c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7145d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_to_midi(notes2, seconds=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28789f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "16*1/fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_tensor_list = []\n",
    "prev_dur = None\n",
    "total_16ths = 0\n",
    "\n",
    "for sub in ['train','valid','test']:\n",
    "    print(sub)\n",
    "    for i in range(len(data_dict[sub])):\n",
    "        song = np.array(data_dict[sub][i])\n",
    "        print(f\"   i = {i}, len(song) = {len(song)}\")\n",
    "        piano_roll = song_to_piano_roll(song)\n",
    "        pm2 = piano_roll_to_pretty_midi(piano_roll, fs=fs)\n",
    "        # how to set step for first/last note of song? \n",
    "        extra_pitch = 127 # a rest\n",
    "        extra_step = 0 if prev_dur is None else prev_dur\n",
    "        extra_dur = 0.96  # that's what's used elsewhere\n",
    "        extra_note = torch.tensor([extra_pitch, extra_step, extra_dur]).unsqueeze(0)  \n",
    "        notes_tensor = pm_to_tensor(pm2)\n",
    "        notes_tensor[0,1] = extra_dur # first note comes after the initial rest\n",
    "        notes_tensor = torch.cat((extra_note, notes_tensor),dim=0)\n",
    "        notes_tensor_list.append(notes_tensor)\n",
    "        total_16ths = total_16ths + len(song)\n",
    "        prev_dur = notes_tensor[-1,2]\n",
    "print(\"total_16ths,  total_16ths/4 =\",total_16ths, total_16ths/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121a47d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(notes_tensor_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7650a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tl_to_notes(tensor_list, shuffle=False, delimit=False):\n",
    "  \"list of tensors (of arbitrary length, for each song) converted to one big long tensor of notes all running togehter\"\n",
    "  if shuffle:random.shuffle(tensor_list)\n",
    "  if delimit:\n",
    "    delimiter = torch.zeros(3)  # use all zeros to show ends of songs\n",
    "    tensor_list = [element for item in tensor_list for element in (item, delimiter)]\n",
    "  return torch.vstack(tensor_list).type(torch.float32)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_notes = tl_to_notes(notes_tensor_list)\n",
    "all_notes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_notes[:,1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8bb689",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_notes[:,2].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f41d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_notes[:,1:] = torch.clamp(all_notes[:,1:], 0, 5.7600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264abf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_notes, 'jsb_tensor_rests.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c736cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_to_midi(all_notes[2000:5000], seconds=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad550572",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = all_notes[:,0] == 127\n",
    "indices = b.nonzero()\n",
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i= indices[2].item()\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f3290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_notes[i-3:i+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ffb941",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_to_midi(all_notes[i-40:i+40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3409a618",
   "metadata": {},
   "source": [
    "---\n",
    "------------------- my old way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4b4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_repeats(song):\n",
    "    for i in range(len(song)-1,0,-1):\n",
    "        for j in range(4): \n",
    "            if song[i,j] == song[i-1,j]: song[i,j] = 0 \n",
    "    return song\n",
    "    \n",
    "song = zero_repeats(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae64411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "song = song[:,::-1]\n",
    "print(\"len(song) = \",len(song))\n",
    "song[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee584a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.nonzero(np.array([58, 65, 70, 74]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_to_stepdur(song):\n",
    "    notelist = []\n",
    "    for i in range(0,len(song)):\n",
    "        for j in range(4): \n",
    "            if song[i,j] > 0: # we have a new note\n",
    "                # time step\n",
    "                step = 4\n",
    "                if len(np.nonzero(song[i,:j])[0]) >= 1:\n",
    "                    step = 0         \n",
    "                else:  # how many rows back to get any non-zero value? \n",
    "                    prev = np.sum(song[:i,:],axis=-1)[::-1]\n",
    "                    #print(\"i, j, song[i,j], prev = \",i, j, song[i,j], prev) \n",
    "                    try: \n",
    "                        step = 1+np.nonzero(prev)[0][0]\n",
    "                    except:\n",
    "                        step = 0\n",
    "                # to get the duration, count how many zeros are under it \n",
    "                dur = np.nonzero(song[i+1:,j])[0]\n",
    "                try:\n",
    "                    dur = dur[0]\n",
    "                except:\n",
    "                    dur = 0\n",
    "                \n",
    "                notelist.append([song[i,j], step, 1+dur]) \n",
    "        notelist[0][1]=0 # 0 step at start\n",
    "    return notelist\n",
    "    \n",
    "notelist = grid_to_stepdur(song)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbfbc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "notelist = []\n",
    "songs_count = 0 \n",
    "for sub in ['train','valid','test']:\n",
    "    for i in range(len(data_dict[sub])):\n",
    "        songs_count += 1\n",
    "        song = np.array(data_dict[sub][i])\n",
    "        song = zero_repeats(song)\n",
    "        song = song[:,::-1]\n",
    "        notelist = notelist + grid_to_stepdur(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef24c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12566858",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsbnotes = np.array(notelist)\n",
    "print(\"jsbnotes.shape = \",jsbnotes.shape)\n",
    "jsbnotes[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50712509",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsbnotes[:,1:] *= 16\n",
    "jsbnotes[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsbnotes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08bff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''jsbnotes = np.zeros((len(pitchvals),3))\n",
    "jsbnotes[:,0] = pitchvals\n",
    "note_dur = int(0.5/.008)\n",
    "jsbnotes[:,2] = note_dur\n",
    "jsbnotes[4::4,1] = note_dur\n",
    "jsbnotes.shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsbnotes[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464d0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsbtensor = torch.tensor(jsbnotes) \n",
    "jsbtensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsbtensor = torch.clamp(jsbtensor, 0, 4*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e65545",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(jsbtensor, 'jsb_tensor_sd.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c25fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsbtensor[:,2].unique()/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d053ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsbtensor[jsbtensor[:,1]>0][:,1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaedd668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b251bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

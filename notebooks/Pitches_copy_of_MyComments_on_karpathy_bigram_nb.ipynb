{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT5iM-Dq6J1A"
   },
   "source": [
    "Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXLciiRULajm",
    "outputId": "0db3399b-35cd-400b-b743-7f2041340bd2"
   },
   "outputs": [],
   "source": [
    "#!sudo apt install -qq -y fluidsynth  # that's for Linux.  on Mac: \"brew install fluidsynth\"\n",
    "#!pip install -Uqq pyfluidsynth pretty_midi wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtPj8fGF6I2V",
    "outputId": "74d7330e-fbd1-464e-9ad8-00251913dcd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-25 17:33:59--  https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.215.207, 142.250.9.207, 74.125.136.207, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.215.207|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘maestro-v3.0.0-midi.zip’ not modified on server. Omitting download.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_source = \"maestro\"\n",
    "\n",
    "if data_source == \"shakespeare\":\n",
    "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "else:\n",
    "    !wget -N https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip\n",
    "    !unzip -n -qq maestro-v3.0.0-midi.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from glob import glob\n",
    "import pathlib\n",
    "import pretty_midi\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "TDCI1d2qQNOa",
    "outputId": "16c93faf-0185-4a14-bedf-213e01e65a03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdrscotthawley\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "use_wandb = True\n",
    "if use_wandb:\n",
    "  import wandb\n",
    "  wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJCJGzlOKycp",
    "outputId": "7af47cfe-a399-4e56-d1f7-c2c94eabf3ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 1276\n"
     ]
    }
   ],
   "source": [
    "data_dir = pathlib.Path('maestro-v3.0.0')\n",
    "filenames = glob(str(data_dir/'**/*.mid*'))\n",
    "print('Number of files:', len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ex5X1GC6LN5A"
   },
   "outputs": [],
   "source": [
    "vocab_size = 128\n",
    "\n",
    "def midi_file_to_tensor(midi_file):\n",
    "    pm = pretty_midi.PrettyMIDI(midi_file) # read in the whole file. this is incredibly slow\n",
    "\n",
    "    # Sort the notes first by start time (then by pitch if two notes start at the same time)\n",
    "    sorted_notes = sorted(pm.instruments[0].notes, key=lambda note: (note.start, note.pitch))\n",
    "    notes = torch.empty( (len(sorted_notes), 3), dtype=torch.float32 ) # allocate storage\n",
    "\n",
    "    prev_start = sorted_notes[0].start\n",
    "    for i, note in enumerate(sorted_notes):\n",
    "        notes[i] = note.pitch #/ vocab_size  # normalized pitch\n",
    "        notes[i, 1] = note.start - prev_start  # step, i.e. time since last note started\n",
    "        notes[i, 2] = note.end - note.start    # duration\n",
    "        prev_start = note.start\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3xP4JuTOCkH",
    "outputId": "7846f54a-064c-4238-9226-044a92615398"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3816, 3]), torch.Size([3816]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspect one file\n",
    "notes = midi_file_to_tensor(filenames[0])\n",
    "pitches = notes[:,0].type(torch.long)  # just the pitch info\n",
    "notes.shape, pitches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1276"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes_list = torch.load('maestro3_tensor_list.pt')  # load from previous computation\n",
    "#notes_list = torch.load('rastro-120bpm_16th_tensor_list.pt')  # load from previous computation\n",
    "len(notes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7041440, 3]), torch.Size([7041440]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tl_to_notes(tensor_list, shuffle=False, delimit=True):\n",
    "  \"list of tensors (of arbitrary length, for each song) converted to one big long tensor of notes all running togehter\"\n",
    "  if shuffle:random.shuffle(tensor_list)\n",
    "  if delimit:\n",
    "    delimiter = torch.zeros(3)  # use all zeros to show ends of songs\n",
    "    tensor_list = [element for item in tensor_list for element in (item, delimiter)]\n",
    "  return torch.vstack(tensor_list).type(torch.float32)  # return one big tensor of floats\n",
    "\n",
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "all_notes = tl_to_notes(notes_list, shuffle=True) \n",
    "all_pitches = all_notes[:,0].type(torch.long)  # just the pitch info\n",
    "all_notes.shape, all_pitches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef time_quantize(notes_tensor,  # a single song\\n                  time_res=0.008, # resolution in seconds.  8ms is from Google \"This Time With Feeling\" paper\\n                  t_max=1.0, # again, from Google paper. This will give us from 0 to 1 second. Anything beyond that gets clipped\\n                  use_buckets=True,\\n                 ):\\n    nt2 = notes_tensor.contiguous().clone()\\n    if use_buckets:\\n        bucket_vals = torch.arange(0, t_max, time_res)\\n        boundaries = torch.arange(time_res/2, t_max - time_res/2, time_res)\\n        inds = torch.bucketize(nt2[:,1:].contiguous(), boundaries)\\n        #nt2[:,1:] = bucket_vals[inds]\\n        nt2[:,1:] = inds  # time is now in divisions with resolution time_res \\n    else:\\n        nt2[:,1:] = torch.clamp(torch.floor(nt2[:,1:]/time_res)*time_res, 0.0, t_max)\\n    return nt2\\n\\nquant_notes_list = [time_quantize(q) for q in notes_list]\\nquant_notes = time_quantize(all_notes)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def time_quantize(notes_tensor,  # a single song\n",
    "                  time_res=0.008, # resolution in seconds.  8ms is from Google \"This Time With Feeling\" paper\n",
    "                  t_max=1.0, # again, from Google paper. This will give us from 0 to 1 second. Anything beyond that gets clipped\n",
    "                  use_buckets=True,\n",
    "                 ):\n",
    "    nt2 = notes_tensor.contiguous().clone()\n",
    "    if use_buckets:\n",
    "        bucket_vals = torch.arange(0, t_max, time_res)\n",
    "        boundaries = torch.arange(time_res/2, t_max - time_res/2, time_res)\n",
    "        inds = torch.bucketize(nt2[:,1:].contiguous(), boundaries)\n",
    "        #nt2[:,1:] = bucket_vals[inds]\n",
    "        nt2[:,1:] = inds  # time is now in divisions with resolution time_res \n",
    "    else:\n",
    "        nt2[:,1:] = torch.clamp(torch.floor(nt2[:,1:]/time_res)*time_res, 0.0, t_max)\n",
    "    return nt2\n",
    "\n",
    "quant_notes_list = [time_quantize(q) for q in notes_list]\n",
    "quant_notes = time_quantize(all_notes)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "y06bDCiX6R0C"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if data_source == \"shakespeare\":\n",
    "    # hyperparameters\n",
    "    batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "    block_size = 32 # what is the maximum context length for predictions?\n",
    "    max_iters = 5000\n",
    "    eval_interval = 100\n",
    "    learning_rate = 1e-3\n",
    "    eval_iters = 200\n",
    "    n_embd = 64\n",
    "    n_head = 4\n",
    "    n_layer = 4\n",
    "    dropout = 0.1\n",
    "else:\n",
    "    # hyperparameters\n",
    "    batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "    block_size = 256 # what is the maximum context length for predictions?\n",
    "    max_iters = 5000\n",
    "    eval_interval = 100\n",
    "    learning_rate = 1e-3\n",
    "    eval_iters = 200\n",
    "    n_embd = 128 # 64\n",
    "    n_head = 8 # 4\n",
    "    n_layer = 4 * 4\n",
    "    dropout = 0.1\n",
    "\n",
    "\n",
    "config = {\n",
    "  'learning_rate': learning_rate,\n",
    "  'batch_size': batch_size,\n",
    "  'block_size': block_size,\n",
    "  'n_embd': n_embd,\n",
    "  'n_head': n_head,\n",
    "  'n_layer': n_layer,\n",
    "  'dropout': dropout\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QlkM3xo6WhL"
   },
   "source": [
    "Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaAMdat36Vj4",
    "outputId": "26100d42-7ab8-4195-9516-5f1a58e91ee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape, val_data.shape = torch.Size([6337296]) torch.Size([704144])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "#chars = sorted(list(set(text)))\n",
    "#vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "#stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "#itos = { i:ch for i,ch in enumerate(chars) }\n",
    "#encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "#decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "\n",
    "\n",
    "# Train and test splits\n",
    "#data = torch.tensor(encode(text), dtype=torch.long)\n",
    "data = all_pitches\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(\"train_data.shape, val_data.shape =\",train_data.shape, val_data.shape)\n",
    "\n",
    "\n",
    "def augment_data(x,y): # db = datablock - a seqeunce , likely of length 1+block_size \n",
    "    x,y = x.clone(), y.clone()  # avoid in-place alterations of data\n",
    "    pitch_shift = torch.randint(low=-6, high=6, size=(1,1,1), dtype=torch.long).item()\n",
    "    x, y = [torch.clamp(q + pitch_shift, 0, vocab_size-1) for q in [x,y]] \n",
    "    return x,y\n",
    "\n",
    "# data loading\n",
    "def get_batch(split, debug=False):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = augment_data(x,y)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    if debug: print(f\"get_batch: x.shape = {x.shape}, y.shape = {y.shape}\")\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.57 ms ± 4.01 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "x,y = get_batch('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9PfGh_57IY0",
    "outputId": "ad46bdc3-07d6-4dec-fd94-8f472e0cb489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_batch: x.shape = torch.Size([64, 256]), y.shape = torch.Size([64, 256])\n",
      "B, T = 64, 256\n"
     ]
    }
   ],
   "source": [
    "x,y = get_batch('train',debug=True)\n",
    "print(f\"B, T = {batch_size}, {block_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Kr70iKZ7pPd"
   },
   "source": [
    "x is a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBwJTuc17kxe",
    "outputId": "64977d81-5d4d-4ec7-a9e1-b6b49ab7c573"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([68, 67, 58, 75, 59, 68, 74, 67, 58, 75, 74, 59, 68, 67, 75, 58, 44, 54,\n",
       "        92, 60, 93, 92, 91, 89, 87, 61, 52, 85, 87, 85, 83, 42, 82, 52, 80, 58,\n",
       "        79, 80, 78, 76, 75, 73, 59, 75, 51, 47, 73, 71, 70, 68, 40, 66, 47, 64,\n",
       "        56, 63, 61, 59, 58, 49, 46, 55, 56, 52, 51, 49, 47, 46, 44, 27, 43, 40,\n",
       "        39, 37, 34, 20, 32, 27, 44, 59, 51, 56, 52, 61, 55, 59, 51, 56, 49, 64,\n",
       "        55, 47, 56, 63, 44, 56, 40, 28, 55, 39, 46, 58, 56, 27, 88, 55, 49, 51,\n",
       "        46, 87, 85, 83, 82, 80, 79, 56, 46, 51, 49, 76, 75, 73, 71, 70, 68, 59,\n",
       "        51, 67, 64, 63, 58, 62, 61, 51, 56, 59, 44, 43, 53, 61, 59, 42, 59, 64,\n",
       "        56, 63, 54, 52, 68, 58, 66, 59, 51, 43, 53, 59, 31, 49, 40, 61, 59, 42,\n",
       "        54, 30, 58, 92, 52, 54, 49, 90, 88, 87, 85, 83, 82, 59, 54, 52, 49, 80,\n",
       "        78, 76, 75, 73, 71, 63, 52, 54, 70, 68, 66, 61, 65, 64, 63, 47, 59, 64,\n",
       "        54, 66, 68, 70, 71, 76, 75, 77, 73, 59, 71, 70, 52, 68, 49, 67, 64, 63,\n",
       "        56, 61, 52, 60, 61, 42, 63, 64, 65, 58, 66, 68, 52, 66, 47, 63, 64, 59,\n",
       "        66, 68, 51, 70, 71, 44, 76, 75, 59, 73, 71, 52, 70, 68, 66, 49, 64, 63,\n",
       "        56, 61, 52, 60], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WP9U8aIm7qh3"
   },
   "source": [
    "y is x shifted back by one and including new data.\n",
    "in this sense only y[:,-1] is the \"next token\" being predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Wjb4QMS7lub",
    "outputId": "4ad6e99e-9635-47f0-8186-ef0cb56560a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([67, 58, 75, 59, 68, 74, 67, 58, 75, 74, 59, 68, 67, 75, 58, 44, 54, 92,\n",
       "        60, 93, 92, 91, 89, 87, 61, 52, 85, 87, 85, 83, 42, 82, 52, 80, 58, 79,\n",
       "        80, 78, 76, 75, 73, 59, 75, 51, 47, 73, 71, 70, 68, 40, 66, 47, 64, 56,\n",
       "        63, 61, 59, 58, 49, 46, 55, 56, 52, 51, 49, 47, 46, 44, 27, 43, 40, 39,\n",
       "        37, 34, 20, 32, 27, 44, 59, 51, 56, 52, 61, 55, 59, 51, 56, 49, 64, 55,\n",
       "        47, 56, 63, 44, 56, 40, 28, 55, 39, 46, 58, 56, 27, 88, 55, 49, 51, 46,\n",
       "        87, 85, 83, 82, 80, 79, 56, 46, 51, 49, 76, 75, 73, 71, 70, 68, 59, 51,\n",
       "        67, 64, 63, 58, 62, 61, 51, 56, 59, 44, 43, 53, 61, 59, 42, 59, 64, 56,\n",
       "        63, 54, 52, 68, 58, 66, 59, 51, 43, 53, 59, 31, 49, 40, 61, 59, 42, 54,\n",
       "        30, 58, 92, 52, 54, 49, 90, 88, 87, 85, 83, 82, 59, 54, 52, 49, 80, 78,\n",
       "        76, 75, 73, 71, 63, 52, 54, 70, 68, 66, 61, 65, 64, 63, 47, 59, 64, 54,\n",
       "        66, 68, 70, 71, 76, 75, 77, 73, 59, 71, 70, 52, 68, 49, 67, 64, 63, 56,\n",
       "        61, 52, 60, 61, 42, 63, 64, 65, 58, 66, 68, 52, 66, 47, 63, 64, 59, 66,\n",
       "        68, 51, 70, 71, 44, 76, 75, 59, 73, 71, 52, 70, 68, 66, 49, 64, 63, 56,\n",
       "        61, 52, 60, 61], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CO7ZJhu-6dvk"
   },
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zOeDesr4348z"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            #print(\"logits.shape =\",logits.shape,\", targets.shape =\",targets.shape)\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits/temperature, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSUO8GJx6tpw"
   },
   "source": [
    "Instantiate and get ready to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3yYAD0t6tOK",
    "outputId": "ada3d9e7-7ea6-45e8-82a5-5bc1d3217a6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.232128 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(step, model, optimizer,loss, name):\n",
    "    name = name + f'_{step}.pt'\n",
    "    print(\"Saving checkpoint to\",name)\n",
    "    torch.save({\n",
    "            'step': step,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, name)\n",
    "\n",
    "def load_checkpoint(name, model, optimizer):\n",
    "    checkpoint = torch.load(name)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    m = model.to(device)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    step = checkpoint['step']\n",
    "    loss = checkpoint['loss']\n",
    "    return step, m, optimizer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1aKE0Qi6QYFI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shawley/gpt-midi/wandb/run-20231125_173748-5boludp1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/drscotthawley/musicbox_pitches/runs/5boludp1' target=\"_blank\">fine-oath-9</a></strong> to <a href='https://wandb.ai/drscotthawley/musicbox_pitches' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/drscotthawley/musicbox_pitches' target=\"_blank\">https://wandb.ai/drscotthawley/musicbox_pitches</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/drscotthawley/musicbox_pitches/runs/5boludp1' target=\"_blank\">https://wandb.ai/drscotthawley/musicbox_pitches/runs/5boludp1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if use_wandb: wandb.init(project='musicbox_pitches', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mO4XTmlj58La"
   },
   "source": [
    "# Do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "YCWAxp3I57eO",
    "outputId": "539d53d0-0864-4235-ce59-16a4c6b5a26d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.2965, val loss 2.3290\n",
      "step 100: train loss 2.2877, val loss 2.3201\n",
      "step 200: train loss 2.2877, val loss 2.3229\n",
      "step 300: train loss 2.2903, val loss 2.3109\n",
      "step 400: train loss 2.2993, val loss 2.3195\n",
      "step 500: train loss 2.2968, val loss 2.3241\n"
     ]
    }
   ],
   "source": [
    "max_iters = 30000\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if use_wandb: wandb.log(losses)\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint to pitches_only_model_30000.pt\n"
     ]
    }
   ],
   "source": [
    "save_checkpoint(max_iters, model, optimizer,loss, f\"pitches_only_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jw0mtVErQkcH"
   },
   "outputs": [],
   "source": [
    "if use_wandb: wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMVARJ7_58_U"
   },
   "source": [
    "Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TwLRHwwh54ez",
    "outputId": "819e1154-b657-4a37-926d-a5fccfe3f2da"
   },
   "outputs": [],
   "source": [
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "#context = torch.tensor([57, 93, 43, 79, 87, 55, 91, 38, 74], dtype=torch.long, device=device).unsqueeze(0)\n",
    "context = x[0,:5].detach().type(torch.long).to(device=device).unsqueeze(0)\n",
    "print(\"context =\",context)\n",
    "#print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
    "pitch_list = m.generate(context, max_new_tokens=200)[0].tolist()\n",
    "print(\"pitch_list =     \",pitch_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "lUOaORcKTRW4",
    "outputId": "a480e335-d04e-4362-b7bf-8f546e95fa38"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "\n",
    "def notes_arr_to_df(notes_arr) -> pd.DataFrame:\n",
    "    columns = ['pitch','step','duration']\n",
    "    df = pd.DataFrame(notes_arr, columns=columns)\n",
    "    df[\"start\"] = \"\"\n",
    "    df[\"end\"] = \"\"\n",
    "\n",
    "    prev_start = 0\n",
    "    #for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    for i, row in df.iterrows():\n",
    "        start = prev_start + float(row['step'])\n",
    "        df.at[i, 'start'] = start\n",
    "        df.at[i, 'end'] = start + float(row['duration'])\n",
    "        prev_start = start\n",
    "    return df\n",
    "\n",
    "def df_to_midi(\n",
    "        notes_df: pd.DataFrame,\n",
    "        out_file: str = '',  # output file to save to, if any\n",
    "        instrument_name: str = 'Acoustic Grand Piano', # whatever you want to call this instrument\n",
    "        velocity: int = 100,  # note loudness\n",
    "    ) -> pretty_midi.PrettyMIDI:\n",
    "    \"converts a dataframe to valid midi\"\n",
    "\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.Instrument(\n",
    "        program=pretty_midi.instrument_name_to_program(\n",
    "            instrument_name))\n",
    "\n",
    "    prev_start = 0\n",
    "    for i, note in notes_df.iterrows(): # this is a serial operation, not sure how to parallelize\n",
    "        start = float(prev_start + note['step'])\n",
    "        end = float(start + note['duration'])\n",
    "        note = pretty_midi.Note(\n",
    "            velocity=velocity,\n",
    "            pitch=int(note['pitch']),\n",
    "            start=start,\n",
    "            end=end,\n",
    "        )\n",
    "        instrument.notes.append(note)\n",
    "        prev_start = start\n",
    "\n",
    "    pm.instruments.append(instrument)\n",
    "    if out_file: pm.write(out_file)\n",
    "    return pm\n",
    "\n",
    "def plot_piano_roll(notes_df: pd.DataFrame, count: Optional[int] = None):\n",
    "    \"produce a piano roll plot\"\n",
    "    if count:\n",
    "        title = f'First {count} notes'\n",
    "    else:\n",
    "        title = f'Whole track'\n",
    "        count = len(notes_df['pitch'])\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plot_pitch = np.stack([notes_df['pitch'], notes_df['pitch']], axis=0)\n",
    "    plot_start_stop = np.stack([notes_df['start'], notes_df['end']], axis=0)\n",
    "    plt.plot(\n",
    "        plot_start_stop[:, :count], plot_pitch[:, :count], color=\"b\", marker=\".\")\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Pitch')\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim([0, vocab_size])\n",
    "    _ = plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def midi_to_audio(pm: pretty_midi.PrettyMIDI, seconds=30, sr=16000):\n",
    "  \"midi to audio, playable in notebook\"\n",
    "  waveform = pm.fluidsynth(fs=float(sr))\n",
    "  # Take a sample of the generated waveform to mitigate kernel resets\n",
    "  waveform_short = waveform[:seconds*sr]\n",
    "  return display(Audio(waveform_short, rate=sr))\n",
    "\n",
    "def pitches_to_midi(pitch_list, seconds=30):\n",
    "    notes_tensor = torch.zeros((len(pitch_list), 3)) + 0.25\n",
    "    for i, p in enumerate(pitch_list):\n",
    "        notes_tensor[i,0] = p\n",
    "    notes_df = notes_arr_to_df(notes_tensor.cpu().detach().numpy())\n",
    "    midi = df_to_midi(notes_df)\n",
    "    plot_piano_roll(notes_df)\n",
    "    audio_display = midi_to_audio(midi, seconds=seconds)\n",
    "    return audio_display\n",
    "\n",
    "print(\"len(x[0]) =\",len(x[0]))\n",
    "pitches_to_midi(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "K4z_uVprWeSE",
    "outputId": "956cf785-8b5a-4fca-90a1-41c6744ea0bc"
   },
   "outputs": [],
   "source": [
    "for variation in range(4):\n",
    "  pitch_list = m.generate(context, max_new_tokens=200, temperature=2.0)[0].tolist()\n",
    "  pitches_to_midi(pitch_list[:len(x[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "2Jp1OyphlpkI",
    "outputId": "3b0139dc-59af-4717-b763-1b70114c6546"
   },
   "outputs": [],
   "source": [
    "pitch_list = m.generate(context, max_new_tokens=2000)[0].tolist()\n",
    "print(\"len(pitch_list) =\",len(pitch_list))\n",
    "pitches_to_midi(pitch_list, seconds=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v18YWrfdmH01"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

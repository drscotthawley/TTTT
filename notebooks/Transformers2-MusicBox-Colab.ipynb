{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6668d1-f371-4528-bd5c-59d4d4f30d44",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6eace-bb66-42d5-8d9f-b19003e535a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install fluidsynth\n",
    "\n",
    "!pip install -qq gdown pyfluidsynth pretty_midi torch numpy pandas midi-player multiprocess wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e1ec8-f4ce-42a9-aef8-b5945339511c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939ffa5a-109c-439d-8528-941162b075aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import collections\n",
    "import datetime\n",
    "import fluidsynth\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pretty_midi\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from typing import Optional\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import multiprocess as mp   # multiprocess is a Jupyter-compatible fork of multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm  # note: Quarto blog won't print output from tqdm.notebook\n",
    "#from tqdm.contrib.concurrent import process_map  # process_map throws errors on Mac :'-( \n",
    "\n",
    "import random\n",
    "import numpy as np \n",
    "import pprint\n",
    "\n",
    "from collections import defaultdict \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from midi_player import MIDIPlayer\n",
    "from midi_player.stylers import general, dark # I like dark mode\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import wandb \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c76b6-2a7c-4b48-9857-448b75f637bf",
   "metadata": {},
   "source": [
    "## MIDI Dataset Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efa9d8-14a4-4dde-bbeb-c275550d194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the data\n",
    "data_source = 'jsb_chorales_midi'\n",
    "# TODO: add options for MAESTRO, others\n",
    "\n",
    "data_dir = Path('midi_data') # generic name for whatever midi we might want\n",
    "REST_PITCH = 127  # special code  used to denote rests\n",
    "\n",
    "!gdown -O {data_source}.tgz '1MdJiNEgtkvCx9tnyQWcnEE5GPMY6ADUb'\n",
    "!tar zxf {data_source}.tgz; rm -rf midi_data; ln -s {data_source} midi_data ;\n",
    "\n",
    "# get the list of MIDI filenames\n",
    "filenames = sorted(glob(str(data_dir/'**/*.mid*'),recursive=True))\n",
    "print('Number of files:', len(filenames))\n",
    "\n",
    "\n",
    "\n",
    "midi_file = filenames[0]\n",
    "MIDIPlayer(midi_file, 360, styler=dark, title=f\"midi_file = {midi_file}\")\n",
    "\n",
    "\n",
    "def quantize_times(times, \n",
    "                   res_ms=8, # resolution in milliseconds. 8ms was deemed sufficient for MAESTRO.\n",
    "                   clamp_range_s=[0.0,4.0]):\n",
    "    \"reasonably coarse time quantization is really helpful for facilitating learning temporal behavior\"\n",
    "    quant_step = res_ms / 1000.0\n",
    "    q_times = torch.round(times / quant_step) * quant_step\n",
    "    if clamp_range_s is not None: \n",
    "        q_times = torch.clamp(q_times, clamp_range_s[0], clamp_range_s[1])\n",
    "    return q_times\n",
    "\n",
    "\n",
    "def midi_file_to_tensor(filenames, i=None, keep_start_end=False, rest_pitches=[REST_PITCH]):  # filenames could just be a single filename\n",
    "    midi_file = filenames if i is None or type(filenames)==str  else filenames[i]\n",
    "    \"reads a single midi file and converts it to tensor with elements (pitch, step, duration)\"\n",
    "    pm = pretty_midi.PrettyMIDI(midi_file) # read in the whole file. this can be very slow for long MIDI files (e.g. in MAESTRO)\n",
    "\n",
    "    # Sort the notes first by start time (and then by pitch if two notes start at the same time)\n",
    "    sorted_notes = sorted(pm.instruments[0].notes, key=lambda note: (note.start, note.pitch))\n",
    "    \n",
    "    prev_start = sorted_notes[0].start\n",
    "    notes = []\n",
    "    for i, note in enumerate(sorted_notes):\n",
    "        new_note =  torch.empty( (3 + 2*keep_start_end) , dtype=torch.float32)\n",
    "        if int(note.pitch) in rest_pitches: continue  # we can actually delete the rests! \n",
    "        new_note[0] = note.pitch\n",
    "        new_note[1] = note.start - prev_start  # step, i.e. time since start of previous note\n",
    "        new_note[2] = note.end - note.start    # duration\n",
    "        if keep_start_end:                     # might find it useful be able to keep these for easier analysis later\n",
    "            new_note[3] = note.start\n",
    "            new_note[4] = note.end\n",
    "        prev_start = note.start\n",
    "        notes.append(new_note)\n",
    "\n",
    "    notes = torch.vstack(notes)\n",
    "    notes[:,1:] = quantize_times(notes[:,1:])\n",
    "    return notes\n",
    "\n",
    "notes_tensor = midi_file_to_tensor(midi_file)\n",
    "\n",
    "\n",
    "\n",
    "def files_to_tensor_list(filenames, keep_start_end=False, serial=True):\n",
    "    \"Reads MIDI files in parallel so should be reasonably fast. JSB Chorales are no prob but for MAESTRO you want this\"\n",
    "    # tensor_list = process_map(midi_file_to_tensor, filenames, max_workers=mp.cpu_count(), chunksize=1) # Doesn't work on Mac\n",
    "    tensor_list = []\n",
    "    max_ = len(filenames)\n",
    "    if serial:\n",
    "        for i, filename in enumerate(tqdm(filenames)):\n",
    "            tensor_list.append(midi_file_to_tensor(filename,  keep_start_end=keep_start_end))\n",
    "    else:\n",
    "        with mp.Pool(processes=mp.cpu_count()) as p:\n",
    "            with tqdm(total=max_) as pbar:\n",
    "                for r in p.imap_unordered(partial(midi_file_to_tensor, filenames, keep_start_end=keep_start_end), range(0, max_)):\n",
    "                    tensor_list.append(r)\n",
    "                    pbar.update()\n",
    "    return  tensor_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "notes_tensor_list_filename = f'{data_source}_tensor_list.pt'         # we'll save the result to a file for easier re-reading next time\n",
    "read_all_midi_files = True # not os.path.exists(notes_tensor_list_filename) # check to see if it already exists\n",
    "\n",
    "if read_all_midi_files:\n",
    "    notes_tensor_list = files_to_tensor_list(filenames, serial=True)\n",
    "    torch.save(notes_tensor_list, notes_tensor_list_filename) # save for next time\n",
    "else:\n",
    "    notes_tensor_list = torch.load(notes_tensor_list_filename) # just read from the last time we made one\n",
    "    \n",
    "print(f\"\\nlen(notes_tensor_list) = {len(notes_tensor_list)}\")\n",
    "\n",
    "notes_tl = notes_tensor_list\n",
    "all_notes = torch.vstack(notes_tl).type(torch.float32)\n",
    "print(\"all_notes.shape = \",all_notes.shape)\n",
    "\n",
    "\n",
    "\n",
    "def make_codebooks(all_notes, verbose=False):\n",
    "    codebooks = []\n",
    "    n_codebooks = all_notes.shape[-1] # should be 3\n",
    "    for i in range(n_codebooks): \n",
    "        if i==0:  # i=0 means pitch\n",
    "            cb_vals = torch.arange(128)  # just use all possible pitches \n",
    "        else:     # i!=0 means timing\n",
    "            cb_vals = all_notes[:,i].unique().sort()[0] \n",
    "        if verbose: print(f\"\\n---\\ncb {i}: cb_vals = {cb_vals}\")\n",
    "        codebooks.append({'encode':{np.round(k.item(),3): int(v) for v, k in enumerate(cb_vals)}, # codebooks go both ways\n",
    "                          'decode':{int(v): k for v, k in enumerate(cb_vals)}})\n",
    "        if verbose: print(f\" cb {i}: cb keys = {codebooks[-1]['encode'].keys()}\")\n",
    "    return codebooks\n",
    "\n",
    "all_notes[:,1:] = quantize_times(all_notes[:,1:])\n",
    "codebooks = make_codebooks(all_notes)\n",
    "\n",
    "vocab_sizes = [len(cb['encode']) for cb in codebooks]\n",
    "display(HTML(f\"vocab_sizes = {vocab_sizes}\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remap_vals(seq, encdec_str, dtype=torch.long):\n",
    "    out = torch.zeros_like(seq, dtype=dtype)\n",
    "    for cb in range(seq.shape[-1]): \n",
    "        dict_map = codebooks[cb][encdec_str]\n",
    "        default_map = max(codebooks[cb][encdec_str].values())\n",
    "        out[:,cb] = torch.tensor([dict_map.get(np.round(x.item(),3),default_map) for x in seq[:,cb]], dtype=dtype) # that np.round(,3) is good for avoiding KeyErrors\n",
    "        #out[:,cb] = torch.tensor([dict_map[np.round(x.item(),3)] for x in seq[:,cb]], dtype=dtype)\n",
    "    return out \n",
    "\n",
    "# We'll be calling these 'encode' and 'decode functions', which really just pull up the applicable part of the codebooks\n",
    "encode = lambda s: remap_vals(s, 'encode')\n",
    "decode = lambda s: remap_vals(s, 'decode', dtype=all_notes.dtype)\n",
    "\n",
    "\n",
    "# And let's do a little test\n",
    "midi_seq = all_notes[0:6].clone()\n",
    "print(\"Before encoding, midi_seq =\\n\",midi_seq)\n",
    "token_list = encode(midi_seq)\n",
    "print(\"After encoding, token_list =\\n\",token_list)\n",
    "return_seq = decode(token_list)\n",
    "print(\"After decoding, return_seq =\\n\",return_seq)\n",
    "assert torch.equal(midi_seq, return_seq), f\"Oops. midi_seq={midi_seq}, but return_seq={return_seq}. Should be the same\"\n",
    "\n",
    "\n",
    "midi_seq[-1,-1] = 100.0  # give the last time a huge value to check that our mapper won't crash\n",
    "token_list = encode(midi_seq) # if it doesn't crash, we're good\n",
    "assert token_list[-1,-1] == list(codebooks[-1]['encode'].values())[-1], \"Big value should have gotten the last spot in the last codebook\"\n",
    "\n",
    "print(\"Checks pass! :-)\")\n",
    "\n",
    "\n",
    "def get_startend(notes:torch.Tensor) -> torch.Tensor:\n",
    "    \"integrates (step,duration) timing pairs to recover (start,end) info. concats them as new columns\"\n",
    "    newnotes = torch.zeros((len(notes), 5), dtype=notes.dtype, device=notes.device)\n",
    "    newnotes[:,:3] = notes[:,:3]\n",
    "    prev_start = 0.0\n",
    "    for i, note in enumerate(notes): \n",
    "        step, dur = note[1], note[2]\n",
    "        start = step  + prev_start \n",
    "        end   = start + dur\n",
    "        newnotes[i,3], newnotes[i,4] = start, end\n",
    "        prev_start = start   \n",
    "    return newnotes\n",
    "    \n",
    "\n",
    "def notes_to_midi(notes:torch.Tensor, \n",
    "                  time_rescale=None, \n",
    "                  out_file: str = '',\n",
    "                  instrument_name: str = 'Acoustic Grand Piano',\n",
    "                  velocity: int = 64,  # default loudness for all notes\n",
    "                 ) -> pretty_midi.PrettyMIDI:\n",
    "    notes = notes.clone() # just to avoid weird overwrites of memory addresses\n",
    "    if notes.min() < 0.0:\n",
    "      print(\"WARNING: You have negative pitches, steps or durations. Setting them to zero\")\n",
    "      notes = notes * (notes >= 0)\n",
    "    if time_rescale is not None: # just added this because sometime I want to slow/speed up\n",
    "        notes[:,1:] = notes[:,1:] *time_rescale\n",
    "        \n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.Instrument(\n",
    "        program=pretty_midi.instrument_name_to_program(\n",
    "            instrument_name))\n",
    "    \n",
    "    if notes.shape[-1] < 5: notes = get_startend(notes)\n",
    "        \n",
    "    notes = notes.cpu().numpy()\n",
    "    prev_start = 0.0\n",
    "    for note in notes: \n",
    "        pitch, start, end = int(note[0]), note[3], note[4] \n",
    "        midi_note = pretty_midi.Note( velocity=velocity, pitch=pitch, start=start, end=end, )\n",
    "        instrument.notes.append(midi_note)\n",
    "        prev_start = start   \n",
    "        \n",
    "    pm.instruments.append(instrument)\n",
    "    if out_file: pm.write(out_file)\n",
    "    return pm\n",
    "\n",
    "\n",
    "\n",
    "def midiplayer(notes_tensor, height=400, time_rescale=None, midi_file=\"/tmp/tmp.mid\", title='', styler=dark):\n",
    "    \"MIDIplayer that writes input tensor to temporary file\"\n",
    "    pm = notes_to_midi(notes_tensor, time_rescale=time_rescale, out_file=midi_file)\n",
    "    return MIDIPlayer(midi_file, height, styler=dark, dl=True, title=title)\n",
    "\n",
    "midiplayer(decode(encode(midi_file_to_tensor(filenames[0]))), title='Encode-Decode Test')\n",
    "\n",
    "\n",
    "# tensor lists\n",
    "train_filenames = [x for x in filenames if '/train' in x]\n",
    "val_filenames   = [x for x in filenames if '/val'   in x]\n",
    "test_filenames  = [x for x in filenames if '/test'  in x]\n",
    "\n",
    "train_notes_tl = files_to_tensor_list(train_filenames, serial=True)\n",
    "val_notes_tl   = files_to_tensor_list(val_filenames, serial=True)\n",
    "test_notes_tl  = files_to_tensor_list(test_filenames, serial=True)\n",
    "\n",
    "for name, tl in zip(['train','val','test'],[train_notes_tl, val_notes_tl, test_notes_tl]):\n",
    "    stack = torch.vstack(tl)\n",
    "    print(f\"{len(tl)} songs in {name}, {stack.shape[0]} notes\")\n",
    "\n",
    "\n",
    "def augment_data(data, pitch_shift=12, debug=True, extra_augs=False):\n",
    "    datanew = data.clone()                                     # avoid overwriting memory of data\n",
    "    # pitch\n",
    "    change = torch.randint(-pitch_shift, pitch_shift, (1,))    # how many semitones to change all the pitches\n",
    "    datanew[ datanew[:,0] != REST_PITCH ] += torch.tensor((change, 0, 0))  # change the pitches\n",
    "    if not extra_augs: return datanew   #only do pitches\n",
    "\n",
    "    if torch.rand(1) < 0.2:                         # sometimes invert pitches? Probably not useful but anyway\n",
    "        datanew[ datanew[:,0] != REST_PITCH ] *= torch.tensor((-1, 1, 1))\n",
    "        datanew[ datanew[:,0] != REST_PITCH ] += torch.tensor((127, 0, 0))\n",
    "    \n",
    "    # time - if we sometimes increase each non-zero time-token by one, that should be ok, right? \n",
    "    if torch.rand(1) < 0.2: # do step\n",
    "        datanew[ datanew[:,1] > 0]  += torch.tensor((0,1,0))\n",
    "    if torch.rand(1) < 0.2: # do duration\n",
    "        datanew[ datanew[:,2] > 0]  += torch.tensor((0,0,1))\n",
    "        \n",
    "    # extra 'safety' constraint: clamp to range of valid values (of tokens)\n",
    "    for i, cb in enumerate(codebooks):\n",
    "        datanew[:,i] = torch.clamp(datanew[:,i], 0, len(cb['encode'])-1)\n",
    "    return datanew\n",
    "\n",
    "\n",
    "# testing code: \n",
    "torch.manual_seed(3) # setting this just  to make sure something happens ;-) \n",
    "data = torch.tensor([[54,12,6],[61,0,40],[127,14,4],[86,0,12],[126,7,12]])\n",
    "print(\"data.shape = \",data.shape)\n",
    "print(\"original data = \\n\",data)\n",
    "aug = augment_data(data)\n",
    "print(\"augmented data = \\n\",aug) # \n",
    "\n",
    "assert not torch.equal(aug[:,0], data[:,0]), \"Oops, nothing changed\"\n",
    "assert aug[2,0]==data[2,0], \"Oops,  The 127 got changed\"\n",
    "print(\"Checks passed! :-) \")\n",
    "\n",
    "\n",
    "class NotesDataset(Dataset):\n",
    "    \"simple custom dataset of sliding windows\"\n",
    "    def __init__(self, \n",
    "                 tensor_list, \n",
    "                 seq_length:int, \n",
    "                 tokenizer=encode,\n",
    "                 codebooks=codebooks, \n",
    "                 aug_callback=augment_data,\n",
    "                 len_mult=100,  # factor to 'fudge' the dataset length when it's inspected by DataLoaders\n",
    "                 pad=True, # pad end with rests\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.sl = seq_length\n",
    "        self.len_mult = len_mult\n",
    "        self.data_list = [tokenizer(t) for t in tensor_list] # encoded tokens are all we'll use\n",
    "        if pad:\n",
    "            rests = torch.tensor([REST_PITCH,1,1]).unsqueeze(0).tile((seq_length,1))\n",
    "            self.data_list = [torch.cat((toks,rests), dim=0) for toks in self.data_list]\n",
    "        self.aug_callback = aug_callback\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"we're going to be grabbing random windows from the data, so just the len of the tensor \n",
    "        list will be too small for large batch sizes, hence we multiply by len_mult\"\"\"\n",
    "        return len(self.data_list)*self.len_mult  # this will keep the DataLoader going longer\n",
    "\n",
    "    def __getitem__(self, idx, shift=1) -> (torch.Tensor, torch.Tensor):\n",
    "        \"grabs a random 'window' from a random song, with an offset of `shift` tokens between inputs and targets\"\n",
    "        i_song = torch.randint(0, len(self.data_list), (1,)) # pick a song\n",
    "        ix =  torch.randint(0, len(self.data_list[i_song]) - self.sl - 1, (1,))  # start of window within song\n",
    "        data_block = self.data_list[i_song][ix:ix+self.sl+1]  # grab window plus an extra character\n",
    "        if self.aug_callback is not None: \n",
    "            data_block = self.aug_callback(data_block)\n",
    "        inputs, targets = data_block[:self.sl], data_block[shift:self.sl+shift]\n",
    "        return inputs, targets\n",
    "    \n",
    "\n",
    "seq_length = 64\n",
    "train_ds = NotesDataset(train_notes_tl, seq_length)\n",
    "val_ds = NotesDataset(val_notes_tl, seq_length, aug_callback=None, len_mult=1000000) # no aug, neverending\n",
    "# save test_ds for Evaluation section, later\n",
    "len(train_ds), len(val_ds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 128 # We may change this further down, for now it's just a test\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, )\n",
    "\n",
    "batch_x, batch_y = next(iter(train_dl))\n",
    "print(\"batch_x.shape, batch_y.shape = \",batch_x.shape, batch_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e34924-4b5d-49a8-9797-a33579f8d9c1",
   "metadata": {},
   "source": [
    "## Model Architecture Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25407c35-f43c-4e16-b39b-07314e7a2e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class MusicBoxConfig:\n",
    "    # model architecture details\n",
    "    seq_length: int = 64\n",
    "    batch_size: int = 128\n",
    "    n_embd:     int = 128     # embedding dimension to use for tokens & positions\n",
    "    n_heads:    int = 8       # number of attention heads\n",
    "    n_blocks:   int = 4       # number of attention blocks\n",
    "    dropout:  float = 0.1     # dropout value applied everywhere\n",
    "    bias:      bool = False   # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "    # training details\n",
    "    learning_rate: float = 0.001\n",
    "    weight_decay:  float = 0.01   # 0.01 is pytorch default\n",
    "    epochs:          int = 20\n",
    "\n",
    "    # other handy bookkeeping\n",
    "    vocab_sizes: tuple = tuple(vocab_sizes)\n",
    " \n",
    "        \n",
    "config = MusicBoxConfig()\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(config)\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, config):\n",
    "        super().__init__()\n",
    "        n_embd, block_size = config.n_embd, config.seq_length\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size, config):\n",
    "        super().__init__()\n",
    "        n_embd = config.n_embd\n",
    "\n",
    "        self.heads = nn.ModuleList([Head(head_size, config) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "    \n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embd, config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(config.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, config):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, config)\n",
    "        self.ffwd = FeedFoward(n_embd, config)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config, debug=False):\n",
    "        super().__init__()\n",
    "        n_cb = len(config.vocab_sizes)\n",
    "        self.block_size, n_embd, n_head, n_layer = config.seq_length, config.n_embd, config.n_heads, config.n_blocks \n",
    "        \n",
    "        # seperate embeddings for pitch, step & dur part of notes\n",
    "        self.token_embedding_tables = nn.ModuleList([nn.Embedding(vocab_sizes[cbi], n_embd) for cbi in range(n_cb)])\n",
    "\n",
    "        self.position_embedding_table = nn.Embedding(self.block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, config) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_heads = nn.ModuleList([nn.Linear(n_embd, vocab_sizes[cbi]) for cbi in range(n_cb)]) # output token predictors\n",
    "        self.debug=False\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is array of input token indices in the current context\n",
    "        B, T, CBS = idx.shape\n",
    "        \n",
    "        tok_emb = 0\n",
    "        for cb in range(CBS): # just sum codebook reps\n",
    "            tok_emb = tok_emb + self.token_embedding_tables[cb](idx[:,:,cb])\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T,E)\n",
    "        x = tok_emb + pos_emb # sum token embeddings & positional embeddings\n",
    "        \n",
    "        x = self.blocks(x) # Main computation loop! \n",
    "        x = self.ln_f(x)   # final layernorm\n",
    "        logits_list = [head(x) for head in self.lm_heads]  # list of output projections over all codebook values\n",
    "\n",
    "        if targets is None:  # need targets to compute loss\n",
    "            loss = None\n",
    "        else:\n",
    "            lambdas = [0.5]*CBS   # relative \"weights\" to pitch, step, dur parts of loss\n",
    "            loss = 0.0\n",
    "            for cb in range(CBS):        # loop over codebooks  (for pitch, step & dur), summing loss\n",
    "                logits = logits_list[cb]  \n",
    "                B, T, V = logits.shape   # V = vocab size, i.e. codebook length\n",
    "                targ = targets[:,:,cb]   # B, T \n",
    "                logits = logits.view(B*T, V)\n",
    "                targ = targ.reshape(B*T)    # needs reshape & not view b/c of contiguous memory issues.\n",
    "                loss = loss +  lambdas[cb] * F.cross_entropy(logits, targ)\n",
    "        return logits_list, loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        # idx is (B, T, CBS) array of token indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]      # crop idx to the last block_size tokens\n",
    "            logits_list, loss = self(idx_cond)   # get the predictions\n",
    "            idx_next_list = []\n",
    "            for cb in range(idx_cond.shape[-1]):\n",
    "                # focus only on the last time step\n",
    "                logits = logits_list[cb]  # B, T, V  where V = vocab/embedding size \n",
    "                logits = logits[:, -1, :] # get last time.  becomes (B, V)\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits/temperature, dim=-1) # (B, V)\n",
    "                # sample from the distribution\n",
    "                idx_next_list.append(torch.multinomial(probs, num_samples=1)) # (B, 1)\n",
    "                \n",
    "            idx_next = torch.tensor(idx_next_list).unsqueeze(0).unsqueeze(0).to(idx.device)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    \n",
    "# test that, make sure we don't get any errors: \n",
    "model = Transformer(config)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters in the model') \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ccd4da-5c0d-477a-ad43-cfe74a677bc5",
   "metadata": {},
   "source": [
    "## Stuff Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97efbb-c4ef-4403-ab07-5f38e2382d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"device is\",device)\n",
    "\n",
    "\n",
    "demo_prompt_idx = 0    # file index from which to pull the demo prompt\n",
    "demo_target = val_notes_tl[demo_prompt_idx]\n",
    "display(midiplayer(demo_target, title=f\"Full demo 'target', {len(demo_target)} notes in length\"))\n",
    "\n",
    "demo_prompt_length = 16  # number of notes in demo prompt context\n",
    "demo_max_new_tokens = min(150, len(demo_target))  # try to make the whole song, but don't go on too long\n",
    "prompt = demo_target[:demo_prompt_length]\n",
    "display(midiplayer(prompt, title=f\"{demo_prompt_length}-note 'prompt' for demos\"))\n",
    "prompt_tokens = encode(prompt).unsqueeze(0).to(device)\n",
    "\n",
    "#  set RNG seeds for reproducibility.\n",
    "def set_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) \n",
    "        \n",
    "set_seeds(0)\n",
    "\n",
    "train_ds = NotesDataset(train_notes_tl, config.seq_length, len_mult=config.batch_size)\n",
    "val_ds = NotesDataset(val_notes_tl, seq_length, aug_callback=None, len_mult=1000000) # no aug, neverending\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "model = Transformer(config)\n",
    "#model = BigramLanguageModel()\n",
    "model = model.to(device)\n",
    "total_params_str = f'{sum(p.numel() for p in model.parameters())/1e6} M'\n",
    "print(total_params_str,'parameters in the model')  \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "use_wandb = True  # Tip: leave this off at first, until you're sure everything's working!\n",
    "if use_wandb:\n",
    "    \n",
    "    wandb.login()\n",
    "    wandb.init(project=\"musicbox-jsb-tutorial\", config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a072b2-9f44-4f4e-988b-bd026a7ab86f",
   "metadata": {},
   "source": [
    "# Training stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fcd3fa-4462-4a2b-95f9-12db20c49f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_every  = 1 # in steps, evaluate loss on val dataset\n",
    "cp_every   = 60 # in epochs, checkpoint every\n",
    "demo_every = 4 # in epochs, make a midi player demo\n",
    "\n",
    "best_loss = 999\n",
    "\n",
    "ema_weight, loss_ema, val_loss_ema = 0.95, None , None # exponential moving averages for loss reporting\n",
    "step = 0 \n",
    "losses = {}\n",
    "epochs = config.epochs\n",
    "for epoch in range(1,epochs+1):\n",
    "    pbar = tqdm(total=len(train_dl), desc=f\"Epoch {epoch}/{config.epochs}\", dynamic_ncols=False, ncols=100) # progress bar, per epoch\n",
    "    for bi, batch in enumerate(train_dl):\n",
    "        step += 1\n",
    "        xb, yb = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        logits, loss = model(xb, yb) # evaluate the loss\n",
    "        loss_ema = (1.0-ema_weight)*loss.item() + ema_weight*loss_ema if loss_ema is not None else loss.item()\n",
    "        losses['train'], losses['train_ema'] = loss.item(), loss_ema\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # status / diagnostics:\n",
    "        if (step % val_every == 0):\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                xvb, yvb = next(iter(val_dl))\n",
    "                val_logits, val_loss = model( xvb.to(device), yvb.to(device) ) \n",
    "                val_loss_ema =  (1.0-ema_weight)*val_loss.item() + ema_weight*val_loss_ema if val_loss_ema is not None else val_loss.item() \n",
    "                losses['val'], losses['val_ema'] = val_loss.item(), val_loss_ema\n",
    "            model.train()\n",
    "            \n",
    "        wbl_dict = {'step':step, 'epoch':epoch} | losses   # dict for logging losses, midi examples, etc to wandb\n",
    "        pbar.set_postfix( dict((k,losses[k]) for k in ['train', 'val','val_ema'])) # loss info for progress bar\n",
    "        pbar.update(1)\n",
    "        if use_wandb and wbl_dict != {}: wandb.log(wbl_dict)\n",
    "            \n",
    "    #--- end of epoch ---\n",
    "\n",
    "    if losses['val_ema'] < best_loss: # Tracking best val_loss_ema for checkpointing purposes\n",
    "        best_loss = losses['val_ema']\n",
    "        pbar.set_postfix(dict( (k,losses[k]) for k in ['train', 'val']) | {'BEST val_ema':best_loss})\n",
    "        if (epoch) % cp_every==0:   # occasionally save a checkpoint of best model/optimizer states\n",
    "            cp_file = f\"musicbox-jsb\" #    -{step}\" # let's leave out step to avoid filling disk\n",
    "            print(f\"Saving a checkpoint to {cp_file}\")\n",
    "            torch.save({ 'step': step, 'model_state_dict': model.state_dict(), 'loss': loss,\n",
    "                         'optimizer_state_dict': optimizer.state_dict(),}, cp_file)\n",
    "    pbar.refresh()\n",
    "    pbar.close()\n",
    "    \n",
    "    if (epoch % demo_every == 0) or (epoch==epochs):  # demo of midi generation\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            new_notes = decode( model.generate(prompt_tokens, max_new_tokens=demo_max_new_tokens, temperature=1)[0].cpu() )\n",
    "            p2 = midiplayer(new_notes,title=f\"Demo on val dataset, Epoch={epoch}\")\n",
    "            display(p2)\n",
    "            if use_wandb: wandb.log( {'step':step, 'player':wandb.Html(p2.html)} )              \n",
    "        model.train()\n",
    "\n",
    "\n",
    "if use_wandb: wandb.finish()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21425f9-75b4-4976-a9c5-c1bd1c0db970",
   "metadata": {},
   "source": [
    "## Evaluation Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd51564d-7095-44f0-bbad-ab27c288f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "file_ind = 1  \n",
    "original = test_notes_tl[file_ind]\n",
    "num_tokens = len(original)\n",
    "display(midiplayer(original, title=f\"Full Evaluation Target, {num_tokens} notes long\"))\n",
    "\n",
    "\n",
    "prompt_len = 21\n",
    "prompt = original[:prompt_len] \n",
    "display(midiplayer(prompt, title=f\"Evaluation Prompt, {prompt_len} notes long\"))\n",
    "prompt_tokens = encode(prompt).unsqueeze(0).to(device)\n",
    "\n",
    "new_tokens = num_tokens - prompt_len\n",
    "\n",
    "for temperature in [ 0.7, 0.85, 0.92, 1.0, 1.2, 1.5]:\n",
    "    set_seeds(1337) # same temp for same seed will yield same output\n",
    "    notes = decode( model.generate(prompt_tokens, max_new_tokens=new_tokens, temperature=temperature)[0].cpu() )\n",
    "    display(midiplayer(notes, title=f\"Temperature = {temperature}\"))\n",
    "\n",
    "\n",
    "test_ds = NotesDataset(test_notes_tl, config.seq_length, len_mult=config.batch_size, pad=False)\n",
    "test_dl = DataLoader(train_ds, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "total, batches = 0, 0 \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(tqdm(test_dl, ncols=100)): \n",
    "        batches +=1 \n",
    "        xb, yb = [q.to(device) for q in batch]\n",
    "        logits, loss = model(xb, yb)\n",
    "        total = total + loss.cpu().item()\n",
    "perplexity = np.exp(total/batches) \n",
    "print(f\"(Average) Perplexity = {perplexity:5.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "all_test_notes = torch.vstack(test_notes_tl).type(torch.float32)\n",
    "all_test_notes = all_test_notes[:len(all_test_notes)//4]  # to save time, let's not do stats on ALL-all of them, just a quarter\n",
    "\n",
    "# generate a big batch of fake data. \n",
    "# WARNING: this is super slow because our generator only works for batches of one, oops\n",
    "prompt_len, max_new_tokens = 12, 54          # I just made these numbers up. not great science yet, sorry.\n",
    "new_notes_list = []\n",
    "print(\"Generating new notes...\") \n",
    "for i, notes in enumerate(tqdm(test_notes_tl, ncols=100)): \n",
    "    ptoks = encode(notes[:prompt_len]).unsqueeze(0).to(device)\n",
    "    # generate more notes, chop off prompt before adding to list of all generated notes\n",
    "    new_notes_list.append( decode( model.generate(ptoks, max_new_tokens=max_new_tokens)[0].cpu()[prompt_len:] ) )\n",
    "all_new_notes = torch.vstack(new_notes_list).type(torch.float32)\n",
    "\n",
    "print(f\"# of test notes = {len(all_test_notes)},  # of generated notes = {len(all_new_notes)} --- Close enough!\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "names = ['pitch', 'step', 'duration']\n",
    "for i in range(3):\n",
    "    bins = 32 if i !=0 else 0.5 + 2*np.arange(128//2) \n",
    "    ax[i].hist(all_test_notes[:,i].numpy(), bins=bins, label='test data', alpha=0.6)\n",
    "    ax[i].hist(all_new_notes[:,i].numpy(),  bins=bins, label='generated', alpha=0.6)\n",
    "    ax[i].set_title(names[i])\n",
    "    if i==2: ax[i].legend()\n",
    "    ax[i].set_xlabel('Value'+f'{\" (s)\" if i>0 else \"\"}')\n",
    "    if i==0: ax[i].set_ylabel('Count')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

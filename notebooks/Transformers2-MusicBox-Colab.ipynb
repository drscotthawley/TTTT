{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Ql1SrMmXm4Wk",
      "metadata": {
        "id": "Ql1SrMmXm4Wk"
      },
      "source": [
        "# Music Box / Transformers2 Lesson Colab\n",
        "\n",
        "\n",
        "<img src=\"https://drscotthawley.github.io/blog/posts/images/musicbox_prime.jpeg\" width=\"60%\">\n",
        "\n",
        "This Colab accompanies the blog post [\"Understanding Transformers, Part 2: Wee Music Box\"](https://drscotthawley.github.io/blog/posts/Transformers2-MusicBox.html\") by Scott H. Hawley.\n",
        "\n",
        "Refer to that post for commentary & exposition.\n",
        "Here is \"just the code\" (with comments).\n",
        "\n",
        "This whole thing executes in under 15 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd6668d1-f371-4528-bd5c-59d4d4f30d44",
      "metadata": {
        "id": "dd6668d1-f371-4528-bd5c-59d4d4f30d44"
      },
      "source": [
        "## Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c5d6eace-bb66-42d5-8d9f-b19003e535a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5d6eace-bb66-42d5-8d9f-b19003e535a1",
        "outputId": "6d892033-e260-40a9-bc6b-afd97c7a5327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  fluid-soundfont-gm libevdev2 libfluidsynth3 libgudev-1.0-0 libinput-bin\n",
            "  libinput10 libinstpatch-1.0-2 libmd4c0 libmtdev1 libqt5core5a libqt5dbus5\n",
            "  libqt5gui5 libqt5network5 libqt5svg5 libqt5widgets5 libwacom-bin\n",
            "  libwacom-common libwacom9 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
            "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1\n",
            "  libxkbcommon-x11-0 qsynth qt5-gtk-platformtheme qttranslations5-l10n\n",
            "  timgm6mb-soundfont\n",
            "Suggested packages:\n",
            "  fluid-soundfont-gs qt5-image-formats-plugins qtwayland5 jackd\n",
            "The following NEW packages will be installed:\n",
            "  fluid-soundfont-gm fluidsynth libevdev2 libfluidsynth3 libgudev-1.0-0\n",
            "  libinput-bin libinput10 libinstpatch-1.0-2 libmd4c0 libmtdev1 libqt5core5a\n",
            "  libqt5dbus5 libqt5gui5 libqt5network5 libqt5svg5 libqt5widgets5 libwacom-bin\n",
            "  libwacom-common libwacom9 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
            "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1\n",
            "  libxkbcommon-x11-0 qsynth qt5-gtk-platformtheme qttranslations5-l10n\n",
            "  timgm6mb-soundfont\n",
            "0 upgraded, 32 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 148 MB of archives.\n",
            "After this operation, 207 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5core5a amd64 5.15.3+dfsg-2ubuntu0.2 [2,006 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevdev2 amd64 1.12.1+dfsg-1 [39.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmtdev1 amd64 1.1.6-1build4 [14.5 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgudev-1.0-0 amd64 1:237-2build1 [16.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom-common all 2.2.0-1 [54.3 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom9 amd64 2.2.0-1 [22.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libinput-bin amd64 1.20.0-1ubuntu0.3 [19.9 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libinput10 amd64 1.20.0-1ubuntu0.3 [131 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmd4c0 amd64 0.4.8-1 [42.0 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5dbus5 amd64 5.15.3+dfsg-2ubuntu0.2 [222 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5network5 amd64 5.15.3+dfsg-2ubuntu0.2 [731 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5gui5 amd64 5.15.3+dfsg-2ubuntu0.2 [3,722 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5widgets5 amd64 5.15.3+dfsg-2ubuntu0.2 [2,561 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5svg5 amd64 5.15.3-1 [149 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fluid-soundfont-gm all 3.1-5.3 [130 MB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libinstpatch-1.0-2 amd64 1.1.6-1 [240 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/universe amd64 timgm6mb-soundfont all 1.3-5 [5,427 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfluidsynth3 amd64 2.2.5-1 [246 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fluidsynth amd64 2.2.5-1 [27.4 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom-bin amd64 2.2.0-1 [13.6 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qsynth amd64 0.9.6-1 [305 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 qt5-gtk-platformtheme amd64 5.15.3+dfsg-2ubuntu0.2 [130 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qttranslations5-l10n all 5.15.3-1 [1,983 kB]\n",
            "Fetched 148 MB in 12s (12.8 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 32.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libqt5core5a:amd64.\n",
            "(Reading database ... 121658 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libqt5core5a_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libevdev2:amd64.\n",
            "Preparing to unpack .../01-libevdev2_1.12.1+dfsg-1_amd64.deb ...\n",
            "Unpacking libevdev2:amd64 (1.12.1+dfsg-1) ...\n",
            "Selecting previously unselected package libmtdev1:amd64.\n",
            "Preparing to unpack .../02-libmtdev1_1.1.6-1build4_amd64.deb ...\n",
            "Unpacking libmtdev1:amd64 (1.1.6-1build4) ...\n",
            "Selecting previously unselected package libgudev-1.0-0:amd64.\n",
            "Preparing to unpack .../03-libgudev-1.0-0_1%3a237-2build1_amd64.deb ...\n",
            "Unpacking libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Selecting previously unselected package libwacom-common.\n",
            "Preparing to unpack .../04-libwacom-common_2.2.0-1_all.deb ...\n",
            "Unpacking libwacom-common (2.2.0-1) ...\n",
            "Selecting previously unselected package libwacom9:amd64.\n",
            "Preparing to unpack .../05-libwacom9_2.2.0-1_amd64.deb ...\n",
            "Unpacking libwacom9:amd64 (2.2.0-1) ...\n",
            "Selecting previously unselected package libinput-bin.\n",
            "Preparing to unpack .../06-libinput-bin_1.20.0-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking libinput-bin (1.20.0-1ubuntu0.3) ...\n",
            "Selecting previously unselected package libinput10:amd64.\n",
            "Preparing to unpack .../07-libinput10_1.20.0-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking libinput10:amd64 (1.20.0-1ubuntu0.3) ...\n",
            "Selecting previously unselected package libmd4c0:amd64.\n",
            "Preparing to unpack .../08-libmd4c0_0.4.8-1_amd64.deb ...\n",
            "Unpacking libmd4c0:amd64 (0.4.8-1) ...\n",
            "Selecting previously unselected package libqt5dbus5:amd64.\n",
            "Preparing to unpack .../09-libqt5dbus5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5network5:amd64.\n",
            "Preparing to unpack .../10-libqt5network5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libxcb-icccm4:amd64.\n",
            "Preparing to unpack .../11-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n",
            "Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Selecting previously unselected package libxcb-util1:amd64.\n",
            "Preparing to unpack .../12-libxcb-util1_0.4.0-1build2_amd64.deb ...\n",
            "Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Selecting previously unselected package libxcb-image0:amd64.\n",
            "Preparing to unpack .../13-libxcb-image0_0.4.0-2_amd64.deb ...\n",
            "Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Selecting previously unselected package libxcb-keysyms1:amd64.\n",
            "Preparing to unpack .../14-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n",
            "Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Selecting previously unselected package libxcb-render-util0:amd64.\n",
            "Preparing to unpack .../15-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n",
            "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Selecting previously unselected package libxcb-xinerama0:amd64.\n",
            "Preparing to unpack .../16-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-xinput0:amd64.\n",
            "Preparing to unpack .../17-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-xkb1:amd64.\n",
            "Preparing to unpack .../18-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxkbcommon-x11-0:amd64.\n",
            "Preparing to unpack .../19-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libqt5gui5:amd64.\n",
            "Preparing to unpack .../20-libqt5gui5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5widgets5:amd64.\n",
            "Preparing to unpack .../21-libqt5widgets5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5svg5:amd64.\n",
            "Preparing to unpack .../22-libqt5svg5_5.15.3-1_amd64.deb ...\n",
            "Unpacking libqt5svg5:amd64 (5.15.3-1) ...\n",
            "Selecting previously unselected package fluid-soundfont-gm.\n",
            "Preparing to unpack .../23-fluid-soundfont-gm_3.1-5.3_all.deb ...\n",
            "Unpacking fluid-soundfont-gm (3.1-5.3) ...\n",
            "Selecting previously unselected package libinstpatch-1.0-2:amd64.\n",
            "Preparing to unpack .../24-libinstpatch-1.0-2_1.1.6-1_amd64.deb ...\n",
            "Unpacking libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
            "Selecting previously unselected package timgm6mb-soundfont.\n",
            "Preparing to unpack .../25-timgm6mb-soundfont_1.3-5_all.deb ...\n",
            "Unpacking timgm6mb-soundfont (1.3-5) ...\n",
            "Selecting previously unselected package libfluidsynth3:amd64.\n",
            "Preparing to unpack .../26-libfluidsynth3_2.2.5-1_amd64.deb ...\n",
            "Unpacking libfluidsynth3:amd64 (2.2.5-1) ...\n",
            "Selecting previously unselected package fluidsynth.\n",
            "Preparing to unpack .../27-fluidsynth_2.2.5-1_amd64.deb ...\n",
            "Unpacking fluidsynth (2.2.5-1) ...\n",
            "Selecting previously unselected package libwacom-bin.\n",
            "Preparing to unpack .../28-libwacom-bin_2.2.0-1_amd64.deb ...\n",
            "Unpacking libwacom-bin (2.2.0-1) ...\n",
            "Selecting previously unselected package qsynth.\n",
            "Preparing to unpack .../29-qsynth_0.9.6-1_amd64.deb ...\n",
            "Unpacking qsynth (0.9.6-1) ...\n",
            "Selecting previously unselected package qt5-gtk-platformtheme:amd64.\n",
            "Preparing to unpack .../30-qt5-gtk-platformtheme_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package qttranslations5-l10n.\n",
            "Preparing to unpack .../31-qttranslations5-l10n_5.15.3-1_all.deb ...\n",
            "Unpacking qttranslations5-l10n (5.15.3-1) ...\n",
            "Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up qttranslations5-l10n (5.15.3-1) ...\n",
            "Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Setting up libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libmtdev1:amd64 (1.1.6-1build4) ...\n",
            "Setting up libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libmd4c0:amd64 (0.4.8-1) ...\n",
            "Setting up fluid-soundfont-gm (3.1-5.3) ...\n",
            "update-alternatives: using /usr/share/sounds/sf2/FluidR3_GM.sf2 to provide /usr/share/sounds/sf2/default-GM.sf2 (default-GM.sf2) in auto mode\n",
            "update-alternatives: using /usr/share/sounds/sf2/FluidR3_GM.sf2 to provide /usr/share/sounds/sf3/default-GM.sf3 (default-GM.sf3) in auto mode\n",
            "Setting up timgm6mb-soundfont (1.3-5) ...\n",
            "Setting up libevdev2:amd64 (1.12.1+dfsg-1) ...\n",
            "Setting up libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
            "Setting up libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Setting up libfluidsynth3:amd64 (2.2.5-1) ...\n",
            "Setting up libwacom-common (2.2.0-1) ...\n",
            "Setting up libwacom9:amd64 (2.2.0-1) ...\n",
            "Setting up libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libinput-bin (1.20.0-1ubuntu0.3) ...\n",
            "Setting up fluidsynth (2.2.5-1) ...\n",
            "Created symlink /etc/systemd/user/default.target.wants/fluidsynth.service → /usr/lib/systemd/user/fluidsynth.service.\n",
            "Setting up libwacom-bin (2.2.0-1) ...\n",
            "Setting up libinput10:amd64 (1.20.0-1ubuntu0.3) ...\n",
            "Setting up libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libqt5svg5:amd64 (5.15.3-1) ...\n",
            "Setting up qsynth (0.9.6-1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m446.5/446.5 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.1/247.1 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for mamba-ssm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for causal-conv1d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for buildtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get -q install fluidsynth\n",
        "\n",
        "!pip install -qq gdown pyfluidsynth pretty_midi torch numpy pandas midi-player multiprocess wandb matplotlib\n",
        "\n",
        "\n",
        "# Update: Added support for Mamba!\n",
        "use_mamba = False\n",
        "if use_mamba:  # Note: mamba-ssm requires CUDA. No CPU or MPS support\n",
        "    !pip install -q mamba-ssm causal-conv1d"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e2e1ec8-f4ce-42a9-aef8-b5945339511c",
      "metadata": {
        "id": "6e2e1ec8-f4ce-42a9-aef8-b5945339511c"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "939ffa5a-109c-439d-8528-941162b075aa",
      "metadata": {
        "id": "939ffa5a-109c-439d-8528-941162b075aa"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "import collections\n",
        "import datetime\n",
        "import fluidsynth\n",
        "import glob\n",
        "import pandas as pd\n",
        "import pretty_midi\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from typing import Optional\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "import multiprocess as mp   # multiprocess is a Jupyter-compatible fork of multiprocessing\n",
        "from functools import partial\n",
        "\n",
        "#from tqdm.notebook import tqdm   # colab gets the shiny progress bars but the text is all mangled\n",
        "from tqdm import tqdm  # note: Quarto blog won't print output from tqdm.notebook\n",
        "#from tqdm.contrib.concurrent import process_map  # process_map throws errors on Mac :'-(\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pprint\n",
        "\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from midi_player import MIDIPlayer\n",
        "from midi_player.stylers import general, dark # I like dark mode\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import wandb\n",
        "\n",
        "if use_mamba:\n",
        "    from mamba_ssm import Mamba"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "413c76b6-2a7c-4b48-9857-448b75f637bf",
      "metadata": {
        "id": "413c76b6-2a7c-4b48-9857-448b75f637bf"
      },
      "source": [
        "## MIDI Dataset Stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "qr3h3f7AqX5o",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr3h3f7AqX5o",
        "outputId": "a5fdbb08-8871-4dbc-ff71-255e90b05309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MdJiNEgtkvCx9tnyQWcnEE5GPMY6ADUb\n",
            "To: /content/jsb_chorales_midi.tgz\n",
            "\r  0% 0.00/137k [00:00<?, ?B/s]\r100% 137k/137k [00:00<00:00, 80.0MB/s]\n",
            "Number of files: 382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 382/382 [00:09<00:00, 38.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "len(notes_tensor_list) = 382\n",
            "all_notes.shape =  torch.Size([78349, 3])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# READ ALL THE DATA\n",
        "\n",
        "# Get the data\n",
        "data_source = 'jsb_chorales_midi'\n",
        "# TODO: add options for MAESTRO, others\n",
        "\n",
        "data_dir = Path('midi_data') # generic name for whatever midi we might want\n",
        "REST_PITCH = 127  # special code  used to denote rests\n",
        "\n",
        "!gdown -O {data_source}.tgz '1MdJiNEgtkvCx9tnyQWcnEE5GPMY6ADUb'\n",
        "!tar zxf {data_source}.tgz; rm -rf midi_data; ln -s {data_source} midi_data ;\n",
        "\n",
        "# get the list of MIDI filenames\n",
        "filenames = sorted(glob(str(data_dir/'**/*.mid*'),recursive=True))\n",
        "print('Number of files:', len(filenames))\n",
        "\n",
        "\n",
        "\n",
        "midi_file = filenames[0]\n",
        "MIDIPlayer(midi_file, 360, styler=dark, title=f\"midi_file = {midi_file}\")\n",
        "\n",
        "\n",
        "def quantize_times(times,\n",
        "                   res_ms=8, # resolution in milliseconds. 8ms was deemed sufficient for MAESTRO.\n",
        "                   clamp_range_s=[0.0,4.0]):\n",
        "    \"reasonably coarse time quantization is really helpful for facilitating learning temporal behavior\"\n",
        "    quant_step = res_ms / 1000.0\n",
        "    q_times = torch.round(times / quant_step) * quant_step\n",
        "    if clamp_range_s is not None:\n",
        "        q_times = torch.clamp(q_times, clamp_range_s[0], clamp_range_s[1])\n",
        "    return q_times\n",
        "\n",
        "\n",
        "def midi_file_to_tensor(filenames, i=None, keep_start_end=False, rest_pitches=[REST_PITCH]):  # filenames could just be a single filename\n",
        "    midi_file = filenames if i is None or type(filenames)==str  else filenames[i]\n",
        "    \"reads a single midi file and converts it to tensor with elements (pitch, step, duration)\"\n",
        "    pm = pretty_midi.PrettyMIDI(midi_file) # read in the whole file. this can be very slow for long MIDI files (e.g. in MAESTRO)\n",
        "\n",
        "    # Sort the notes first by start time (and then by pitch if two notes start at the same time)\n",
        "    sorted_notes = sorted(pm.instruments[0].notes, key=lambda note: (note.start, note.pitch))\n",
        "\n",
        "    prev_start = sorted_notes[0].start\n",
        "    notes = []\n",
        "    for i, note in enumerate(sorted_notes):\n",
        "        new_note =  torch.empty( (3 + 2*keep_start_end) , dtype=torch.float32)\n",
        "        if int(note.pitch) in rest_pitches: continue  # we can actually delete the rests!\n",
        "        new_note[0] = note.pitch\n",
        "        new_note[1] = note.start - prev_start  # step, i.e. time since start of previous note\n",
        "        new_note[2] = note.end - note.start    # duration\n",
        "        if keep_start_end:                     # might find it useful be able to keep these for easier analysis later\n",
        "            new_note[3] = note.start\n",
        "            new_note[4] = note.end\n",
        "        prev_start = note.start\n",
        "        notes.append(new_note)\n",
        "\n",
        "    notes = torch.vstack(notes)\n",
        "    notes[:,1:] = quantize_times(notes[:,1:])\n",
        "    return notes\n",
        "\n",
        "notes_tensor = midi_file_to_tensor(midi_file)\n",
        "\n",
        "\n",
        "\n",
        "def files_to_tensor_list(filenames, keep_start_end=False, serial=True):\n",
        "    \"Reads MIDI files in parallel so should be reasonably fast. JSB Chorales are no prob but for MAESTRO you want this\"\n",
        "    # tensor_list = process_map(midi_file_to_tensor, filenames, max_workers=mp.cpu_count(), chunksize=1) # Doesn't work on Mac\n",
        "    tensor_list = []\n",
        "    max_ = len(filenames)\n",
        "    if serial:\n",
        "        for i, filename in enumerate(tqdm(filenames)):\n",
        "            tensor_list.append(midi_file_to_tensor(filename,  keep_start_end=keep_start_end))\n",
        "    else:\n",
        "        with mp.Pool(processes=mp.cpu_count()) as p:\n",
        "            with tqdm(total=max_) as pbar:\n",
        "                for r in p.imap_unordered(partial(midi_file_to_tensor, filenames, keep_start_end=keep_start_end), range(0, max_)):\n",
        "                    tensor_list.append(r)\n",
        "                    pbar.update()\n",
        "    return  tensor_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "notes_tensor_list_filename = f'{data_source}_tensor_list.pt'         # we'll save the result to a file for easier re-reading next time\n",
        "read_all_midi_files = True # not os.path.exists(notes_tensor_list_filename) # check to see if it already exists\n",
        "\n",
        "if read_all_midi_files:\n",
        "    notes_tensor_list = files_to_tensor_list(filenames, serial=True)\n",
        "    torch.save(notes_tensor_list, notes_tensor_list_filename) # save for next time\n",
        "else:\n",
        "    notes_tensor_list = torch.load(notes_tensor_list_filename) # just read from the last time we made one\n",
        "\n",
        "print(f\"\\nlen(notes_tensor_list) = {len(notes_tensor_list)}\")\n",
        "\n",
        "notes_tl = notes_tensor_list\n",
        "all_notes = torch.vstack(notes_tl).type(torch.float32)\n",
        "print(\"all_notes.shape = \",all_notes.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "75efa9d8-14a4-4dde-bbeb-c275550d194f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "75efa9d8-14a4-4dde-bbeb-c275550d194f",
        "outputId": "096988dc-142f-4132-d4fc-8a291ffd4385"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Codebook sizes = [128, 15, 28]<br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Codebooks for steps & durations are not the same:"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"width: 100%; text-align: center;\">\n",
              "     <table style=\"margin-left: auto; margin-right: auto;\"> <tr>\n",
              "     <td style=\"vertical-align: top;\"><table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>steps (s)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>0.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>0.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>0.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>0.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table></td>\n",
              "     <td style=\"width: 50px;\"></td>\n",
              "     <td style=\"vertical-align: top;\"><table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>durations (s)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>0.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>0.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>0.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>0.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>0.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>0.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table></td>\n",
              "     </tr></table></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before encoding, midi_seq =\n",
            " tensor([[53.0000,  0.0000,  0.4800],\n",
            "        [57.0000,  0.0000,  0.4800],\n",
            "        [60.0000,  0.0000,  1.4400],\n",
            "        [65.0000,  0.0000,  0.4800],\n",
            "        [52.0000,  0.4800,  0.4800],\n",
            "        [55.0000,  0.0000,  0.4800],\n",
            "        [72.0000,  0.0000,  0.2400],\n",
            "        [70.0000,  0.2400,  0.2400]])\n",
            "After encoding, token_list =\n",
            " tensor([[53,  0,  3],\n",
            "        [57,  0,  3],\n",
            "        [60,  0, 11],\n",
            "        [65,  0,  3],\n",
            "        [52,  4,  3],\n",
            "        [55,  0,  3],\n",
            "        [72,  0,  1],\n",
            "        [70,  2,  1]])\n",
            "After decoding, return_seq =\n",
            " tensor([[53.0000,  0.0000,  0.4800],\n",
            "        [57.0000,  0.0000,  0.4800],\n",
            "        [60.0000,  0.0000,  1.4400],\n",
            "        [65.0000,  0.0000,  0.4800],\n",
            "        [52.0000,  0.4800,  0.4800],\n",
            "        [55.0000,  0.0000,  0.4800],\n",
            "        [72.0000,  0.0000,  0.2400],\n",
            "        [70.0000,  0.2400,  0.2400]])\n",
            "Preliminary Encoding/Decoding checks pass! :-)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 229/229 [00:04<00:00, 48.03it/s]\n",
            "100%|██████████| 76/76 [00:01<00:00, 70.93it/s]\n",
            "100%|██████████| 77/77 [00:01<00:00, 63.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "229 songs in train, 46660 notes\n",
            "76 songs in val, 15052 notes\n",
            "77 songs in test, 16637 notes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ENCODE & DECODE THE DATA\n",
        "\n",
        "def make_codebooks(all_notes, verbose=False):\n",
        "    codebooks = []\n",
        "    n_codebooks = all_notes.shape[-1] # should be 3\n",
        "    for i in range(n_codebooks):\n",
        "        if i==0:  # i=0 means pitch\n",
        "            cb_vals = torch.arange(128)  # just use all possible pitches. could limit to min/max range but...not gonna.\n",
        "        else:     # i!=0 means timing\n",
        "            cb_vals = all_notes[:,i].unique().sort()[0]\n",
        "        if verbose: print(f\"\\n---\\ncb {i}: cb_vals = {cb_vals}\")\n",
        "        codebooks.append(cb_vals)\n",
        "    return codebooks\n",
        "\n",
        "\n",
        "all_notes[:,1:] = quantize_times(all_notes[:,1:]) # we should already be time-quantized so this should be unnecessary, but...just in case\n",
        "codebooks = make_codebooks(all_notes, verbose=False)\n",
        "\n",
        "vocab_sizes = [len(cb) for cb in codebooks]\n",
        "display(HTML(f\"Codebook sizes = {vocab_sizes}<br><br>\"))\n",
        "\n",
        "\n",
        "###--- From here down, it's just a nice way to display the codebooks in the blog post:\n",
        "\n",
        "df1 = pd.DataFrame(codebooks[1].numpy(), columns=['steps (s)'])\n",
        "df2 = pd.DataFrame(codebooks[2].numpy(), columns=['durations (s)'])\n",
        "display(HTML(f\"Codebooks for steps & durations are {'' if df1.equals(df2) else 'not '}the same:\"))\n",
        "\n",
        "html1, html2 = df1.to_html(index=False),  df2.to_html(index=False)\n",
        "\n",
        "# Combine HTML strings with a space in between\n",
        "combined_html = f'''<div style=\"width: 100%; text-align: center;\">\n",
        "     <table style=\"margin-left: auto; margin-right: auto;\"> <tr>\n",
        "     <td style=\"vertical-align: top;\">{html1}</td>\n",
        "     <td style=\"width: 50px;\"></td>\n",
        "     <td style=\"vertical-align: top;\">{html2}</td>\n",
        "     </tr></table></div>'''\n",
        "\n",
        "# Display the combined HTML\n",
        "display(HTML(combined_html))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def encode_one_cb(seq:torch.Tensor, cb:torch.Tensor) -> int:\n",
        "    \"\"\" Encodes one 'column' of note info (either pitches, steps, or durations)\n",
        "        given new value(s) and codebook, return argmin of the difference\n",
        "        Difficulty: seq and cb may have different lengths & may not broadcast,\n",
        "        thus I wrote a loop that broadcasts each note over all cb values\n",
        "    \"\"\"\n",
        "    sidx = torch.empty( seq.shape, dtype=int, device=seq.device)\n",
        "    for i, note in enumerate(seq):                          # note: this is trivially parallel but jupyter mp hangs for me :'-(\n",
        "        sidx[i] = torch.argmin( ( note - cb )**2, dim=0 )   # could use torch.abs() instead of ^2 since we're in 1-D, but I want this to be general\n",
        "    return sidx\n",
        "\n",
        "def decode_one_cb(xi, cb:torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Decodes one 'column' of note-index info (either pitch_i, step_i, or duration_i)\n",
        "    xi should be an int, a list of ints, or a tensor of ints\"\"\"\n",
        "    return cb[xi]\n",
        "\n",
        "def encode(seq:torch.Tensor, cbs=codebooks) -> torch.Tensor:\n",
        "    \"Encodes a sequence of notes, where each note is (pitch, step, dur)\"\n",
        "    assert len(seq.shape) == 2, \"Not set up for batch processing yet, but could be a fun exercise for a student ;-) \"\n",
        "    encoded = torch.empty( seq.shape, dtype=int, device=seq.device)\n",
        "    for i, cb in enumerate(cbs):\n",
        "        encoded[:,i] = encode_one_cb( seq[:,i], cb)\n",
        "    return encoded\n",
        "\n",
        "def decode(sidx:torch.Tensor, cbs=codebooks) -> torch.Tensor:\n",
        "    \"Decodes a sequence of integer indices for notes (pitch_i, step_i, dur_i)\"\n",
        "    assert len(sidx.shape) == 2, \"Not set up for batch processing yet, but could be a fun exercise for a student ;-)\"\n",
        "    decoded = torch.empty( sidx.shape, dtype=torch.float32, device=sidx.device)\n",
        "    for i, cb in enumerate(cbs):\n",
        "        decoded[:,i] = decode_one_cb( sidx[:,i], cb)\n",
        "    return decoded\n",
        "\n",
        "\n",
        "\n",
        "# Let's do a little test to see if we get back what we encode\n",
        "midi_seq = all_notes[0:8].clone()\n",
        "print(\"Before encoding, midi_seq =\\n\",midi_seq)\n",
        "token_list = encode(midi_seq)\n",
        "print(\"After encoding, token_list =\\n\",token_list)\n",
        "return_seq = decode(token_list)\n",
        "print(\"After decoding, return_seq =\\n\",return_seq)\n",
        "assert torch.equal( midi_seq, return_seq), f\"Oops. midi_seq={midi_seq}, but return_seq={return_seq}. Should be the same\"\n",
        "\n",
        "# \"safety\" check re. values that exceed codebook range -- should get truncated to last cb entry\n",
        "midi_seq[-1,-1] = 100.0  # give the last time a huge value to check that our mapper won't crash\n",
        "token_list = encode(midi_seq, codebooks) # if it doesn't crash, we're good\n",
        "assert token_list[-1,-1] == len(codebooks[-1])-1, f\"Big value ({midi_seq[-1,-1]}) should have gotten the last spot in the last codebook ({len(codebooks[-1])-1}) but came back as {token_list[-1,-1]}\"\n",
        "\n",
        "print(\"Preliminary Encoding/Decoding checks pass! :-)\")\n",
        "\n",
        "\n",
        "\n",
        "def get_start_end(notes:torch.Tensor) -> torch.Tensor:\n",
        "    \"integrates (step,duration) timing pairs to recover (start,end) info. concats them as new columns\"\n",
        "    newnotes = torch.zeros((len(notes), 5), dtype=notes.dtype, device=notes.device)\n",
        "    newnotes[:,:3] = notes[:,:3]\n",
        "    prev_start = 0.0\n",
        "    for i, note in enumerate(notes):\n",
        "        step, dur = note[1], note[2]\n",
        "        start = step  + prev_start\n",
        "        end   = start + dur\n",
        "        newnotes[i,3], newnotes[i,4] = start, end\n",
        "        prev_start = start\n",
        "    return newnotes\n",
        "\n",
        "\n",
        "def notes_to_midi(notes:torch.Tensor,\n",
        "                  time_rescale=None,\n",
        "                  out_file: str = '',\n",
        "                  instrument_name: str = 'Acoustic Grand Piano',\n",
        "                  velocity: int = 64,  # default loudness for all notes\n",
        "                 ) -> pretty_midi.PrettyMIDI:\n",
        "    notes = notes.clone() # just to avoid weird overwrites of memory addresses\n",
        "    if notes.min() < 0.0:\n",
        "      print(\"WARNING: You have negative pitches, steps or durations. Setting them to zero\")\n",
        "      notes = notes * (notes >= 0)\n",
        "    if time_rescale is not None: # just added this because sometime I want to slow/speed up\n",
        "        notes[:,1:] = notes[:,1:] *time_rescale\n",
        "\n",
        "    pm = pretty_midi.PrettyMIDI()\n",
        "    instrument = pretty_midi.Instrument(\n",
        "        program=pretty_midi.instrument_name_to_program(\n",
        "            instrument_name))\n",
        "\n",
        "    if notes.shape[-1] < 5: notes = get_start_end(notes)\n",
        "\n",
        "    notes = notes.cpu().numpy()\n",
        "    prev_start = 0.0\n",
        "    for note in notes:\n",
        "        pitch, start, end = int(note[0]), note[3], note[4]\n",
        "        midi_note = pretty_midi.Note( velocity=velocity, pitch=pitch, start=start, end=end, )\n",
        "        instrument.notes.append(midi_note)\n",
        "        prev_start = start\n",
        "\n",
        "    pm.instruments.append(instrument)\n",
        "    if out_file: pm.write(out_file)\n",
        "    return pm\n",
        "\n",
        "\n",
        "\n",
        "def midiplayer(notes_tensor, height=400, time_rescale=None, midi_file=\"/tmp/tmp.mid\", title='', styler=dark):\n",
        "    \"MIDIplayer that writes input tensor to temporary file\"\n",
        "    pm = notes_to_midi(notes_tensor, time_rescale=time_rescale, out_file=midi_file)\n",
        "    return MIDIPlayer(midi_file, height, styler=dark, dl=True, title=title)\n",
        "\n",
        "midiplayer(decode(encode(midi_file_to_tensor(filenames[0]))), title='Encode-Decode Test')\n",
        "\n",
        "\n",
        "\n",
        "# tensor lists\n",
        "train_filenames = [x for x in filenames if '/train' in x]\n",
        "val_filenames   = [x for x in filenames if '/val'   in x]\n",
        "test_filenames  = [x for x in filenames if '/test'  in x]\n",
        "\n",
        "train_notes_tl = files_to_tensor_list(train_filenames, serial=True)\n",
        "val_notes_tl   = files_to_tensor_list(val_filenames, serial=True)\n",
        "test_notes_tl  = files_to_tensor_list(test_filenames, serial=True)\n",
        "\n",
        "for name, tl in zip(['train','val','test'],[train_notes_tl, val_notes_tl, test_notes_tl]):\n",
        "    stack = torch.vstack(tl)\n",
        "    print(f\"{len(tl)} songs in {name}, {stack.shape[0]} notes\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "rES5v85Kqnuc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rES5v85Kqnuc",
        "outputId": "693be09c-d40f-47f9-a645-51d18bd98100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.shape =  torch.Size([5, 3])\n",
            "original data = \n",
            " tensor([[ 54,  12,   6],\n",
            "        [ 61,   0,  40],\n",
            "        [127,  14,   4],\n",
            "        [ 86,   0,  12],\n",
            "        [126,   7,  12]])\n",
            "augmented data = \n",
            " tensor([[ 52,  12,   6],\n",
            "        [ 59,   0,  40],\n",
            "        [127,  14,   4],\n",
            "        [ 84,   0,  12],\n",
            "        [124,   7,  12]])\n",
            "Checks passed! :-) \n",
            "batch_x.shape, batch_y.shape =  torch.Size([128, 64, 3]) torch.Size([128, 64, 3])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# MAKE THE DATASET\n",
        "\n",
        "def augment_data(data, pitch_shift=12, debug=True, extra_augs=False):\n",
        "    datanew = data.clone()                                     # avoid overwriting memory of data\n",
        "    # pitch\n",
        "    change = torch.randint(-pitch_shift, pitch_shift, (1,))    # how many semitones to change all the pitches\n",
        "    datanew[ datanew[:,0] != REST_PITCH ] += torch.tensor((change, 0, 0))  # change the pitches\n",
        "    if not extra_augs: return datanew   #only do pitches\n",
        "\n",
        "    if torch.rand(1) < 0.2:                         # sometimes invert pitches? Probably not useful but anyway\n",
        "        datanew[ datanew[:,0] != REST_PITCH ] *= torch.tensor((-1, 1, 1))\n",
        "        datanew[ datanew[:,0] != REST_PITCH ] += torch.tensor((127, 0, 0))\n",
        "\n",
        "    # time - if we sometimes increase each non-zero time-token by one, that should be ok, right?\n",
        "    if torch.rand(1) < 0.2: # do step\n",
        "        datanew[ datanew[:,1] > 0]  += torch.tensor((0,1,0))\n",
        "    if torch.rand(1) < 0.2: # do duration\n",
        "        datanew[ datanew[:,2] > 0]  += torch.tensor((0,0,1))\n",
        "\n",
        "    # extra 'safety' constraint: clamp to range of valid values (of tokens)\n",
        "    for i, cb in enumerate(codebooks):\n",
        "        datanew[:,i] = torch.clamp(datanew[:,i], 0, len(cb['encode'])-1)\n",
        "    return datanew\n",
        "\n",
        "\n",
        "# testing augmentation:\n",
        "torch.manual_seed(3) # setting this just  to make sure something happens ;-)\n",
        "data = torch.tensor([[54,12,6],[61,0,40],[127,14,4],[86,0,12],[126,7,12]])\n",
        "print(\"data.shape = \",data.shape)\n",
        "print(\"original data = \\n\",data)\n",
        "aug = augment_data(data)\n",
        "print(\"augmented data = \\n\",aug) #\n",
        "\n",
        "assert not torch.equal(aug[:,0], data[:,0]), \"Oops, nothing changed\"\n",
        "assert aug[2,0]==data[2,0], \"Oops,  The 127 got changed\"\n",
        "print(\"Checks passed! :-) \")\n",
        "\n",
        "\n",
        "class NotesDataset(Dataset):\n",
        "    \"simple custom dataset of sliding windows\"\n",
        "    def __init__(self,\n",
        "                 tensor_list,\n",
        "                 seq_length:int,\n",
        "                 tokenizer=encode,\n",
        "                 codebooks=codebooks,\n",
        "                 aug_callback=augment_data,\n",
        "                 len_mult=100,  # factor to 'fudge' the dataset length when it's inspected by DataLoaders\n",
        "                 pad=True, # pad end with rests\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.sl = seq_length\n",
        "        self.len_mult = len_mult\n",
        "        self.data_list = [tokenizer(t) for t in tensor_list] # encoded tokens are all we'll use\n",
        "        if pad:\n",
        "            rests = torch.tensor([REST_PITCH,1,1]).unsqueeze(0).tile((seq_length,1))\n",
        "            self.data_list = [torch.cat((toks,rests), dim=0) for toks in self.data_list]\n",
        "        self.aug_callback = aug_callback\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"we're going to be grabbing random windows from the data, so just the len of the tensor\n",
        "        list will be too small for large batch sizes, hence we multiply by len_mult\"\"\"\n",
        "        return len(self.data_list)*self.len_mult  # this will keep the DataLoader going longer\n",
        "\n",
        "    def __getitem__(self, idx, shift=1) -> (torch.Tensor, torch.Tensor):\n",
        "        \"grabs a random 'window' from a random song, with an offset of `shift` tokens between inputs and targets\"\n",
        "        i_song = torch.randint(0, len(self.data_list), (1,)) # pick a song\n",
        "        ix =  torch.randint(0, len(self.data_list[i_song]) - self.sl - 1, (1,))  # start of window within song\n",
        "        data_block = self.data_list[i_song][ix:ix+self.sl+1]  # grab window plus an extra character\n",
        "        if self.aug_callback is not None:\n",
        "            data_block = self.aug_callback(data_block)\n",
        "        inputs, targets = data_block[:self.sl], data_block[shift:self.sl+shift]\n",
        "        return inputs, targets\n",
        "\n",
        "\n",
        "seq_length = 64\n",
        "train_ds = NotesDataset(train_notes_tl, seq_length)\n",
        "val_ds = NotesDataset(val_notes_tl, seq_length, aug_callback=None, len_mult=1000000) # no aug, neverending\n",
        "# save test_ds for Evaluation section, later\n",
        "len(train_ds), len(val_ds)\n",
        "\n",
        "\n",
        "batch_size = 128 # We may change this further down, for now it's just a test\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, )\n",
        "\n",
        "batch_x, batch_y = next(iter(train_dl))\n",
        "print(\"batch_x.shape, batch_y.shape = \",batch_x.shape, batch_y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V9HOLLYXptuI",
      "metadata": {
        "id": "V9HOLLYXptuI"
      },
      "source": [
        "## Hyperparam stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "D26vi67EpxtV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D26vi67EpxtV",
        "outputId": "07108811-4398-44b7-c239-a9740b6ed003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MonsterConfig(seq_length=128,\n",
            "              batch_size=256,\n",
            "              n_embd=256,\n",
            "              n_heads=16,\n",
            "              n_blocks=8,\n",
            "              dropout=0.7,\n",
            "              bias=False,\n",
            "              use_alibi=True,\n",
            "              use_mamba=True,\n",
            "              learning_rate=0.001,\n",
            "              weight_decay=0.08,\n",
            "              epochs=12,\n",
            "              vocab_sizes=(128, 15, 28))\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class MusicBoxConfig:\n",
        "    # model architecture details\n",
        "    seq_length: int = 64\n",
        "    batch_size: int = 128\n",
        "    n_embd:     int = 128     # embedding dimension to use for tokens & positions\n",
        "    n_heads:    int = 8       # number of attention heads\n",
        "    n_blocks:   int = 4       # number of attention blocks\n",
        "    dropout:  float = 0.1     # dropout value applied everywhere\n",
        "    bias:      bool = False   # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "    use_alibi: bool = False   # use ALiBi for/instead of PE\n",
        "    use_mamba: bool = use_mamba  # this gets set near the top of this notebook\n",
        "\n",
        "    # training details\n",
        "    learning_rate: float = 0.001\n",
        "    weight_decay:  float = 0.01   # 0.01 is pytorch default\n",
        "    epochs:          int = 20\n",
        "\n",
        "    # other handy bookkeeping\n",
        "    vocab_sizes: tuple = tuple(vocab_sizes)\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class MonsterConfig:  # \"Monster\" size model ;-)\n",
        "    # model architecture details\n",
        "    seq_length: int = 128\n",
        "    batch_size: int = 256\n",
        "    n_embd:     int = 256\n",
        "    n_heads:    int = 16\n",
        "    n_blocks:   int = 8\n",
        "    dropout:  float = 0.7\n",
        "    bias:      bool = False\n",
        "    use_alibi: bool = True\n",
        "    use_mamba: bool = use_mamba  # this gets set near the top of this notebook\n",
        "\n",
        "    # training details\n",
        "    learning_rate: float = 0.001\n",
        "    weight_decay:  float = 0.08   # 0.01 is pytorch default\n",
        "    epochs:          int = 12\n",
        "\n",
        "    # other handy bookkeeping\n",
        "    vocab_sizes: tuple = tuple(vocab_sizes)\n",
        "\n",
        "\n",
        "\n",
        "config = MusicBoxConfig()\n",
        "#config = MonsterConfig()\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "pp.pprint(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25e34924-4b5d-49a8-9797-a33579f8d9c1",
      "metadata": {
        "id": "25e34924-4b5d-49a8-9797-a33579f8d9c1"
      },
      "source": [
        "## Transformer Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "25407c35-f43c-4e16-b39b-07314e7a2e7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25407c35-f43c-4e16-b39b-07314e7a2e7b",
        "outputId": "cfe94af5-cd5b-45a4-a2d4-53f923625245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.803051 M parameters in the model\n"
          ]
        }
      ],
      "source": [
        "# from my mod of Karpathy's code.\n",
        "\n",
        "# Unless otherwise noted, anything good in what follows was pasted\n",
        "# from https://github.com/karpathy/minGPT and adapted by Scott H. Hawley (SHH)\n",
        "\n",
        "\n",
        "def create_alibi_mask(n):\n",
        "    \"SHH: added this after the fact to demo ALiBi support\"\n",
        "    mat = torch.zeros((n, n))\n",
        "\n",
        "    # Iterate over the lower triangular indices (excluding the diagonal)\n",
        "    for i in range(1, n):\n",
        "        for j in range(i):\n",
        "            mat[i, j] = -(i - j)\n",
        "\n",
        "    # Create a mask for the upper triangular part, excluding the diagonal\n",
        "    mask = torch.triu(torch.ones_like(mat), diagonal=1)\n",
        "\n",
        "    mat[mask.bool()] = float('-inf') # Apply mask, set upper triangular -inf\n",
        "    return mat\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention\n",
        "        SHH: Note: Often Multi-head Attention is done all at once via one big matrix multiply,\n",
        "                   but Karpathy (perhaps for reasons of clarity or to keep VRAM usage low)\n",
        "                   implemented each Head separately and then looped over them.\n",
        "    \"\"\"\n",
        "    def __init__(self, head_size, config):\n",
        "        super().__init__()\n",
        "        n_embd, block_size = config.n_embd, config.seq_length\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.use_alibi = config.use_alibi\n",
        "        if self.use_alibi:\n",
        "            self.register_buffer('alibi', create_alibi_mask(block_size))\n",
        "        else:\n",
        "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        if self.use_alibi:\n",
        "            wei = wei + self.alibi[:T,:T]\n",
        "        else:\n",
        "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel\n",
        "        SHH: As noted above, typically MHA is one big matmul (as in my original code),\n",
        "             but Karpathy looped over heads so we'll do that.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, head_size, config):\n",
        "        super().__init__()\n",
        "        n_embd = config.n_embd\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(head_size, config) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "    def __init__(self, n_embd, config):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation\n",
        "        SHH: Note: The ordering of the LayerNorm ops has differed in different Transformer implementations\n",
        "                   Some have pointed out that the original Transformer architecture diagram didn't match the code\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, config):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        if not config.use_mamba:\n",
        "            self.sa = MultiHeadAttention(n_head, head_size, config)\n",
        "        else:\n",
        "            self.sa = Mamba( # TODO: Attribution: Whose code is this?\n",
        "                # This module uses roughly 3 * expand * d_model^2 parameters\n",
        "                d_model=n_embd, # Model dimension d_model\n",
        "                d_state=16,  # SSM state expansion factor\n",
        "                d_conv=4,    # Local convolution width\n",
        "                expand=2,    # Block expansion factor\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "        self.ffwd = FeedFoward(n_embd, config)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"  Main Transformer class, from Karpathy's \"BigramLanguageModel\" class.\n",
        "         SHH added the looping over multiple codebooks / vocabs\n",
        "    \"\"\"\n",
        "    def __init__(self, config, debug=False):\n",
        "        super().__init__()\n",
        "        n_cb = len(config.vocab_sizes)\n",
        "        self.block_size, n_embd, n_head, n_layer, self.use_alibi = config.seq_length, config.n_embd, config.n_heads, config.n_blocks, config.use_alibi\n",
        "\n",
        "        # seperate embeddings for pitch, step & dur part of notes\n",
        "        self.token_embedding_tables = nn.ModuleList([nn.Embedding(vocab_sizes[cbi], n_embd) for cbi in range(n_cb)])\n",
        "        if not config.use_alibi: self.position_embedding_table = nn.Embedding(self.block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, config) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_heads = nn.ModuleList([nn.Linear(n_embd, vocab_sizes[cbi]) for cbi in range(n_cb)]) # output token predictors\n",
        "        self.debug=False\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is array of input token indices in the current context\n",
        "        B, T, CBS = idx.shape\n",
        "\n",
        "        tok_emb = 0\n",
        "        for cb in range(CBS): # just sum codebook reps\n",
        "            tok_emb = tok_emb + self.token_embedding_tables[cb](idx[:,:,cb])\n",
        "        x = tok_emb\n",
        "        if not self.use_alibi:\n",
        "            pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T,E)\n",
        "            x = x + pos_emb    #  sum token embeddings & positional embeddings\n",
        "\n",
        "        x = self.blocks(x) # Main computation loop!\n",
        "        x = self.ln_f(x)   # final layernorm\n",
        "        logits_list = [head(x) for head in self.lm_heads]  # list of output projections over all codebook values\n",
        "\n",
        "        if targets is None:  # need targets to compute loss\n",
        "            loss = None\n",
        "        else:\n",
        "            lambdas = [0.5]*CBS   # relative \"weights\" to pitch, step, dur parts of loss. TODO: Should be in config as hyperparams\n",
        "            loss = 0.0\n",
        "            for cb in range(CBS):        # loop over codebooks  (for pitch, step & dur), summing loss\n",
        "                logits = logits_list[cb]\n",
        "                B, T, V = logits.shape   # V = vocab size, i.e. codebook length\n",
        "                targ = targets[:,:,cb]   # B, T\n",
        "                logits = logits.view(B*T, V)\n",
        "                targ = targ.reshape(B*T)    # needs reshape & not view b/c of contiguous memory issues.\n",
        "                loss = loss +  lambdas[cb] * F.cross_entropy(logits, targ)\n",
        "        return logits_list, loss\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
        "        # idx is (B, T, CBS) array of token indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size:]      # crop idx to the last block_size tokens\n",
        "            logits_list, loss = self(idx_cond)   # get the predictions\n",
        "            idx_next_list = []\n",
        "            for cb in range(idx_cond.shape[-1]):\n",
        "                # focus only on the last time step\n",
        "                logits = logits_list[cb]  # B, T, V  where V = vocab/embedding size\n",
        "                logits = logits[:, -1, :] # get last time.  becomes (B, V)\n",
        "                # apply softmax to get probabilities\n",
        "                probs = F.softmax(logits/temperature, dim=-1) # (B, V)\n",
        "                # sample from the distribution\n",
        "                idx_next_list.append(torch.multinomial(probs, num_samples=1)) # (B, 1)\n",
        "\n",
        "            idx_next = torch.tensor(idx_next_list).unsqueeze(0).unsqueeze(0).to(idx.device)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "# test that, make sure we don't get any errors:\n",
        "model = Transformer(config)\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters in the model')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52ccd4da-5c0d-477a-ad43-cfe74a677bc5",
      "metadata": {
        "id": "52ccd4da-5c0d-477a-ad43-cfe74a677bc5"
      },
      "source": [
        "## Stuff Before Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "cb97efbb-c4ef-4403-ab07-5f38e2382d53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "id": "cb97efbb-c4ef-4403-ab07-5f38e2382d53",
        "outputId": "5bfbe628-432f-45d1-a561-fcc39fcf8afc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device is cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<midi_player.midi_player.MIDIPlayer at 0x7b26951139a0>"
            ],
            "text/html": [
              "<iframe srcdoc=\"&lt;script src=&quot;https://cdn.jsdelivr.net/combine/npm/tone@14.7.58,npm/@magenta/music@1.23.1/es6/core.js,npm/focus-visible@5,npm/html-midi-player@1.5.0&quot;&gt;&lt;/script&gt;\n",
              "\n",
              "&lt;style&gt;\n",
              "/* Custom player style */\n",
              "p { \n",
              "  margin:0; \n",
              "  color: #c4c4c4; /* mid-lightness text color for title, intended for dark backgrounds */\n",
              "}\n",
              "\n",
              "#section864 midi-player {\n",
              "  display: block;\n",
              "  width: inherit;\n",
              "  margin: 4px;\n",
              "  margin-bottom: 0;\n",
              "  color: #d4d4d4; /* Lighter text color for better readability */\n",
              "}\n",
              "#section864 midi-player::part(control-panel) {\n",
              "  background: #222; /* Dark background */\n",
              "  border: 2px solid #888; /* Lightened border color for contrast */\n",
              "  border-radius: 10px 10px 0 0;\n",
              "}\n",
              "#section864 midi-player::part(play-button) {\n",
              "  color: #ffffff; /* White text for visibility */\n",
              "  border: 2px solid currentColor;\n",
              "  background-color: #6c7a89; \n",
              "  border-radius: 20px;\n",
              "  transition: all 0.2s;\n",
              "  content: &#x27;hello&#x27;;\n",
              "}\n",
              "#section864 midi-player::part(play-button):hover {\n",
              "  color: #00a; \n",
              "  background-color: #9fafc9; \n",
              "  border-radius: 10px;\n",
              "}\n",
              "#section864 midi-player::part(time) {\n",
              "  font-family: monospace; /* Monospace font for time */\n",
              "}\n",
              "\n",
              "/* Custom visualizer style */\n",
              "#section864 midi-visualizer .piano-roll-visualizer {\n",
              "  background: #333; /* Dark background for visualizer */\n",
              "  border: 2px solid #505050; /* Dark border for subtle appearance */\n",
              "  border-top: none;\n",
              "  border-radius: 0 0 10px 10px;\n",
              "  margin: 4px;\n",
              "  width: inherit;\n",
              "  margin-top: 0;\n",
              "  overflow: auto;\n",
              "}\n",
              "#section864 midi-visualizer svg rect.note {\n",
              "  opacity: 0.9; \n",
              "  stroke-width: 1; /* Stroke width for note clarity */\n",
              "}\n",
              "\n",
              "/* Different instrument colors */\n",
              "#section864 midi-visualizer svg rect.note[data-instrument=&quot;0&quot;]{\n",
              "  fill: #7aa6ed; /*  blue for Instrument 0 */\n",
              "  stroke: #444; \n",
              "}\n",
              "#section864 midi-visualizer svg rect.note[data-instrument=&quot;2&quot;]{\n",
              "  fill: #d586d0; /* purple for Instrument 2 for consistency */\n",
              "  stroke: #444; /* White stroke for visibility */\n",
              "}\n",
              "#section864 midi-visualizer svg rect.note[data-is-drum=&quot;true&quot;]{\n",
              "  fill: brightorange; \n",
              "  stroke: #bbb;\n",
              "}\n",
              "#section864 midi-visualizer svg rect.note.active {\n",
              "  opacity: 0.9; /* Highlight active notes */\n",
              "  stroke: #ddd; /* White stroke for maximum contrast */\n",
              "  stroke-width: 2; /* Thicker stroke for active notes */\n",
              "}\n",
              "&lt;/style&gt;\n",
              "\n",
              "          &lt;section id=&quot;section864&quot;&gt;&lt;p style=&quot;text-align:left;font-family:Arial;&quot;&gt;Full demo &#x27;target&#x27;, 149 notes in length&lt;span style=&quot;float:right;&quot;&gt;&lt;a href=&quot;data:audio/midi;base64,TVRoZAAAAAYAAQACANxNVHJrAAAAEwD/UQMHoSAA/1gEBAIYCAH/LwBNVHJrAAADlwDAAACQMEAAPEAAQ0AASECBUzwAAEBAajAAADJAaTIAADRAajQAADVAajUAADdAAD5AAEAAAEdAAEgAaUFAAEMAai1AADcAADxAAD4AAEBAAEEAAEVAAEcAaS0AAC9Aai8AADBAajAAADJAAEAAAEFAaTIAADRAAEEAAENAAEUAgVM0AAA5QABAQABDAABIQIFUNEAAN0AAOQAAPAAAQACBUzQAAENAAEdAAEgAAEpAgVMwQAA3AABAQABHAABKAABMQIMmPEAAQACCPTAAADJAajIAADRAaTQAADVAajUAADdAADtAADwAajcAADlAADsAaThAADkAADtAAEFAAEMAAEpAAEwAajZAADgAaTYAADhAAEBAAEEAajRAADgAajQAADlAADsAADxAAEhAAEoAaTdAADkAajVAADcAAEAAAEVAAEgAAE1AgVM0QAA1AABDQABFAGoyQAA0AAA7QAA8AABBQABDAGkwQAAyAAA7AABBAABDQABIQABMQABNAIFTMAAAN0AAR0AASAAASkAATACDJzcAADlAADxAAEBAAEMAAEcAAEhAAEoAaUAAAEJAajdAADkAADtAADwAAEIAAENAAEgAAEpAajVAADcAaTRAADUAajJAADQAaTBAADIAADsAADxAAEoAAExAajAAADJAajIAADRAADtAADwAAEMAAERAAEpAAEwAaTJAADQAajBAADIAaS9AADAAai1AAC8AADsAADxAAEQAAEVAAEhAAEoAgVMtAAA1QABIAIFTMkAANQAAO0AAPAAAQ0AARQAAR0BqOwAAPEAAQUAAQwBqMgAAN0AAPAAAPkAAQQAAQ0CBUzBAADcAAD4AAEBAAEcAAEhAgyYvQAAwAAA+QABAAABIAABPQIFTLwAAMEAAPEAAPgAATEAATwBqL0AAMABqLwAAMEAAN0AAPABpMAAAMkAAQUAAQwAASkAATABqMgAANEAANwAAPEAAQQAAQ0AASEAASgBpMEAANABqMAAANUAAQwAARUAASAAASkBqNEAANQBpNAAANUAAO0AAPAAAQ0AARQBqNQAAN0AASgAATEBpNwAAOUAAOwAAQwAASEAATAAATUBqO0BqN0AAOQAAOwAAPEAATEAATQCBUzVAADcAADwAAENAAEdAAEgAAEpAAEwAaUVAAEcAakUAakFAaTBAADUAAEBAAEEAAEhAAEoAhHowAABAAABDAABIAAH/LwA=&quot; target=&quot;_blank&quot;&gt;Download MIDI&lt;/a&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;\n",
              "          &lt;midi-player src=data:audio/midi;base64,TVRoZAAAAAYAAQACANxNVHJrAAAAEwD/UQMHoSAA/1gEBAIYCAH/LwBNVHJrAAADlwDAAACQMEAAPEAAQ0AASECBUzwAAEBAajAAADJAaTIAADRAajQAADVAajUAADdAAD5AAEAAAEdAAEgAaUFAAEMAai1AADcAADxAAD4AAEBAAEEAAEVAAEcAaS0AAC9Aai8AADBAajAAADJAAEAAAEFAaTIAADRAAEEAAENAAEUAgVM0AAA5QABAQABDAABIQIFUNEAAN0AAOQAAPAAAQACBUzQAAENAAEdAAEgAAEpAgVMwQAA3AABAQABHAABKAABMQIMmPEAAQACCPTAAADJAajIAADRAaTQAADVAajUAADdAADtAADwAajcAADlAADsAaThAADkAADtAAEFAAEMAAEpAAEwAajZAADgAaTYAADhAAEBAAEEAajRAADgAajQAADlAADsAADxAAEhAAEoAaTdAADkAajVAADcAAEAAAEVAAEgAAE1AgVM0QAA1AABDQABFAGoyQAA0AAA7QAA8AABBQABDAGkwQAAyAAA7AABBAABDQABIQABMQABNAIFTMAAAN0AAR0AASAAASkAATACDJzcAADlAADxAAEBAAEMAAEcAAEhAAEoAaUAAAEJAajdAADkAADtAADwAAEIAAENAAEgAAEpAajVAADcAaTRAADUAajJAADQAaTBAADIAADsAADxAAEoAAExAajAAADJAajIAADRAADtAADwAAEMAAERAAEpAAEwAaTJAADQAajBAADIAaS9AADAAai1AAC8AADsAADxAAEQAAEVAAEhAAEoAgVMtAAA1QABIAIFTMkAANQAAO0AAPAAAQ0AARQAAR0BqOwAAPEAAQUAAQwBqMgAAN0AAPAAAPkAAQQAAQ0CBUzBAADcAAD4AAEBAAEcAAEhAgyYvQAAwAAA+QABAAABIAABPQIFTLwAAMEAAPEAAPgAATEAATwBqL0AAMABqLwAAMEAAN0AAPABpMAAAMkAAQUAAQwAASkAATABqMgAANEAANwAAPEAAQQAAQ0AASEAASgBpMEAANABqMAAANUAAQwAARUAASAAASkBqNEAANQBpNAAANUAAO0AAPAAAQ0AARQBqNQAAN0AASgAATEBpNwAAOUAAOwAAQwAASEAATAAATUBqO0BqN0AAOQAAOwAAPEAATEAATQCBUzVAADcAADwAAENAAEdAAEgAAEpAAEwAaUVAAEcAakUAakFAaTBAADUAAEBAAEEAAEhAAEoAhHowAABAAABDAABIAAH/LwA= sound-font visualizer=&quot;#section864 midi-visualizer&quot;&gt;&lt;/midi-player&gt;\n",
              "          &lt;midi-visualizer src=data:audio/midi;base64,TVRoZAAAAAYAAQACANxNVHJrAAAAEwD/UQMHoSAA/1gEBAIYCAH/LwBNVHJrAAADlwDAAACQMEAAPEAAQ0AASECBUzwAAEBAajAAADJAaTIAADRAajQAADVAajUAADdAAD5AAEAAAEdAAEgAaUFAAEMAai1AADcAADxAAD4AAEBAAEEAAEVAAEcAaS0AAC9Aai8AADBAajAAADJAAEAAAEFAaTIAADRAAEEAAENAAEUAgVM0AAA5QABAQABDAABIQIFUNEAAN0AAOQAAPAAAQACBUzQAAENAAEdAAEgAAEpAgVMwQAA3AABAQABHAABKAABMQIMmPEAAQACCPTAAADJAajIAADRAaTQAADVAajUAADdAADtAADwAajcAADlAADsAaThAADkAADtAAEFAAEMAAEpAAEwAajZAADgAaTYAADhAAEBAAEEAajRAADgAajQAADlAADsAADxAAEhAAEoAaTdAADkAajVAADcAAEAAAEVAAEgAAE1AgVM0QAA1AABDQABFAGoyQAA0AAA7QAA8AABBQABDAGkwQAAyAAA7AABBAABDQABIQABMQABNAIFTMAAAN0AAR0AASAAASkAATACDJzcAADlAADxAAEBAAEMAAEcAAEhAAEoAaUAAAEJAajdAADkAADtAADwAAEIAAENAAEgAAEpAajVAADcAaTRAADUAajJAADQAaTBAADIAADsAADxAAEoAAExAajAAADJAajIAADRAADtAADwAAEMAAERAAEpAAEwAaTJAADQAajBAADIAaS9AADAAai1AAC8AADsAADxAAEQAAEVAAEhAAEoAgVMtAAA1QABIAIFTMkAANQAAO0AAPAAAQ0AARQAAR0BqOwAAPEAAQUAAQwBqMgAAN0AAPAAAPkAAQQAAQ0CBUzBAADcAAD4AAEBAAEcAAEhAgyYvQAAwAAA+QABAAABIAABPQIFTLwAAMEAAPEAAPgAATEAATwBqL0AAMABqLwAAMEAAN0AAPABpMAAAMkAAQUAAQwAASkAATABqMgAANEAANwAAPEAAQQAAQ0AASEAASgBpMEAANABqMAAANUAAQwAARUAASAAASkBqNEAANQBpNAAANUAAO0AAPAAAQ0AARQBqNQAAN0AASgAATEBpNwAAOUAAOwAAQwAASEAATAAATUBqO0BqN0AAOQAAOwAAPEAATEAATQCBUzVAADcAADwAAENAAEdAAEgAAEpAAEwAaUVAAEcAakUAakFAaTBAADUAAEBAAEEAAEhAAEoAhHowAABAAABDAABIAAH/LwA= type=piano-roll&gt;&lt;/midi-visualizer&gt;\n",
              "          &lt;/section&gt;\n",
              "          \" width=\"100%\" height=\"400\"\n",
              "            style=\"border:none !important;\"\n",
              "            \"allowfullscreen\" \"webkitallowfullscreen\" \"mozallowfullscreen\">'\n",
              "            </iframe>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<midi_player.midi_player.MIDIPlayer at 0x7b25a318d870>"
            ],
            "text/html": [
              "<iframe srcdoc=\"&lt;script src=&quot;https://cdn.jsdelivr.net/combine/npm/tone@14.7.58,npm/@magenta/music@1.23.1/es6/core.js,npm/focus-visible@5,npm/html-midi-player@1.5.0&quot;&gt;&lt;/script&gt;\n",
              "\n",
              "&lt;style&gt;\n",
              "/* Custom player style */\n",
              "p { \n",
              "  margin:0; \n",
              "  color: #c4c4c4; /* mid-lightness text color for title, intended for dark backgrounds */\n",
              "}\n",
              "\n",
              "#section394 midi-player {\n",
              "  display: block;\n",
              "  width: inherit;\n",
              "  margin: 4px;\n",
              "  margin-bottom: 0;\n",
              "  color: #d4d4d4; /* Lighter text color for better readability */\n",
              "}\n",
              "#section394 midi-player::part(control-panel) {\n",
              "  background: #222; /* Dark background */\n",
              "  border: 2px solid #888; /* Lightened border color for contrast */\n",
              "  border-radius: 10px 10px 0 0;\n",
              "}\n",
              "#section394 midi-player::part(play-button) {\n",
              "  color: #ffffff; /* White text for visibility */\n",
              "  border: 2px solid currentColor;\n",
              "  background-color: #6c7a89; \n",
              "  border-radius: 20px;\n",
              "  transition: all 0.2s;\n",
              "  content: &#x27;hello&#x27;;\n",
              "}\n",
              "#section394 midi-player::part(play-button):hover {\n",
              "  color: #00a; \n",
              "  background-color: #9fafc9; \n",
              "  border-radius: 10px;\n",
              "}\n",
              "#section394 midi-player::part(time) {\n",
              "  font-family: monospace; /* Monospace font for time */\n",
              "}\n",
              "\n",
              "/* Custom visualizer style */\n",
              "#section394 midi-visualizer .piano-roll-visualizer {\n",
              "  background: #333; /* Dark background for visualizer */\n",
              "  border: 2px solid #505050; /* Dark border for subtle appearance */\n",
              "  border-top: none;\n",
              "  border-radius: 0 0 10px 10px;\n",
              "  margin: 4px;\n",
              "  width: inherit;\n",
              "  margin-top: 0;\n",
              "  overflow: auto;\n",
              "}\n",
              "#section394 midi-visualizer svg rect.note {\n",
              "  opacity: 0.9; \n",
              "  stroke-width: 1; /* Stroke width for note clarity */\n",
              "}\n",
              "\n",
              "/* Different instrument colors */\n",
              "#section394 midi-visualizer svg rect.note[data-instrument=&quot;0&quot;]{\n",
              "  fill: #7aa6ed; /*  blue for Instrument 0 */\n",
              "  stroke: #444; \n",
              "}\n",
              "#section394 midi-visualizer svg rect.note[data-instrument=&quot;2&quot;]{\n",
              "  fill: #d586d0; /* purple for Instrument 2 for consistency */\n",
              "  stroke: #444; /* White stroke for visibility */\n",
              "}\n",
              "#section394 midi-visualizer svg rect.note[data-is-drum=&quot;true&quot;]{\n",
              "  fill: brightorange; \n",
              "  stroke: #bbb;\n",
              "}\n",
              "#section394 midi-visualizer svg rect.note.active {\n",
              "  opacity: 0.9; /* Highlight active notes */\n",
              "  stroke: #ddd; /* White stroke for maximum contrast */\n",
              "  stroke-width: 2; /* Thicker stroke for active notes */\n",
              "}\n",
              "&lt;/style&gt;\n",
              "\n",
              "          &lt;section id=&quot;section394&quot;&gt;&lt;p style=&quot;text-align:left;font-family:Arial;&quot;&gt;16-note &#x27;prompt&#x27; for demos&lt;span style=&quot;float:right;&quot;&gt;&lt;a href=&quot;data:audio/midi;base64,TVRoZAAAAAYAAQACANxNVHJrAAAAEwD/UQMHoSAA/1gEBAIYCAH/LwBNVHJrAAAAawDAAACQMEAAPEAAQ0AASECBUzwAAEBAajAAADJAaTIAADRAajQAADVAajUAADdAAD5AAEAAAEdAAEgAaUFAAEMAai1AADcAADxAAD4AAEBAAEEAAEVAAEcAaS0AgVRAAGlFAIMnPAAB/y8A&quot; target=&quot;_blank&quot;&gt;Download MIDI&lt;/a&gt;&lt;br&gt;&lt;/span&gt;&lt;/p&gt;\n",
              "          &lt;midi-player src=data:audio/midi;base64,TVRoZAAAAAYAAQACANxNVHJrAAAAEwD/UQMHoSAA/1gEBAIYCAH/LwBNVHJrAAAAawDAAACQMEAAPEAAQ0AASECBUzwAAEBAajAAADJAaTIAADRAajQAADVAajUAADdAAD5AAEAAAEdAAEgAaUFAAEMAai1AADcAADxAAD4AAEBAAEEAAEVAAEcAaS0AgVRAAGlFAIMnPAAB/y8A sound-font visualizer=&quot;#section394 midi-visualizer&quot;&gt;&lt;/midi-player&gt;\n",
              "          &lt;midi-visualizer src=data:audio/midi;base64,TVRoZAAAAAYAAQACANxNVHJrAAAAEwD/UQMHoSAA/1gEBAIYCAH/LwBNVHJrAAAAawDAAACQMEAAPEAAQ0AASECBUzwAAEBAajAAADJAaTIAADRAajQAADVAajUAADdAAD5AAEAAAEdAAEgAaUFAAEMAai1AADcAADxAAD4AAEBAAEEAAEVAAEcAaS0AgVRAAGlFAIMnPAAB/y8A type=piano-roll&gt;&lt;/midi-visualizer&gt;\n",
              "          &lt;/section&gt;\n",
              "          \" width=\"100%\" height=\"400\"\n",
              "            style=\"border:none !important;\"\n",
              "            \"allowfullscreen\" \"webkitallowfullscreen\" \"mozallowfullscreen\">'\n",
              "            </iframe>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.803051 M parameters in the model\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(\"device is\",device)\n",
        "\n",
        "\n",
        "demo_prompt_idx = 0    # file index from which to pull the demo prompt\n",
        "demo_target = val_notes_tl[demo_prompt_idx]\n",
        "display(midiplayer(demo_target, title=f\"Full demo 'target', {len(demo_target)} notes in length\"))\n",
        "\n",
        "demo_prompt_length = 16  # number of notes in demo prompt context\n",
        "demo_max_new_tokens = min(150, len(demo_target))  # try to make the whole song, but don't go on too long\n",
        "prompt = demo_target[:demo_prompt_length]\n",
        "display(midiplayer(prompt, title=f\"{demo_prompt_length}-note 'prompt' for demos\"))\n",
        "prompt_tokens = encode(prompt).unsqueeze(0).to(device)\n",
        "\n",
        "#  set RNG seeds for reproducibility.\n",
        "def set_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seeds(0)\n",
        "\n",
        "train_ds = NotesDataset(train_notes_tl, config.seq_length, len_mult=config.batch_size)\n",
        "val_ds = NotesDataset(val_notes_tl, seq_length, aug_callback=None, len_mult=1000000) # no aug, neverending\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True)\n",
        "val_dl   = DataLoader(val_ds,   batch_size=config.batch_size, shuffle=False)\n",
        "\n",
        "model = Transformer(config)\n",
        "model = model.to(device)\n",
        "total_params_str = f'{sum(p.numel() for p in model.parameters())/1e6} M'\n",
        "print(total_params_str,'parameters in the model')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W6DPe-acrrYR",
      "metadata": {
        "id": "W6DPe-acrrYR"
      },
      "source": [
        "## Want WandB?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8v6wj6ZZn6cC",
      "metadata": {
        "id": "8v6wj6ZZn6cC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "76a2d23b-1ef8-4ae8-daf8-fe72ebc19aeb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231231_190436-wugwqy5a</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/drscotthawley/musicbox-jsb-tutorial/runs/wugwqy5a' target=\"_blank\">restful-dew-47</a></strong> to <a href='https://wandb.ai/drscotthawley/musicbox-jsb-tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/drscotthawley/musicbox-jsb-tutorial' target=\"_blank\">https://wandb.ai/drscotthawley/musicbox-jsb-tutorial</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/drscotthawley/musicbox-jsb-tutorial/runs/wugwqy5a' target=\"_blank\">https://wandb.ai/drscotthawley/musicbox-jsb-tutorial/runs/wugwqy5a</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# wandb stuff\n",
        "\n",
        "use_wandb = np.true_divide  # Tip: leave this off at first, until you're sure everything's working!\n",
        "if use_wandb:\n",
        "    wandb.login()\n",
        "    wandb.init(project=\"musicbox-jsb-tutorial\", config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45a072b2-9f44-4f4e-988b-bd026a7ab86f",
      "metadata": {
        "id": "45a072b2-9f44-4f4e-988b-bd026a7ab86f"
      },
      "source": [
        "# Training stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "98fcd3fa-4462-4a2b-95f9-12db20c49f81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "98fcd3fa-4462-4a2b-95f9-12db20c49f81",
        "outputId": "22c26b85-1987-44b7-9eb5-55015d2b5456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 1/12:   0%|                                                           | 0/229 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py:2163: UserWarning: Run (l7mcznw1) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
            "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
            "Epoch 2/12:  67%|████████    | 153/229 [04:17<02:07,  1.68s/it, train=0.661, val=1.55, val_ema=1.49]\n",
            "\n",
            "Epoch 1/12:   0%|                         | 0/229 [00:01<?, ?it/s, train=5.74, val=3.7, val_ema=3.7]\u001b[A\n",
            "Epoch 1/12:   0%|                 | 1/229 [00:01<03:53,  1.02s/it, train=5.74, val=3.7, val_ema=3.7]\u001b[A\n",
            "Epoch 1/12:   0%|               | 1/229 [00:01<03:53,  1.02s/it, train=3.43, val=3.33, val_ema=3.68]\u001b[A\n",
            "Epoch 1/12:   1%|▏              | 2/229 [00:01<03:43,  1.02it/s, train=3.43, val=3.33, val_ema=3.68]\u001b[A\n",
            "Epoch 1/12:   1%|▏               | 2/229 [00:02<03:43,  1.02it/s, train=2.9, val=3.15, val_ema=3.65]\u001b[A\n",
            "Epoch 1/12:   1%|▏               | 3/229 [00:02<03:41,  1.02it/s, train=2.9, val=3.15, val_ema=3.65]\u001b[A\n",
            "Epoch 1/12:   1%|▏              | 3/229 [00:03<03:41,  1.02it/s, train=2.71, val=3.07, val_ema=3.62]\u001b[A\n",
            "Epoch 1/12:   2%|▎              | 4/229 [00:03<03:38,  1.03it/s, train=2.71, val=3.07, val_ema=3.62]\u001b[A\n",
            "Epoch 1/12:   2%|▎              | 4/229 [00:04<03:38,  1.03it/s, train=2.59, val=2.85, val_ema=3.58]\u001b[A\n",
            "Epoch 1/12:   2%|▎              | 5/229 [00:04<03:36,  1.04it/s, train=2.59, val=2.85, val_ema=3.58]\u001b[A\n",
            "Epoch 1/12:   2%|▎              | 5/229 [00:05<03:36,  1.04it/s, train=2.53, val=2.84, val_ema=3.55]\u001b[A\n",
            "Epoch 1/12:   3%|▍              | 6/229 [00:05<03:34,  1.04it/s, train=2.53, val=2.84, val_ema=3.55]\u001b[A\n",
            "Epoch 1/12:   3%|▍               | 6/229 [00:06<03:34,  1.04it/s, train=2.43, val=2.66, val_ema=3.5]\u001b[A\n",
            "Epoch 1/12:   3%|▍               | 7/229 [00:06<03:32,  1.04it/s, train=2.43, val=2.66, val_ema=3.5]\u001b[A\n",
            "Epoch 1/12:   3%|▍               | 7/229 [00:07<03:32,  1.04it/s, train=2.29, val=2.7, val_ema=3.46]\u001b[A\n",
            "Epoch 1/12:   3%|▌               | 8/229 [00:07<03:31,  1.04it/s, train=2.29, val=2.7, val_ema=3.46]\u001b[A\n",
            "Epoch 1/12:   3%|▌              | 8/229 [00:08<03:31,  1.04it/s, train=2.23, val=2.71, val_ema=3.43]\u001b[A\n",
            "Epoch 1/12:   4%|▌              | 9/229 [00:08<03:31,  1.04it/s, train=2.23, val=2.71, val_ema=3.43]\u001b[A\n",
            "Epoch 1/12:   4%|▌              | 9/229 [00:09<03:31,  1.04it/s, train=2.35, val=2.65, val_ema=3.39]\u001b[A\n",
            "Epoch 1/12:   4%|▌             | 10/229 [00:09<03:31,  1.04it/s, train=2.35, val=2.65, val_ema=3.39]\u001b[A\n",
            "Epoch 1/12:   4%|▌             | 10/229 [00:10<03:31,  1.04it/s, train=2.28, val=2.63, val_ema=3.35]\u001b[A\n",
            "Epoch 1/12:   5%|▋             | 11/229 [00:10<03:32,  1.03it/s, train=2.28, val=2.63, val_ema=3.35]\u001b[A\n",
            "Epoch 1/12:   5%|▋             | 11/229 [00:11<03:32,  1.03it/s, train=2.16, val=2.58, val_ema=3.31]\u001b[A\n",
            "Epoch 1/12:   5%|▋             | 12/229 [00:11<03:32,  1.02it/s, train=2.16, val=2.58, val_ema=3.31]\u001b[A\n",
            "Epoch 1/12:   5%|▋             | 12/229 [00:12<03:32,  1.02it/s, train=2.14, val=2.52, val_ema=3.27]\u001b[A\n",
            "Epoch 1/12:   6%|▊             | 13/229 [00:12<03:32,  1.02it/s, train=2.14, val=2.52, val_ema=3.27]\u001b[A\n",
            "Epoch 1/12:   6%|▊             | 13/229 [00:13<03:32,  1.02it/s, train=2.04, val=2.53, val_ema=3.23]\u001b[A\n",
            "Epoch 1/12:   6%|▊             | 14/229 [00:13<03:30,  1.02it/s, train=2.04, val=2.53, val_ema=3.23]\u001b[A\n",
            "Epoch 1/12:   6%|▉               | 14/229 [00:14<03:30,  1.02it/s, train=2.07, val=2.6, val_ema=3.2]\u001b[A\n",
            "Epoch 1/12:   7%|█               | 15/229 [00:14<03:29,  1.02it/s, train=2.07, val=2.6, val_ema=3.2]\u001b[A\n",
            "Epoch 1/12:   7%|▉             | 15/229 [00:15<03:29,  1.02it/s, train=2.15, val=2.53, val_ema=3.17]\u001b[A\n",
            "Epoch 1/12:   7%|▉             | 16/229 [00:15<03:27,  1.02it/s, train=2.15, val=2.53, val_ema=3.17]\u001b[A\n",
            "Epoch 1/12:   7%|▉             | 16/229 [00:16<03:27,  1.02it/s, train=2.13, val=2.46, val_ema=3.13]\u001b[A\n",
            "Epoch 1/12:   7%|█             | 17/229 [00:16<03:26,  1.03it/s, train=2.13, val=2.46, val_ema=3.13]\u001b[A\n",
            "Epoch 1/12:   7%|█              | 17/229 [00:17<03:26,  1.03it/s, train=1.97, val=2.51, val_ema=3.1]\u001b[A\n",
            "Epoch 1/12:   8%|█▏             | 18/229 [00:17<03:26,  1.02it/s, train=1.97, val=2.51, val_ema=3.1]\u001b[A\n",
            "Epoch 1/12:   8%|█             | 18/229 [00:18<03:26,  1.02it/s, train=2.02, val=2.51, val_ema=3.07]\u001b[A\n",
            "Epoch 1/12:   8%|█▏            | 19/229 [00:18<03:25,  1.02it/s, train=2.02, val=2.51, val_ema=3.07]\u001b[A\n",
            "Epoch 1/12:   8%|█▏            | 19/229 [00:19<03:25,  1.02it/s, train=2.22, val=2.45, val_ema=3.04]\u001b[A\n",
            "Epoch 1/12:   9%|█▏            | 20/229 [00:19<03:25,  1.02it/s, train=2.22, val=2.45, val_ema=3.04]\u001b[A\n",
            "Epoch 1/12:   9%|█▏            | 20/229 [00:20<03:25,  1.02it/s, train=2.02, val=2.57, val_ema=3.02]\u001b[A\n",
            "Epoch 1/12:   9%|█▎            | 21/229 [00:20<03:24,  1.02it/s, train=2.02, val=2.57, val_ema=3.02]\u001b[A\n",
            "Epoch 1/12:   9%|█▌               | 21/229 [00:21<03:24,  1.02it/s, train=2, val=2.48, val_ema=2.99]\u001b[A\n",
            "Epoch 1/12:  10%|█▋               | 22/229 [00:21<03:23,  1.01it/s, train=2, val=2.48, val_ema=2.99]\u001b[A\n",
            "Epoch 1/12:  10%|█▎            | 22/229 [00:22<03:23,  1.01it/s, train=2.01, val=2.39, val_ema=2.96]\u001b[A\n",
            "Epoch 1/12:  10%|█▍            | 23/229 [00:22<03:23,  1.01it/s, train=2.01, val=2.39, val_ema=2.96]\u001b[A\n",
            "Epoch 1/12:  10%|█▍            | 23/229 [00:23<03:23,  1.01it/s, train=1.99, val=2.43, val_ema=2.93]\u001b[A\n",
            "Epoch 1/12:  10%|█▍            | 24/229 [00:23<03:23,  1.01it/s, train=1.99, val=2.43, val_ema=2.93]\u001b[A\n",
            "Epoch 1/12:  10%|█▍            | 24/229 [00:24<03:23,  1.01it/s, train=1.97, val=2.44, val_ema=2.91]\u001b[A\n",
            "Epoch 1/12:  11%|█▌            | 25/229 [00:24<03:25,  1.01s/it, train=1.97, val=2.44, val_ema=2.91]\u001b[A\n",
            "Epoch 1/12:  11%|█▌            | 25/229 [00:25<03:25,  1.01s/it, train=1.98, val=2.44, val_ema=2.89]\u001b[A\n",
            "Epoch 1/12:  11%|█▌            | 26/229 [00:25<03:24,  1.01s/it, train=1.98, val=2.44, val_ema=2.89]\u001b[A\n",
            "Epoch 1/12:  11%|█▌            | 26/229 [00:26<03:24,  1.01s/it, train=1.96, val=2.52, val_ema=2.87]\u001b[A\n",
            "Epoch 1/12:  12%|█▋            | 27/229 [00:26<03:24,  1.01s/it, train=1.96, val=2.52, val_ema=2.87]\u001b[A\n",
            "Epoch 1/12:  12%|█▋            | 27/229 [00:27<03:24,  1.01s/it, train=2.01, val=2.39, val_ema=2.84]\u001b[A\n",
            "Epoch 1/12:  12%|█▋            | 28/229 [00:27<03:23,  1.01s/it, train=2.01, val=2.39, val_ema=2.84]\u001b[A\n",
            "Epoch 1/12:  12%|█▋            | 28/229 [00:28<03:23,  1.01s/it, train=1.88, val=2.35, val_ema=2.82]\u001b[A\n",
            "Epoch 1/12:  13%|█▊            | 29/229 [00:28<03:21,  1.01s/it, train=1.88, val=2.35, val_ema=2.82]\u001b[A\n",
            "Epoch 1/12:  13%|██              | 29/229 [00:29<03:21,  1.01s/it, train=1.98, val=2.4, val_ema=2.8]\u001b[A\n",
            "Epoch 1/12:  13%|██              | 30/229 [00:29<03:20,  1.01s/it, train=1.98, val=2.4, val_ema=2.8]\u001b[A\n",
            "Epoch 1/12:  13%|█▊            | 30/229 [00:30<03:20,  1.01s/it, train=1.97, val=2.38, val_ema=2.78]\u001b[A\n",
            "Epoch 1/12:  14%|█▉            | 31/229 [00:30<03:20,  1.01s/it, train=1.97, val=2.38, val_ema=2.78]\u001b[A\n",
            "Epoch 1/12:  14%|█▉            | 31/229 [00:31<03:20,  1.01s/it, train=1.96, val=2.32, val_ema=2.75]\u001b[A\n",
            "Epoch 1/12:  14%|█▉            | 32/229 [00:31<03:19,  1.01s/it, train=1.96, val=2.32, val_ema=2.75]\u001b[A\n",
            "Epoch 1/12:  14%|█▉            | 32/229 [00:32<03:19,  1.01s/it, train=1.96, val=2.32, val_ema=2.73]\u001b[A\n",
            "Epoch 1/12:  14%|██            | 33/229 [00:32<03:17,  1.01s/it, train=1.96, val=2.32, val_ema=2.73]\u001b[A\n",
            "Epoch 1/12:  14%|██            | 33/229 [00:33<03:17,  1.01s/it, train=1.81, val=2.43, val_ema=2.72]\u001b[A\n",
            "Epoch 1/12:  15%|██            | 34/229 [00:33<03:16,  1.01s/it, train=1.81, val=2.43, val_ema=2.72]\u001b[A\n",
            "Epoch 1/12:  15%|██            | 34/229 [00:34<03:16,  1.01s/it, train=1.86, val=2.24, val_ema=2.69]\u001b[A\n",
            "Epoch 1/12:  15%|██▏           | 35/229 [00:34<03:15,  1.01s/it, train=1.86, val=2.24, val_ema=2.69]\u001b[A\n",
            "Epoch 1/12:  15%|██▏           | 35/229 [00:35<03:15,  1.01s/it, train=1.92, val=2.23, val_ema=2.67]\u001b[A\n",
            "Epoch 1/12:  16%|██▏           | 36/229 [00:35<03:15,  1.01s/it, train=1.92, val=2.23, val_ema=2.67]\u001b[A\n",
            "Epoch 1/12:  16%|██▏           | 36/229 [00:36<03:15,  1.01s/it, train=1.88, val=2.38, val_ema=2.66]\u001b[A\n",
            "Epoch 1/12:  16%|██▎           | 37/229 [00:36<03:13,  1.01s/it, train=1.88, val=2.38, val_ema=2.66]\u001b[A\n",
            "Epoch 1/12:  16%|██▎           | 37/229 [00:37<03:13,  1.01s/it, train=1.85, val=2.26, val_ema=2.64]\u001b[A\n",
            "Epoch 1/12:  17%|██▎           | 38/229 [00:37<03:12,  1.01s/it, train=1.85, val=2.26, val_ema=2.64]\u001b[A\n",
            "Epoch 1/12:  17%|██▎           | 38/229 [00:38<03:12,  1.01s/it, train=1.87, val=2.26, val_ema=2.62]\u001b[A\n",
            "Epoch 1/12:  17%|██▍           | 39/229 [00:38<03:10,  1.00s/it, train=1.87, val=2.26, val_ema=2.62]\u001b[A\n",
            "Epoch 1/12:  17%|██▌            | 39/229 [00:39<03:10,  1.00s/it, train=1.96, val=2.27, val_ema=2.6]\u001b[A\n",
            "Epoch 1/12:  17%|██▌            | 40/229 [00:39<03:08,  1.00it/s, train=1.96, val=2.27, val_ema=2.6]\u001b[A\n",
            "Epoch 1/12:  17%|██▍           | 40/229 [00:40<03:08,  1.00it/s, train=1.85, val=2.25, val_ema=2.58]\u001b[A\n",
            "Epoch 1/12:  18%|██▌           | 41/229 [00:40<03:06,  1.01it/s, train=1.85, val=2.25, val_ema=2.58]\u001b[A\n",
            "Epoch 1/12:  18%|██▌           | 41/229 [00:41<03:06,  1.01it/s, train=1.81, val=2.25, val_ema=2.57]\u001b[A\n",
            "Epoch 1/12:  18%|██▌           | 42/229 [00:41<03:04,  1.01it/s, train=1.81, val=2.25, val_ema=2.57]\u001b[A\n",
            "Epoch 1/12:  18%|██▌           | 42/229 [00:42<03:04,  1.01it/s, train=1.86, val=2.26, val_ema=2.55]\u001b[A\n",
            "Epoch 1/12:  19%|██▋           | 43/229 [00:42<03:02,  1.02it/s, train=1.86, val=2.26, val_ema=2.55]\u001b[A\n",
            "Epoch 1/12:  19%|██▊            | 43/229 [00:43<03:02,  1.02it/s, train=1.78, val=2.2, val_ema=2.53]\u001b[A\n",
            "Epoch 1/12:  19%|██▉            | 44/229 [00:43<03:01,  1.02it/s, train=1.78, val=2.2, val_ema=2.53]\u001b[A\n",
            "Epoch 1/12:  19%|██▋           | 44/229 [00:44<03:01,  1.02it/s, train=1.73, val=2.21, val_ema=2.52]\u001b[A\n",
            "Epoch 1/12:  20%|██▊           | 45/229 [00:44<03:00,  1.02it/s, train=1.73, val=2.21, val_ema=2.52]\u001b[A\n",
            "Epoch 1/12:  20%|███▏            | 45/229 [00:45<03:00,  1.02it/s, train=1.8, val=2.25, val_ema=2.5]\u001b[A\n",
            "Epoch 1/12:  20%|███▏            | 46/229 [00:45<02:58,  1.02it/s, train=1.8, val=2.25, val_ema=2.5]\u001b[A\n",
            "Epoch 1/12:  20%|███            | 46/229 [00:46<02:58,  1.02it/s, train=1.8, val=2.23, val_ema=2.49]\u001b[A\n",
            "Epoch 1/12:  21%|███            | 47/229 [00:46<02:57,  1.03it/s, train=1.8, val=2.23, val_ema=2.49]\u001b[A\n",
            "Epoch 1/12:  21%|██▊           | 47/229 [00:47<02:57,  1.03it/s, train=1.78, val=2.18, val_ema=2.47]\u001b[A\n",
            "Epoch 1/12:  21%|██▉           | 48/229 [00:47<02:56,  1.02it/s, train=1.78, val=2.18, val_ema=2.47]\u001b[A\n",
            "Epoch 1/12:  21%|██▉           | 48/229 [00:48<02:56,  1.02it/s, train=1.75, val=2.17, val_ema=2.46]\u001b[A\n",
            "Epoch 1/12:  21%|██▉           | 49/229 [00:48<02:56,  1.02it/s, train=1.75, val=2.17, val_ema=2.46]\u001b[A\n",
            "Epoch 1/12:  21%|██▉           | 49/229 [00:49<02:56,  1.02it/s, train=1.69, val=2.21, val_ema=2.45]\u001b[A\n",
            "Epoch 1/12:  22%|███           | 50/229 [00:49<02:56,  1.02it/s, train=1.69, val=2.21, val_ema=2.45]\u001b[A\n",
            "Epoch 1/12:  22%|███           | 50/229 [00:50<02:56,  1.02it/s, train=1.76, val=2.07, val_ema=2.43]\u001b[A\n",
            "Epoch 1/12:  22%|███           | 51/229 [00:50<02:55,  1.02it/s, train=1.76, val=2.07, val_ema=2.43]\u001b[A\n",
            "Epoch 1/12:  22%|███           | 51/229 [00:51<02:55,  1.02it/s, train=1.78, val=2.14, val_ema=2.41]\u001b[A\n",
            "Epoch 1/12:  23%|███▏          | 52/229 [00:51<02:52,  1.02it/s, train=1.78, val=2.14, val_ema=2.41]\u001b[A\n",
            "Epoch 1/12:  23%|███▍           | 52/229 [00:52<02:52,  1.02it/s, train=1.71, val=2.11, val_ema=2.4]\u001b[A\n",
            "Epoch 1/12:  23%|███▍           | 53/229 [00:52<02:51,  1.03it/s, train=1.71, val=2.11, val_ema=2.4]\u001b[A\n",
            "Epoch 1/12:  23%|███▍           | 53/229 [00:53<02:51,  1.03it/s, train=1.7, val=2.06, val_ema=2.38]\u001b[A\n",
            "Epoch 1/12:  24%|███▌           | 54/229 [00:53<02:50,  1.03it/s, train=1.7, val=2.06, val_ema=2.38]\u001b[A\n",
            "Epoch 1/12:  24%|███▎          | 54/229 [00:54<02:50,  1.03it/s, train=1.66, val=2.09, val_ema=2.37]\u001b[A\n",
            "Epoch 1/12:  24%|███▎          | 55/229 [00:54<02:48,  1.03it/s, train=1.66, val=2.09, val_ema=2.37]\u001b[A\n",
            "Epoch 1/12:  24%|███▎          | 55/229 [00:55<02:48,  1.03it/s, train=1.72, val=2.11, val_ema=2.35]\u001b[A\n",
            "Epoch 1/12:  24%|███▍          | 56/229 [00:55<02:47,  1.04it/s, train=1.72, val=2.11, val_ema=2.35]\u001b[A\n",
            "Epoch 1/12:  24%|███▍          | 56/229 [00:56<02:47,  1.04it/s, train=1.71, val=2.05, val_ema=2.34]\u001b[A\n",
            "Epoch 1/12:  25%|███▍          | 57/229 [00:56<02:45,  1.04it/s, train=1.71, val=2.05, val_ema=2.34]\u001b[A\n",
            "Epoch 1/12:  25%|███▍          | 57/229 [00:57<02:45,  1.04it/s, train=1.67, val=2.08, val_ema=2.33]\u001b[A\n",
            "Epoch 1/12:  25%|███▌          | 58/229 [00:57<02:43,  1.04it/s, train=1.67, val=2.08, val_ema=2.33]\u001b[A\n",
            "Epoch 1/12:  25%|███▊           | 58/229 [00:58<02:43,  1.04it/s, train=1.7, val=2.16, val_ema=2.32]\u001b[A\n",
            "Epoch 1/12:  26%|███▊           | 59/229 [00:58<02:42,  1.05it/s, train=1.7, val=2.16, val_ema=2.32]\u001b[A\n",
            "Epoch 1/12:  26%|███▌          | 59/229 [00:58<02:42,  1.05it/s, train=1.62, val=2.08, val_ema=2.31]\u001b[A\n",
            "Epoch 1/12:  26%|███▋          | 60/229 [00:59<02:41,  1.05it/s, train=1.62, val=2.08, val_ema=2.31]\u001b[A\n",
            "Epoch 1/12:  26%|███▋          | 60/229 [00:59<02:41,  1.05it/s, train=1.68, val=2.04, val_ema=2.29]\u001b[A\n",
            "Epoch 1/12:  27%|███▋          | 61/229 [00:59<02:40,  1.05it/s, train=1.68, val=2.04, val_ema=2.29]\u001b[A\n",
            "Epoch 1/12:  27%|███▋          | 61/229 [01:00<02:40,  1.05it/s, train=1.64, val=2.07, val_ema=2.28]\u001b[A\n",
            "Epoch 1/12:  27%|███▊          | 62/229 [01:00<02:40,  1.04it/s, train=1.64, val=2.07, val_ema=2.28]\u001b[A\n",
            "Epoch 1/12:  27%|████▌            | 62/229 [01:01<02:40,  1.04it/s, train=1.63, val=2, val_ema=2.27]\u001b[A\n",
            "Epoch 1/12:  28%|████▋            | 63/229 [01:01<02:39,  1.04it/s, train=1.63, val=2, val_ema=2.27]\u001b[A\n",
            "Epoch 1/12:  28%|████▋            | 63/229 [01:02<02:39,  1.04it/s, train=1.62, val=2, val_ema=2.25]\u001b[A\n",
            "Epoch 1/12:  28%|████▊            | 64/229 [01:02<02:38,  1.04it/s, train=1.62, val=2, val_ema=2.25]\u001b[A\n",
            "Epoch 1/12:  28%|███▉          | 64/229 [01:03<02:38,  1.04it/s, train=1.59, val=2.02, val_ema=2.24]\u001b[A\n",
            "Epoch 1/12:  28%|███▉          | 65/229 [01:03<02:36,  1.05it/s, train=1.59, val=2.02, val_ema=2.24]\u001b[A\n",
            "Epoch 1/12:  28%|███▉          | 65/229 [01:04<02:36,  1.05it/s, train=1.52, val=2.02, val_ema=2.23]\u001b[A\n",
            "Epoch 1/12:  29%|████          | 66/229 [01:04<02:35,  1.05it/s, train=1.52, val=2.02, val_ema=2.23]\u001b[A\n",
            "Epoch 1/12:  29%|████          | 66/229 [01:05<02:35,  1.05it/s, train=1.59, val=2.07, val_ema=2.22]\u001b[A\n",
            "Epoch 1/12:  29%|████          | 67/229 [01:05<02:34,  1.05it/s, train=1.59, val=2.07, val_ema=2.22]\u001b[A\n",
            "Epoch 1/12:  29%|████          | 67/229 [01:06<02:34,  1.05it/s, train=1.64, val=1.95, val_ema=2.21]\u001b[A\n",
            "Epoch 1/12:  30%|████▏         | 68/229 [01:06<02:33,  1.05it/s, train=1.64, val=1.95, val_ema=2.21]\u001b[A\n",
            "Epoch 1/12:  30%|████▍          | 68/229 [01:07<02:33,  1.05it/s, train=1.57, val=1.98, val_ema=2.2]\u001b[A\n",
            "Epoch 1/12:  30%|████▌          | 69/229 [01:07<02:32,  1.05it/s, train=1.57, val=1.98, val_ema=2.2]\u001b[A\n",
            "Epoch 1/12:  30%|████▏         | 69/229 [01:08<02:32,  1.05it/s, train=1.64, val=2.09, val_ema=2.19]\u001b[A\n",
            "Epoch 1/12:  31%|████▎         | 70/229 [01:08<02:31,  1.05it/s, train=1.64, val=2.09, val_ema=2.19]\u001b[A\n",
            "Epoch 1/12:  31%|████▎         | 70/229 [01:09<02:31,  1.05it/s, train=1.66, val=2.01, val_ema=2.18]\u001b[A\n",
            "Epoch 1/12:  31%|████▎         | 71/229 [01:09<02:30,  1.05it/s, train=1.66, val=2.01, val_ema=2.18]\u001b[A\n",
            "Epoch 1/12:  31%|█████▎           | 71/229 [01:10<02:30,  1.05it/s, train=1.62, val=2, val_ema=2.17]\u001b[A\n",
            "Epoch 1/12:  31%|█████▎           | 72/229 [01:10<02:28,  1.06it/s, train=1.62, val=2, val_ema=2.17]\u001b[A\n",
            "Epoch 1/12:  31%|████▍         | 72/229 [01:11<02:28,  1.06it/s, train=1.59, val=1.97, val_ema=2.16]\u001b[A\n",
            "Epoch 1/12:  32%|████▍         | 73/229 [01:11<02:27,  1.06it/s, train=1.59, val=1.97, val_ema=2.16]\u001b[A\n",
            "Epoch 1/12:  32%|████▍         | 73/229 [01:12<02:27,  1.06it/s, train=1.51, val=2.01, val_ema=2.16]\u001b[A\n",
            "Epoch 1/12:  32%|████▌         | 74/229 [01:12<02:26,  1.06it/s, train=1.51, val=2.01, val_ema=2.16]\u001b[A\n",
            "Epoch 1/12:  32%|████▌         | 74/229 [01:13<02:26,  1.06it/s, train=1.52, val=1.94, val_ema=2.15]\u001b[A\n",
            "Epoch 1/12:  33%|████▌         | 75/229 [01:13<02:26,  1.05it/s, train=1.52, val=1.94, val_ema=2.15]\u001b[A\n",
            "Epoch 1/12:  33%|████▌         | 75/229 [01:14<02:26,  1.05it/s, train=1.58, val=2.01, val_ema=2.14]\u001b[A\n",
            "Epoch 1/12:  33%|████▋         | 76/229 [01:14<02:25,  1.05it/s, train=1.58, val=2.01, val_ema=2.14]\u001b[A\n",
            "Epoch 1/12:  33%|████▋         | 76/229 [01:15<02:25,  1.05it/s, train=1.65, val=1.91, val_ema=2.13]\u001b[A\n",
            "Epoch 1/12:  34%|████▋         | 77/229 [01:15<02:25,  1.04it/s, train=1.65, val=1.91, val_ema=2.13]\u001b[A\n",
            "Epoch 1/12:  34%|████▋         | 77/229 [01:16<02:25,  1.04it/s, train=1.49, val=1.92, val_ema=2.12]\u001b[A\n",
            "Epoch 1/12:  34%|████▊         | 78/229 [01:16<02:25,  1.04it/s, train=1.49, val=1.92, val_ema=2.12]\u001b[A\n",
            "Epoch 1/12:  34%|████▊         | 78/229 [01:17<02:25,  1.04it/s, train=1.51, val=2.02, val_ema=2.11]\u001b[A\n",
            "Epoch 1/12:  34%|████▊         | 79/229 [01:17<02:23,  1.04it/s, train=1.51, val=2.02, val_ema=2.11]\u001b[A\n",
            "Epoch 1/12:  34%|█████▏         | 79/229 [01:18<02:23,  1.04it/s, train=1.62, val=1.86, val_ema=2.1]\u001b[A\n",
            "Epoch 1/12:  35%|█████▏         | 80/229 [01:18<02:23,  1.04it/s, train=1.62, val=1.86, val_ema=2.1]\u001b[A\n",
            "Epoch 1/12:  35%|█████▏         | 80/229 [01:19<02:23,  1.04it/s, train=1.4, val=1.87, val_ema=2.09]\u001b[A\n",
            "Epoch 1/12:  35%|█████▎         | 81/229 [01:19<02:21,  1.04it/s, train=1.4, val=1.87, val_ema=2.09]\u001b[A\n",
            "Epoch 1/12:  35%|████▉         | 81/229 [01:19<02:21,  1.04it/s, train=1.46, val=1.86, val_ema=2.08]\u001b[A\n",
            "Epoch 1/12:  36%|█████         | 82/229 [01:20<02:20,  1.05it/s, train=1.46, val=1.86, val_ema=2.08]\u001b[A\n",
            "Epoch 1/12:  36%|█████         | 82/229 [01:20<02:20,  1.05it/s, train=1.51, val=1.92, val_ema=2.07]\u001b[A\n",
            "Epoch 1/12:  36%|█████         | 83/229 [01:20<02:19,  1.05it/s, train=1.51, val=1.92, val_ema=2.07]\u001b[A\n",
            "Epoch 1/12:  36%|█████         | 83/229 [01:21<02:19,  1.05it/s, train=1.42, val=1.85, val_ema=2.06]\u001b[A\n",
            "Epoch 1/12:  37%|█████▏        | 84/229 [01:21<02:18,  1.05it/s, train=1.42, val=1.85, val_ema=2.06]\u001b[A\n",
            "Epoch 1/12:  37%|█████▏        | 84/229 [01:22<02:18,  1.05it/s, train=1.42, val=1.88, val_ema=2.05]\u001b[A\n",
            "Epoch 1/12:  37%|█████▏        | 85/229 [01:22<02:17,  1.05it/s, train=1.42, val=1.88, val_ema=2.05]\u001b[A\n",
            "Epoch 1/12:  37%|█████▏        | 85/229 [01:23<02:17,  1.05it/s, train=1.48, val=1.84, val_ema=2.04]\u001b[A\n",
            "Epoch 1/12:  38%|█████▎        | 86/229 [01:23<02:16,  1.05it/s, train=1.48, val=1.84, val_ema=2.04]\u001b[A\n",
            "Epoch 1/12:  38%|█████▋         | 86/229 [01:24<02:16,  1.05it/s, train=1.42, val=1.8, val_ema=2.03]\u001b[A\n",
            "Epoch 1/12:  38%|█████▋         | 87/229 [01:24<02:15,  1.05it/s, train=1.42, val=1.8, val_ema=2.03]\u001b[A\n",
            "Epoch 1/12:  38%|█████▎        | 87/229 [01:25<02:15,  1.05it/s, train=1.49, val=1.83, val_ema=2.02]\u001b[A\n",
            "Epoch 1/12:  38%|█████▍        | 88/229 [01:25<02:14,  1.05it/s, train=1.49, val=1.83, val_ema=2.02]\u001b[A\n",
            "Epoch 1/12:  38%|██████▌          | 88/229 [01:26<02:14,  1.05it/s, train=1.42, val=1.75, val_ema=2]\u001b[A\n",
            "Epoch 1/12:  39%|██████▌          | 89/229 [01:26<02:13,  1.05it/s, train=1.42, val=1.75, val_ema=2]\u001b[A\n",
            "Epoch 1/12:  39%|█████▍        | 89/229 [01:27<02:13,  1.05it/s, train=1.43, val=1.79, val_ema=1.99]\u001b[A\n",
            "Epoch 1/12:  39%|█████▌        | 90/229 [01:27<02:13,  1.04it/s, train=1.43, val=1.79, val_ema=1.99]\u001b[A\n",
            "Epoch 1/12:  39%|█████▌        | 90/229 [01:28<02:13,  1.04it/s, train=1.43, val=1.78, val_ema=1.98]\u001b[A\n",
            "Epoch 1/12:  40%|█████▌        | 91/229 [01:28<02:13,  1.04it/s, train=1.43, val=1.78, val_ema=1.98]\u001b[A\n",
            "Epoch 1/12:  40%|██████▎         | 91/229 [01:29<02:13,  1.04it/s, train=1.5, val=1.8, val_ema=1.97]\u001b[A\n",
            "Epoch 1/12:  40%|██████▍         | 92/229 [01:29<02:12,  1.03it/s, train=1.5, val=1.8, val_ema=1.97]\u001b[A\n",
            "Epoch 1/12:  40%|█████▌        | 92/229 [01:30<02:12,  1.03it/s, train=1.45, val=1.77, val_ema=1.96]\u001b[A\n",
            "Epoch 1/12:  41%|█████▋        | 93/229 [01:30<02:11,  1.04it/s, train=1.45, val=1.77, val_ema=1.96]\u001b[A\n",
            "Epoch 1/12:  41%|█████▋        | 93/229 [01:31<02:11,  1.04it/s, train=1.34, val=1.83, val_ema=1.96]\u001b[A\n",
            "Epoch 1/12:  41%|█████▋        | 94/229 [01:31<02:09,  1.04it/s, train=1.34, val=1.83, val_ema=1.96]\u001b[A\n",
            "Epoch 1/12:  41%|██████▏        | 94/229 [01:32<02:09,  1.04it/s, train=1.33, val=1.8, val_ema=1.95]\u001b[A\n",
            "Epoch 1/12:  41%|██████▏        | 95/229 [01:32<02:08,  1.05it/s, train=1.33, val=1.8, val_ema=1.95]\u001b[A\n",
            "Epoch 1/12:  41%|██████▏        | 95/229 [01:33<02:08,  1.05it/s, train=1.4, val=1.69, val_ema=1.94]\u001b[A\n",
            "Epoch 1/12:  42%|██████▎        | 96/229 [01:33<02:07,  1.05it/s, train=1.4, val=1.69, val_ema=1.94]\u001b[A\n",
            "Epoch 1/12:  42%|█████▊        | 96/229 [01:34<02:07,  1.05it/s, train=1.34, val=1.85, val_ema=1.93]\u001b[A\n",
            "Epoch 1/12:  42%|█████▉        | 97/229 [01:34<02:06,  1.04it/s, train=1.34, val=1.85, val_ema=1.93]\u001b[A\n",
            "Epoch 1/12:  42%|█████▉        | 97/229 [01:35<02:06,  1.04it/s, train=1.44, val=1.81, val_ema=1.92]\u001b[A\n",
            "Epoch 1/12:  43%|█████▉        | 98/229 [01:35<02:05,  1.04it/s, train=1.44, val=1.81, val_ema=1.92]\u001b[A\n",
            "Epoch 1/12:  43%|█████▉        | 98/229 [01:36<02:05,  1.04it/s, train=1.38, val=1.78, val_ema=1.92]\u001b[A\n",
            "Epoch 1/12:  43%|██████        | 99/229 [01:36<02:04,  1.04it/s, train=1.38, val=1.78, val_ema=1.92]\u001b[A\n",
            "Epoch 1/12:  43%|██████        | 99/229 [01:37<02:04,  1.04it/s, train=1.42, val=1.72, val_ema=1.91]\u001b[A\n",
            "Epoch 1/12:  44%|█████▋       | 100/229 [01:37<02:03,  1.04it/s, train=1.42, val=1.72, val_ema=1.91]\u001b[A\n",
            "Epoch 1/12:  44%|██████        | 100/229 [01:38<02:03,  1.04it/s, train=1.38, val=1.78, val_ema=1.9]\u001b[A\n",
            "Epoch 1/12:  44%|██████▏       | 101/229 [01:38<02:03,  1.04it/s, train=1.38, val=1.78, val_ema=1.9]\u001b[A\n",
            "Epoch 1/12:  44%|██████▌        | 101/229 [01:39<02:03,  1.04it/s, train=1.41, val=1.8, val_ema=1.9]\u001b[A\n",
            "Epoch 1/12:  45%|██████▋        | 102/229 [01:39<02:02,  1.04it/s, train=1.41, val=1.8, val_ema=1.9]\u001b[A\n",
            "Epoch 1/12:  45%|█████▊       | 102/229 [01:40<02:02,  1.04it/s, train=1.43, val=1.74, val_ema=1.89]\u001b[A\n",
            "Epoch 1/12:  45%|█████▊       | 103/229 [01:40<02:02,  1.03it/s, train=1.43, val=1.74, val_ema=1.89]\u001b[A\n",
            "Epoch 1/12:  45%|██████▎       | 103/229 [01:41<02:02,  1.03it/s, train=1.4, val=1.71, val_ema=1.88]\u001b[A\n",
            "Epoch 1/12:  45%|██████▎       | 104/229 [01:41<02:02,  1.02it/s, train=1.4, val=1.71, val_ema=1.88]\u001b[A\n",
            "Epoch 1/12:  45%|██████▎       | 104/229 [01:42<02:02,  1.02it/s, train=1.35, val=1.8, val_ema=1.88]\u001b[A\n",
            "Epoch 1/12:  46%|██████▍       | 105/229 [01:42<02:01,  1.02it/s, train=1.35, val=1.8, val_ema=1.88]\u001b[A\n",
            "Epoch 1/12:  46%|█████▉       | 105/229 [01:43<02:01,  1.02it/s, train=1.37, val=1.85, val_ema=1.87]\u001b[A\n",
            "Epoch 1/12:  46%|██████       | 106/229 [01:43<02:00,  1.02it/s, train=1.37, val=1.85, val_ema=1.87]\u001b[A\n",
            "Epoch 1/12:  46%|██████▍       | 106/229 [01:44<02:00,  1.02it/s, train=1.46, val=1.8, val_ema=1.87]\u001b[A\n",
            "Epoch 1/12:  47%|██████▌       | 107/229 [01:44<01:58,  1.03it/s, train=1.46, val=1.8, val_ema=1.87]\u001b[A\n",
            "Epoch 1/12:  47%|██████       | 107/229 [01:45<01:58,  1.03it/s, train=1.36, val=1.79, val_ema=1.87]\u001b[A\n",
            "Epoch 1/12:  47%|██████▏      | 108/229 [01:45<01:57,  1.03it/s, train=1.36, val=1.79, val_ema=1.87]\u001b[A\n",
            "Epoch 1/12:  47%|██████▏      | 108/229 [01:46<01:57,  1.03it/s, train=1.39, val=1.72, val_ema=1.86]\u001b[A\n",
            "Epoch 1/12:  48%|██████▏      | 109/229 [01:46<01:56,  1.03it/s, train=1.39, val=1.72, val_ema=1.86]\u001b[A\n",
            "Epoch 1/12:  48%|██████▏      | 109/229 [01:47<01:56,  1.03it/s, train=1.39, val=1.79, val_ema=1.86]\u001b[A\n",
            "Epoch 1/12:  48%|██████▏      | 110/229 [01:47<01:55,  1.03it/s, train=1.39, val=1.79, val_ema=1.86]\u001b[A\n",
            "Epoch 1/12:  48%|██████▏      | 110/229 [01:47<01:55,  1.03it/s, train=1.32, val=1.68, val_ema=1.85]\u001b[A\n",
            "Epoch 1/12:  48%|██████▎      | 111/229 [01:47<01:54,  1.03it/s, train=1.32, val=1.68, val_ema=1.85]\u001b[A\n",
            "Epoch 1/12:  48%|██████▊       | 111/229 [01:48<01:54,  1.03it/s, train=1.36, val=1.7, val_ema=1.84]\u001b[A\n",
            "Epoch 1/12:  49%|██████▊       | 112/229 [01:48<01:53,  1.03it/s, train=1.36, val=1.7, val_ema=1.84]\u001b[A\n",
            "Epoch 1/12:  49%|██████▎      | 112/229 [01:49<01:53,  1.03it/s, train=1.31, val=1.67, val_ema=1.83]\u001b[A\n",
            "Epoch 1/12:  49%|██████▍      | 113/229 [01:49<01:53,  1.03it/s, train=1.31, val=1.67, val_ema=1.83]\u001b[A\n",
            "Epoch 1/12:  49%|██████▍      | 113/229 [01:50<01:53,  1.03it/s, train=1.33, val=1.74, val_ema=1.83]\u001b[A\n",
            "Epoch 1/12:  50%|██████▍      | 114/229 [01:50<01:52,  1.02it/s, train=1.33, val=1.74, val_ema=1.83]\u001b[A\n",
            "Epoch 1/12:  50%|██████▍      | 114/229 [01:51<01:52,  1.02it/s, train=1.36, val=1.79, val_ema=1.82]\u001b[A\n",
            "Epoch 1/12:  50%|██████▌      | 115/229 [01:51<01:51,  1.03it/s, train=1.36, val=1.79, val_ema=1.82]\u001b[A\n",
            "Epoch 1/12:  50%|██████▌      | 115/229 [01:52<01:51,  1.03it/s, train=1.37, val=1.73, val_ema=1.82]\u001b[A\n",
            "Epoch 1/12:  51%|██████▌      | 116/229 [01:52<01:50,  1.02it/s, train=1.37, val=1.73, val_ema=1.82]\u001b[A\n",
            "Epoch 1/12:  51%|██████▌      | 116/229 [01:53<01:50,  1.02it/s, train=1.33, val=1.77, val_ema=1.82]\u001b[A\n",
            "Epoch 1/12:  51%|██████▋      | 117/229 [01:53<01:50,  1.02it/s, train=1.33, val=1.77, val_ema=1.82]\u001b[A\n",
            "Epoch 1/12:  51%|███████▏      | 117/229 [01:54<01:50,  1.02it/s, train=1.29, val=1.7, val_ema=1.81]\u001b[A\n",
            "Epoch 1/12:  52%|███████▏      | 118/229 [01:54<01:49,  1.01it/s, train=1.29, val=1.7, val_ema=1.81]\u001b[A\n",
            "Epoch 1/12:  52%|███████▏      | 118/229 [01:55<01:49,  1.01it/s, train=1.32, val=1.69, val_ema=1.8]\u001b[A\n",
            "Epoch 1/12:  52%|███████▎      | 119/229 [01:55<01:47,  1.02it/s, train=1.32, val=1.69, val_ema=1.8]\u001b[A\n",
            "Epoch 1/12:  52%|███████▎      | 119/229 [01:56<01:47,  1.02it/s, train=1.34, val=1.66, val_ema=1.8]\u001b[A\n",
            "Epoch 1/12:  52%|███████▎      | 120/229 [01:56<01:46,  1.02it/s, train=1.34, val=1.66, val_ema=1.8]\u001b[A\n",
            "Epoch 1/12:  52%|██████▊      | 120/229 [01:57<01:46,  1.02it/s, train=1.29, val=1.66, val_ema=1.79]\u001b[A\n",
            "Epoch 1/12:  53%|██████▊      | 121/229 [01:57<01:45,  1.02it/s, train=1.29, val=1.66, val_ema=1.79]\u001b[A\n",
            "Epoch 1/12:  53%|██████▊      | 121/229 [01:58<01:45,  1.02it/s, train=1.28, val=1.57, val_ema=1.78]\u001b[A\n",
            "Epoch 1/12:  53%|██████▉      | 122/229 [01:58<01:44,  1.03it/s, train=1.28, val=1.57, val_ema=1.78]\u001b[A\n",
            "Epoch 1/12:  53%|██████▉      | 122/229 [01:59<01:44,  1.03it/s, train=1.24, val=1.65, val_ema=1.77]\u001b[A\n",
            "Epoch 1/12:  54%|██████▉      | 123/229 [01:59<01:43,  1.03it/s, train=1.24, val=1.65, val_ema=1.77]\u001b[A\n",
            "Epoch 1/12:  54%|██████▉      | 123/229 [02:00<01:43,  1.03it/s, train=1.28, val=1.72, val_ema=1.77]\u001b[A\n",
            "Epoch 1/12:  54%|███████      | 124/229 [02:00<01:42,  1.03it/s, train=1.28, val=1.72, val_ema=1.77]\u001b[A\n",
            "Epoch 1/12:  54%|███████      | 124/229 [02:01<01:42,  1.03it/s, train=1.23, val=1.68, val_ema=1.77]\u001b[A\n",
            "Epoch 1/12:  55%|███████      | 125/229 [02:01<01:41,  1.03it/s, train=1.23, val=1.68, val_ema=1.77]\u001b[A\n",
            "Epoch 1/12:  55%|███████      | 125/229 [02:02<01:41,  1.03it/s, train=1.28, val=1.64, val_ema=1.76]\u001b[A\n",
            "Epoch 1/12:  55%|███████▏     | 126/229 [02:02<01:40,  1.03it/s, train=1.28, val=1.64, val_ema=1.76]\u001b[A\n",
            "Epoch 1/12:  55%|███████▏     | 126/229 [02:03<01:40,  1.03it/s, train=1.26, val=1.67, val_ema=1.76]\u001b[A\n",
            "Epoch 1/12:  55%|███████▏     | 127/229 [02:03<01:39,  1.03it/s, train=1.26, val=1.67, val_ema=1.76]\u001b[A\n",
            "Epoch 1/12:  55%|███████▊      | 127/229 [02:04<01:39,  1.03it/s, train=1.28, val=1.7, val_ema=1.75]\u001b[A\n",
            "Epoch 1/12:  56%|███████▊      | 128/229 [02:04<01:38,  1.03it/s, train=1.28, val=1.7, val_ema=1.75]\u001b[A\n",
            "Epoch 1/12:  56%|███████▎     | 128/229 [02:05<01:38,  1.03it/s, train=1.28, val=1.75, val_ema=1.75]\u001b[A\n",
            "Epoch 1/12:  56%|███████▎     | 129/229 [02:05<01:37,  1.03it/s, train=1.28, val=1.75, val_ema=1.75]\u001b[A\n",
            "Epoch 1/12:  56%|███████▎     | 129/229 [02:06<01:37,  1.03it/s, train=1.27, val=1.62, val_ema=1.75]\u001b[A\n",
            "Epoch 1/12:  57%|███████▍     | 130/229 [02:06<01:36,  1.03it/s, train=1.27, val=1.62, val_ema=1.75]\u001b[A\n",
            "Epoch 1/12:  57%|███████▉      | 130/229 [02:07<01:36,  1.03it/s, train=1.3, val=1.65, val_ema=1.74]\u001b[A\n",
            "Epoch 1/12:  57%|████████      | 131/229 [02:07<01:35,  1.02it/s, train=1.3, val=1.65, val_ema=1.74]\u001b[A\n",
            "Epoch 1/12:  57%|███████▍     | 131/229 [02:08<01:35,  1.02it/s, train=1.18, val=1.61, val_ema=1.73]\u001b[A\n",
            "Epoch 1/12:  58%|███████▍     | 132/229 [02:08<01:36,  1.01it/s, train=1.18, val=1.61, val_ema=1.73]\u001b[A\n",
            "Epoch 1/12:  58%|███████▍     | 132/229 [02:09<01:36,  1.01it/s, train=1.25, val=1.66, val_ema=1.73]\u001b[A\n",
            "Epoch 1/12:  58%|███████▌     | 133/229 [02:09<01:34,  1.02it/s, train=1.25, val=1.66, val_ema=1.73]\u001b[A\n",
            "Epoch 1/12:  58%|████████▏     | 133/229 [02:10<01:34,  1.02it/s, train=1.2, val=1.62, val_ema=1.73]\u001b[A\n",
            "Epoch 1/12:  59%|████████▏     | 134/229 [02:10<01:32,  1.02it/s, train=1.2, val=1.62, val_ema=1.73]\u001b[A\n",
            "Epoch 1/12:  59%|████████▊      | 134/229 [02:11<01:32,  1.02it/s, train=1.2, val=1.7, val_ema=1.72]\u001b[A\n",
            "Epoch 1/12:  59%|████████▊      | 135/229 [02:11<01:31,  1.02it/s, train=1.2, val=1.7, val_ema=1.72]\u001b[A\n",
            "Epoch 1/12:  59%|███████▋     | 135/229 [02:12<01:31,  1.02it/s, train=1.21, val=1.69, val_ema=1.72]\u001b[A\n",
            "Epoch 1/12:  59%|███████▋     | 136/229 [02:12<01:30,  1.03it/s, train=1.21, val=1.69, val_ema=1.72]\u001b[A\n",
            "Epoch 1/12:  59%|███████▋     | 136/229 [02:13<01:30,  1.03it/s, train=1.21, val=1.56, val_ema=1.71]\u001b[A\n",
            "Epoch 1/12:  60%|███████▊     | 137/229 [02:13<01:29,  1.03it/s, train=1.21, val=1.56, val_ema=1.71]\u001b[A\n",
            "Epoch 1/12:  60%|███████▊     | 137/229 [02:14<01:29,  1.03it/s, train=1.25, val=1.69, val_ema=1.71]\u001b[A\n",
            "Epoch 1/12:  60%|███████▊     | 138/229 [02:14<01:28,  1.03it/s, train=1.25, val=1.69, val_ema=1.71]\u001b[A\n",
            "Epoch 1/12:  60%|███████▊     | 138/229 [02:15<01:28,  1.03it/s, train=1.24, val=1.58, val_ema=1.71]\u001b[A\n",
            "Epoch 1/12:  61%|███████▉     | 139/229 [02:15<01:27,  1.03it/s, train=1.24, val=1.58, val_ema=1.71]\u001b[A\n",
            "Epoch 1/12:  61%|████████▍     | 139/229 [02:16<01:27,  1.03it/s, train=1.26, val=1.59, val_ema=1.7]\u001b[A\n",
            "Epoch 1/12:  61%|████████▌     | 140/229 [02:16<01:26,  1.03it/s, train=1.26, val=1.59, val_ema=1.7]\u001b[A\n",
            "Epoch 1/12:  61%|███████▉     | 140/229 [02:17<01:26,  1.03it/s, train=1.21, val=1.58, val_ema=1.69]\u001b[A\n",
            "Epoch 1/12:  62%|████████     | 141/229 [02:17<01:25,  1.03it/s, train=1.21, val=1.58, val_ema=1.69]\u001b[A\n",
            "Epoch 1/12:  62%|████████     | 141/229 [02:18<01:25,  1.03it/s, train=1.18, val=1.56, val_ema=1.69]\u001b[A\n",
            "Epoch 1/12:  62%|████████     | 142/229 [02:18<01:24,  1.03it/s, train=1.18, val=1.56, val_ema=1.69]\u001b[A\n",
            "Epoch 1/12:  62%|████████     | 142/229 [02:19<01:24,  1.03it/s, train=1.27, val=1.58, val_ema=1.68]\u001b[A\n",
            "Epoch 1/12:  62%|████████     | 143/229 [02:19<01:23,  1.03it/s, train=1.27, val=1.58, val_ema=1.68]\u001b[A\n",
            "Epoch 1/12:  62%|████████     | 143/229 [02:20<01:23,  1.03it/s, train=1.22, val=1.62, val_ema=1.68]\u001b[A\n",
            "Epoch 1/12:  63%|████████▏    | 144/229 [02:20<01:23,  1.02it/s, train=1.22, val=1.62, val_ema=1.68]\u001b[A\n",
            "Epoch 1/12:  63%|████████▊     | 144/229 [02:21<01:23,  1.02it/s, train=1.2, val=1.62, val_ema=1.68]\u001b[A\n",
            "Epoch 1/12:  63%|████████▊     | 145/229 [02:21<01:22,  1.02it/s, train=1.2, val=1.62, val_ema=1.68]\u001b[A\n",
            "Epoch 1/12:  63%|█████████▍     | 145/229 [02:22<01:22,  1.02it/s, train=1.2, val=1.6, val_ema=1.67]\u001b[A\n",
            "Epoch 1/12:  64%|█████████▌     | 146/229 [02:22<01:21,  1.02it/s, train=1.2, val=1.6, val_ema=1.67]\u001b[A\n",
            "Epoch 1/12:  64%|████████▎    | 146/229 [02:23<01:21,  1.02it/s, train=1.18, val=1.54, val_ema=1.67]\u001b[A\n",
            "Epoch 1/12:  64%|████████▎    | 147/229 [02:23<01:19,  1.03it/s, train=1.18, val=1.54, val_ema=1.67]\u001b[A\n",
            "Epoch 1/12:  64%|████████▎    | 147/229 [02:24<01:19,  1.03it/s, train=1.25, val=1.56, val_ema=1.66]\u001b[A\n",
            "Epoch 1/12:  65%|████████▍    | 148/229 [02:24<01:18,  1.03it/s, train=1.25, val=1.56, val_ema=1.66]\u001b[A\n",
            "Epoch 1/12:  65%|████████▍    | 148/229 [02:25<01:18,  1.03it/s, train=1.24, val=1.59, val_ema=1.66]\u001b[A\n",
            "Epoch 1/12:  65%|████████▍    | 149/229 [02:25<01:17,  1.03it/s, train=1.24, val=1.59, val_ema=1.66]\u001b[A\n",
            "Epoch 1/12:  65%|████████▍    | 149/229 [02:26<01:17,  1.03it/s, train=1.15, val=1.57, val_ema=1.65]\u001b[A\n",
            "Epoch 1/12:  66%|████████▌    | 150/229 [02:26<01:16,  1.03it/s, train=1.15, val=1.57, val_ema=1.65]\u001b[A\n",
            "Epoch 1/12:  66%|█████████▏    | 150/229 [02:26<01:16,  1.03it/s, train=1.2, val=1.57, val_ema=1.65]\u001b[A\n",
            "Epoch 1/12:  66%|█████████▏    | 151/229 [02:27<01:15,  1.03it/s, train=1.2, val=1.57, val_ema=1.65]\u001b[A\n",
            "Epoch 1/12:  66%|████████▌    | 151/229 [02:27<01:15,  1.03it/s, train=1.19, val=1.62, val_ema=1.65]\u001b[A\n",
            "Epoch 1/12:  66%|████████▋    | 152/229 [02:27<01:14,  1.03it/s, train=1.19, val=1.62, val_ema=1.65]\u001b[A\n",
            "Epoch 1/12:  66%|█████████▎    | 152/229 [02:28<01:14,  1.03it/s, train=1.17, val=1.6, val_ema=1.64]\u001b[A\n",
            "Epoch 1/12:  67%|█████████▎    | 153/229 [02:28<01:13,  1.03it/s, train=1.17, val=1.6, val_ema=1.64]\u001b[A\n",
            "Epoch 1/12:  67%|████████▋    | 153/229 [02:29<01:13,  1.03it/s, train=1.22, val=1.57, val_ema=1.64]\u001b[A\n",
            "Epoch 1/12:  67%|████████▋    | 154/229 [02:29<01:12,  1.04it/s, train=1.22, val=1.57, val_ema=1.64]\u001b[A\n",
            "Epoch 1/12:  67%|████████▋    | 154/229 [02:30<01:12,  1.04it/s, train=1.07, val=1.61, val_ema=1.64]\u001b[A\n",
            "Epoch 1/12:  68%|████████▊    | 155/229 [02:30<01:11,  1.04it/s, train=1.07, val=1.61, val_ema=1.64]\u001b[A\n",
            "Epoch 1/12:  68%|████████▊    | 155/229 [02:31<01:11,  1.04it/s, train=1.21, val=1.53, val_ema=1.63]\u001b[A\n",
            "Epoch 1/12:  68%|████████▊    | 156/229 [02:31<01:10,  1.04it/s, train=1.21, val=1.53, val_ema=1.63]\u001b[A\n",
            "Epoch 1/12:  68%|████████▊    | 156/229 [02:32<01:10,  1.04it/s, train=1.11, val=1.56, val_ema=1.63]\u001b[A\n",
            "Epoch 1/12:  69%|████████▉    | 157/229 [02:32<01:09,  1.04it/s, train=1.11, val=1.56, val_ema=1.63]\u001b[A\n",
            "Epoch 1/12:  69%|████████▉    | 157/229 [02:33<01:09,  1.04it/s, train=1.14, val=1.54, val_ema=1.63]\u001b[A\n",
            "Epoch 1/12:  69%|████████▉    | 158/229 [02:33<01:08,  1.03it/s, train=1.14, val=1.54, val_ema=1.63]\u001b[A\n",
            "Epoch 1/12:  69%|████████▉    | 158/229 [02:34<01:08,  1.03it/s, train=1.11, val=1.58, val_ema=1.62]\u001b[A\n",
            "Epoch 1/12:  69%|█████████    | 159/229 [02:34<01:08,  1.03it/s, train=1.11, val=1.58, val_ema=1.62]\u001b[A\n",
            "Epoch 1/12:  69%|█████████    | 159/229 [02:35<01:08,  1.03it/s, train=1.17, val=1.58, val_ema=1.62]\u001b[A\n",
            "Epoch 1/12:  70%|█████████    | 160/229 [02:35<01:07,  1.03it/s, train=1.17, val=1.58, val_ema=1.62]\u001b[A\n",
            "Epoch 1/12:  70%|█████████▊    | 160/229 [02:36<01:07,  1.03it/s, train=1.11, val=1.6, val_ema=1.62]\u001b[A\n",
            "Epoch 1/12:  70%|█████████▊    | 161/229 [02:36<01:05,  1.03it/s, train=1.11, val=1.6, val_ema=1.62]\u001b[A\n",
            "Epoch 1/12:  70%|█████████▏   | 161/229 [02:37<01:05,  1.03it/s, train=1.11, val=1.54, val_ema=1.62]\u001b[A\n",
            "Epoch 1/12:  71%|█████████▏   | 162/229 [02:37<01:04,  1.04it/s, train=1.11, val=1.54, val_ema=1.62]\u001b[A\n",
            "Epoch 1/12:  71%|█████████▏   | 162/229 [02:38<01:04,  1.04it/s, train=1.14, val=1.52, val_ema=1.61]\u001b[A\n",
            "Epoch 1/12:  71%|█████████▎   | 163/229 [02:38<01:03,  1.04it/s, train=1.14, val=1.52, val_ema=1.61]\u001b[A\n",
            "Epoch 1/12:  71%|█████████▎   | 163/229 [02:39<01:03,  1.04it/s, train=1.07, val=1.51, val_ema=1.61]\u001b[A\n",
            "Epoch 1/12:  72%|█████████▎   | 164/229 [02:39<01:02,  1.04it/s, train=1.07, val=1.51, val_ema=1.61]\u001b[A\n",
            "Epoch 1/12:  72%|██████████    | 164/229 [02:40<01:02,  1.04it/s, train=1.18, val=1.53, val_ema=1.6]\u001b[A\n",
            "Epoch 1/12:  72%|██████████    | 165/229 [02:40<01:01,  1.04it/s, train=1.18, val=1.53, val_ema=1.6]\u001b[A\n",
            "Epoch 1/12:  72%|██████████    | 165/229 [02:41<01:01,  1.04it/s, train=1.12, val=1.54, val_ema=1.6]\u001b[A\n",
            "Epoch 1/12:  72%|██████████▏   | 166/229 [02:41<01:00,  1.04it/s, train=1.12, val=1.54, val_ema=1.6]\u001b[A\n",
            "Epoch 1/12:  72%|██████████▏   | 166/229 [02:42<01:00,  1.04it/s, train=1.15, val=1.57, val_ema=1.6]\u001b[A\n",
            "Epoch 1/12:  73%|██████████▏   | 167/229 [02:42<00:59,  1.04it/s, train=1.15, val=1.57, val_ema=1.6]\u001b[A\n",
            "Epoch 1/12:  73%|█████████▍   | 167/229 [02:43<00:59,  1.04it/s, train=1.11, val=1.51, val_ema=1.59]\u001b[A\n",
            "Epoch 1/12:  73%|█████████▌   | 168/229 [02:43<00:58,  1.04it/s, train=1.11, val=1.51, val_ema=1.59]\u001b[A\n",
            "Epoch 1/12:  73%|█████████▌   | 168/229 [02:44<00:58,  1.04it/s, train=1.17, val=1.56, val_ema=1.59]\u001b[A\n",
            "Epoch 1/12:  74%|█████████▌   | 169/229 [02:44<00:57,  1.04it/s, train=1.17, val=1.56, val_ema=1.59]\u001b[A\n",
            "Epoch 1/12:  74%|█████████▌   | 169/229 [02:45<00:57,  1.04it/s, train=1.07, val=1.52, val_ema=1.59]\u001b[A\n",
            "Epoch 1/12:  74%|█████████▋   | 170/229 [02:45<00:57,  1.03it/s, train=1.07, val=1.52, val_ema=1.59]\u001b[A\n",
            "Epoch 1/12:  74%|█████████▋   | 170/229 [02:46<00:57,  1.03it/s, train=1.04, val=1.46, val_ema=1.58]\u001b[A\n",
            "Epoch 1/12:  75%|█████████▋   | 171/229 [02:46<00:56,  1.02it/s, train=1.04, val=1.46, val_ema=1.58]\u001b[A\n",
            "Epoch 1/12:  75%|█████████▋   | 171/229 [02:47<00:56,  1.02it/s, train=1.07, val=1.55, val_ema=1.58]\u001b[A\n",
            "Epoch 1/12:  75%|█████████▊   | 172/229 [02:47<00:55,  1.02it/s, train=1.07, val=1.55, val_ema=1.58]\u001b[A\n",
            "Epoch 1/12:  75%|█████████▊   | 172/229 [02:48<00:55,  1.02it/s, train=1.12, val=1.52, val_ema=1.58]\u001b[A\n",
            "Epoch 1/12:  76%|█████████▊   | 173/229 [02:48<00:54,  1.03it/s, train=1.12, val=1.52, val_ema=1.58]\u001b[A\n",
            "Epoch 1/12:  76%|██████████▌   | 173/229 [02:49<00:54,  1.03it/s, train=1.12, val=1.5, val_ema=1.57]\u001b[A\n",
            "Epoch 1/12:  76%|██████████▋   | 174/229 [02:49<00:53,  1.03it/s, train=1.12, val=1.5, val_ema=1.57]\u001b[A\n",
            "Epoch 1/12:  76%|█████████▉   | 174/229 [02:50<00:53,  1.03it/s, train=1.07, val=1.51, val_ema=1.57]\u001b[A\n",
            "Epoch 1/12:  76%|█████████▉   | 175/229 [02:50<00:52,  1.03it/s, train=1.07, val=1.51, val_ema=1.57]\u001b[A\n",
            "Epoch 1/12:  76%|█████████▉   | 175/229 [02:51<00:52,  1.03it/s, train=1.11, val=1.52, val_ema=1.57]\u001b[A\n",
            "Epoch 1/12:  77%|█████████▉   | 176/229 [02:51<00:51,  1.04it/s, train=1.11, val=1.52, val_ema=1.57]\u001b[A\n",
            "Epoch 1/12:  77%|█████████▉   | 176/229 [02:52<00:51,  1.04it/s, train=1.09, val=1.54, val_ema=1.57]\u001b[A\n",
            "Epoch 1/12:  77%|██████████   | 177/229 [02:52<00:50,  1.04it/s, train=1.09, val=1.54, val_ema=1.57]\u001b[A\n",
            "Epoch 1/12:  77%|██████████   | 177/229 [02:53<00:50,  1.04it/s, train=1.03, val=1.58, val_ema=1.57]\u001b[A\n",
            "Epoch 1/12:  78%|██████████   | 178/229 [02:53<00:49,  1.04it/s, train=1.03, val=1.58, val_ema=1.57]\u001b[A\n",
            "Epoch 1/12:  78%|██████████▉   | 178/229 [02:54<00:49,  1.04it/s, train=1.1, val=1.56, val_ema=1.57]\u001b[A\n",
            "Epoch 1/12:  78%|██████████▉   | 179/229 [02:54<00:48,  1.04it/s, train=1.1, val=1.56, val_ema=1.57]\u001b[A\n",
            "Epoch 1/12:  78%|██████████▏  | 179/229 [02:55<00:48,  1.04it/s, train=1.11, val=1.46, val_ema=1.56]\u001b[A\n",
            "Epoch 1/12:  79%|██████████▏  | 180/229 [02:55<00:47,  1.04it/s, train=1.11, val=1.46, val_ema=1.56]\u001b[A\n",
            "Epoch 1/12:  79%|███████████   | 180/229 [02:55<00:47,  1.04it/s, train=1.1, val=1.45, val_ema=1.56]\u001b[A\n",
            "Epoch 1/12:  79%|███████████   | 181/229 [02:55<00:46,  1.04it/s, train=1.1, val=1.45, val_ema=1.56]\u001b[A\n",
            "Epoch 1/12:  79%|█████████▍  | 181/229 [02:56<00:46,  1.04it/s, train=0.985, val=1.47, val_ema=1.55]\u001b[A\n",
            "Epoch 1/12:  79%|█████████▌  | 182/229 [02:56<00:45,  1.04it/s, train=0.985, val=1.47, val_ema=1.55]\u001b[A\n",
            "Epoch 1/12:  79%|███████████▏  | 182/229 [02:57<00:45,  1.04it/s, train=1.03, val=1.5, val_ema=1.55]\u001b[A\n",
            "Epoch 1/12:  80%|███████████▏  | 183/229 [02:57<00:44,  1.03it/s, train=1.03, val=1.5, val_ema=1.55]\u001b[A\n",
            "Epoch 1/12:  80%|███████████▏  | 183/229 [02:58<00:44,  1.03it/s, train=1.04, val=1.5, val_ema=1.55]\u001b[A\n",
            "Epoch 1/12:  80%|███████████▏  | 184/229 [02:58<00:43,  1.03it/s, train=1.04, val=1.5, val_ema=1.55]\u001b[A\n",
            "Epoch 1/12:  80%|██████████▍  | 184/229 [02:59<00:43,  1.03it/s, train=1.02, val=1.47, val_ema=1.54]\u001b[A\n",
            "Epoch 1/12:  81%|██████████▌  | 185/229 [02:59<00:42,  1.03it/s, train=1.02, val=1.47, val_ema=1.54]\u001b[A\n",
            "Epoch 1/12:  81%|█████████▋  | 185/229 [03:00<00:42,  1.03it/s, train=0.995, val=1.54, val_ema=1.54]\u001b[A\n",
            "Epoch 1/12:  81%|█████████▋  | 186/229 [03:00<00:41,  1.03it/s, train=0.995, val=1.54, val_ema=1.54]\u001b[A\n",
            "Epoch 1/12:  81%|██████████▌  | 186/229 [03:01<00:41,  1.03it/s, train=1.05, val=1.54, val_ema=1.54]\u001b[A\n",
            "Epoch 1/12:  82%|██████████▌  | 187/229 [03:01<00:40,  1.03it/s, train=1.05, val=1.54, val_ema=1.54]\u001b[A\n",
            "Epoch 1/12:  82%|██████████▌  | 187/229 [03:02<00:40,  1.03it/s, train=1.04, val=1.47, val_ema=1.54]\u001b[A\n",
            "Epoch 1/12:  82%|██████████▋  | 188/229 [03:02<00:39,  1.03it/s, train=1.04, val=1.47, val_ema=1.54]\u001b[A\n",
            "Epoch 1/12:  82%|███████████▍  | 188/229 [03:03<00:39,  1.03it/s, train=1.03, val=1.5, val_ema=1.54]\u001b[A\n",
            "Epoch 1/12:  83%|███████████▌  | 189/229 [03:03<00:39,  1.02it/s, train=1.03, val=1.5, val_ema=1.54]\u001b[A\n",
            "Epoch 1/12:  83%|██████████▋  | 189/229 [03:04<00:39,  1.02it/s, train=1.05, val=1.48, val_ema=1.53]\u001b[A\n",
            "Epoch 1/12:  83%|██████████▊  | 190/229 [03:04<00:38,  1.02it/s, train=1.05, val=1.48, val_ema=1.53]\u001b[A\n",
            "Epoch 1/12:  83%|█████████████▎  | 190/229 [03:05<00:38,  1.02it/s, train=1, val=1.46, val_ema=1.53]\u001b[A\n",
            "Epoch 1/12:  83%|█████████████▎  | 191/229 [03:05<00:36,  1.03it/s, train=1, val=1.46, val_ema=1.53]\u001b[A\n",
            "Epoch 1/12:  83%|██████████▊  | 191/229 [03:06<00:36,  1.03it/s, train=1.01, val=1.46, val_ema=1.53]\u001b[A\n",
            "Epoch 1/12:  84%|██████████▉  | 192/229 [03:06<00:35,  1.03it/s, train=1.01, val=1.46, val_ema=1.53]\u001b[A\n",
            "Epoch 1/12:  84%|██████████▉  | 192/229 [03:07<00:35,  1.03it/s, train=1.06, val=1.49, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  84%|██████████▉  | 193/229 [03:07<00:34,  1.03it/s, train=1.06, val=1.49, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  84%|██████████▉  | 193/229 [03:08<00:34,  1.03it/s, train=1.07, val=1.47, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  85%|███████████  | 194/229 [03:08<00:33,  1.03it/s, train=1.07, val=1.47, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  85%|███████████  | 194/229 [03:09<00:33,  1.03it/s, train=1.04, val=1.49, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  85%|███████████  | 195/229 [03:09<00:32,  1.03it/s, train=1.04, val=1.49, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  85%|███████████  | 195/229 [03:10<00:32,  1.03it/s, train=1.06, val=1.57, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  86%|███████████▏ | 196/229 [03:10<00:31,  1.03it/s, train=1.06, val=1.57, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  86%|███████████▏ | 196/229 [03:11<00:31,  1.03it/s, train=1.01, val=1.49, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  86%|███████████▏ | 197/229 [03:11<00:31,  1.03it/s, train=1.01, val=1.49, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  86%|███████████▏ | 197/229 [03:12<00:31,  1.03it/s, train=1.03, val=1.51, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  86%|███████████▏ | 198/229 [03:12<00:30,  1.03it/s, train=1.03, val=1.51, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  86%|███████████▏ | 198/229 [03:13<00:30,  1.03it/s, train=1.02, val=1.45, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  87%|███████████▎ | 199/229 [03:13<00:29,  1.02it/s, train=1.02, val=1.45, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  87%|███████████▎ | 199/229 [03:14<00:29,  1.02it/s, train=1.02, val=1.51, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  87%|███████████▎ | 200/229 [03:14<00:28,  1.03it/s, train=1.02, val=1.51, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  87%|███████████▎ | 200/229 [03:15<00:28,  1.03it/s, train=1.02, val=1.51, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  88%|███████████▍ | 201/229 [03:15<00:27,  1.03it/s, train=1.02, val=1.51, val_ema=1.52]\u001b[A\n",
            "Epoch 1/12:  88%|██████████▌ | 201/229 [03:16<00:27,  1.03it/s, train=0.989, val=1.42, val_ema=1.51]\u001b[A\n",
            "Epoch 1/12:  88%|██████████▌ | 202/229 [03:16<00:26,  1.03it/s, train=0.989, val=1.42, val_ema=1.51]\u001b[A\n",
            "Epoch 1/12:  88%|██████████████  | 202/229 [03:17<00:26,  1.03it/s, train=1, val=1.45, val_ema=1.51]\u001b[A\n",
            "Epoch 1/12:  89%|██████████████▏ | 203/229 [03:17<00:25,  1.03it/s, train=1, val=1.45, val_ema=1.51]\u001b[A\n",
            "Epoch 1/12:  89%|███████████▌ | 203/229 [03:18<00:25,  1.03it/s, train=1.06, val=1.48, val_ema=1.51]\u001b[A\n",
            "Epoch 1/12:  89%|███████████▌ | 204/229 [03:18<00:24,  1.03it/s, train=1.06, val=1.48, val_ema=1.51]\u001b[A\n",
            "Epoch 1/12:  89%|██████████▋ | 204/229 [03:19<00:24,  1.03it/s, train=0.987, val=1.47, val_ema=1.51]\u001b[A\n",
            "Epoch 1/12:  90%|██████████▋ | 205/229 [03:19<00:23,  1.03it/s, train=0.987, val=1.47, val_ema=1.51]\u001b[A\n",
            "Epoch 1/12:  90%|███████████▋ | 205/229 [03:20<00:23,  1.03it/s, train=0.969, val=1.46, val_ema=1.5]\u001b[A\n",
            "Epoch 1/12:  90%|███████████▋ | 206/229 [03:20<00:22,  1.03it/s, train=0.969, val=1.46, val_ema=1.5]\u001b[A\n",
            "Epoch 1/12:  90%|███████████▋ | 206/229 [03:21<00:22,  1.03it/s, train=0.996, val=1.48, val_ema=1.5]\u001b[A\n",
            "Epoch 1/12:  90%|███████████▊ | 207/229 [03:21<00:21,  1.03it/s, train=0.996, val=1.48, val_ema=1.5]\u001b[A\n",
            "Epoch 1/12:  90%|████████████▋ | 207/229 [03:22<00:21,  1.03it/s, train=1.03, val=1.46, val_ema=1.5]\u001b[A\n",
            "Epoch 1/12:  91%|████████████▋ | 208/229 [03:22<00:20,  1.03it/s, train=1.03, val=1.46, val_ema=1.5]\u001b[A\n",
            "Epoch 1/12:  91%|███████████▊ | 208/229 [03:23<00:20,  1.03it/s, train=0.996, val=1.48, val_ema=1.5]\u001b[A\n",
            "Epoch 1/12:  91%|███████████▊ | 209/229 [03:23<00:19,  1.03it/s, train=0.996, val=1.48, val_ema=1.5]\u001b[A\n",
            "Epoch 1/12:  91%|██████████████▌ | 209/229 [03:24<00:19,  1.03it/s, train=1, val=1.41, val_ema=1.49]\u001b[A\n",
            "Epoch 1/12:  92%|██████████████▋ | 210/229 [03:24<00:18,  1.02it/s, train=1, val=1.41, val_ema=1.49]\u001b[A\n",
            "Epoch 1/12:  92%|███████████ | 210/229 [03:25<00:18,  1.02it/s, train=0.994, val=1.47, val_ema=1.49]\u001b[A\n",
            "Epoch 1/12:  92%|███████████ | 211/229 [03:25<00:17,  1.01it/s, train=0.994, val=1.47, val_ema=1.49]\u001b[A\n",
            "Epoch 1/12:  92%|███████████ | 211/229 [03:26<00:17,  1.01it/s, train=0.995, val=1.46, val_ema=1.49]\u001b[A\n",
            "Epoch 1/12:  93%|███████████ | 212/229 [03:26<00:16,  1.01it/s, train=0.995, val=1.46, val_ema=1.49]\u001b[A\n",
            "Epoch 1/12:  93%|████████████ | 212/229 [03:27<00:16,  1.01it/s, train=1.02, val=1.44, val_ema=1.49]\u001b[A\n",
            "Epoch 1/12:  93%|████████████ | 213/229 [03:27<00:15,  1.01it/s, train=1.02, val=1.44, val_ema=1.49]\u001b[A\n",
            "Epoch 1/12:  93%|███████████▏| 213/229 [03:28<00:15,  1.01it/s, train=0.944, val=1.48, val_ema=1.49]\u001b[A\n",
            "Epoch 1/12:  93%|███████████▏| 214/229 [03:28<00:14,  1.02it/s, train=0.944, val=1.48, val_ema=1.49]\u001b[A\n",
            "Epoch 1/12:  93%|███████████▏| 214/229 [03:29<00:14,  1.02it/s, train=0.996, val=1.53, val_ema=1.49]\u001b[A\n",
            "Epoch 1/12:  94%|███████████▎| 215/229 [03:29<00:13,  1.02it/s, train=0.996, val=1.53, val_ema=1.49]\u001b[A\n",
            "Epoch 1/12:  94%|████████████▏| 215/229 [03:30<00:13,  1.02it/s, train=1.02, val=1.48, val_ema=1.49]\u001b[A\n",
            "Epoch 1/12:  94%|████████████▎| 216/229 [03:30<00:12,  1.02it/s, train=1.02, val=1.48, val_ema=1.49]\u001b[A\n",
            "Epoch 1/12:  94%|████████████▎| 216/229 [03:31<00:12,  1.02it/s, train=1.02, val=1.42, val_ema=1.49]\u001b[A\n",
            "Epoch 1/12:  95%|████████████▎| 217/229 [03:31<00:11,  1.03it/s, train=1.02, val=1.42, val_ema=1.49]\u001b[A\n",
            "Epoch 1/12:  95%|███████████▎| 217/229 [03:32<00:11,  1.03it/s, train=0.962, val=1.37, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  95%|███████████▍| 218/229 [03:32<00:10,  1.03it/s, train=0.962, val=1.37, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  95%|███████████████▏| 218/229 [03:33<00:10,  1.03it/s, train=1, val=1.46, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  96%|███████████████▎| 219/229 [03:33<00:09,  1.03it/s, train=1, val=1.46, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  96%|███████████▍| 219/229 [03:33<00:09,  1.03it/s, train=0.957, val=1.42, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  96%|███████████▌| 220/229 [03:33<00:08,  1.03it/s, train=0.957, val=1.42, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  96%|███████████▌| 220/229 [03:34<00:08,  1.03it/s, train=0.984, val=1.47, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  97%|███████████▌| 221/229 [03:34<00:07,  1.02it/s, train=0.984, val=1.47, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  97%|███████████▌| 221/229 [03:35<00:07,  1.02it/s, train=0.939, val=1.53, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  97%|███████████▋| 222/229 [03:35<00:06,  1.02it/s, train=0.939, val=1.53, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  97%|████████████▌| 222/229 [03:36<00:06,  1.02it/s, train=1.01, val=1.46, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  97%|████████████▋| 223/229 [03:36<00:05,  1.02it/s, train=1.01, val=1.46, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  97%|███████████▋| 223/229 [03:37<00:05,  1.02it/s, train=0.991, val=1.47, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  98%|███████████▋| 224/229 [03:37<00:04,  1.01it/s, train=0.991, val=1.47, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  98%|████████████▋| 224/229 [03:38<00:04,  1.01it/s, train=1.02, val=1.44, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  98%|████████████▊| 225/229 [03:38<00:03,  1.01it/s, train=1.02, val=1.44, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  98%|███████████▊| 225/229 [03:39<00:03,  1.01it/s, train=0.934, val=1.46, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  99%|███████████▊| 226/229 [03:39<00:02,  1.01it/s, train=0.934, val=1.46, val_ema=1.48]\u001b[A\n",
            "Epoch 1/12:  99%|███████████▊| 226/229 [03:40<00:02,  1.01it/s, train=0.956, val=1.46, val_ema=1.47]\u001b[A\n",
            "Epoch 1/12:  99%|███████████▉| 227/229 [03:40<00:01,  1.02it/s, train=0.956, val=1.46, val_ema=1.47]\u001b[A\n",
            "Epoch 1/12:  99%|███████████▉| 227/229 [03:41<00:01,  1.02it/s, train=0.926, val=1.43, val_ema=1.47]\u001b[A\n",
            "Epoch 1/12: 100%|███████████▉| 228/229 [03:41<00:00,  1.02it/s, train=0.926, val=1.43, val_ema=1.47]\u001b[A\n",
            "Epoch 1/12: 100%|███████████▉| 228/229 [03:42<00:00,  1.02it/s, train=0.952, val=1.44, val_ema=1.47]\u001b[A\n",
            "Epoch 1/12: 100%|████████████| 229/229 [03:42<00:00,  1.02it/s, train=0.952, val=1.44, val_ema=1.47]\u001b[A\n",
            "Epoch 1/12: 100%|███████| 229/229 [03:42<00:00,  1.02it/s, train=0.952, val=1.44, BEST val_ema=1.47]\u001b[A\n",
            "Epoch 1/12: 100%|███████| 229/229 [03:42<00:00,  1.03it/s, train=0.952, val=1.44, BEST val_ema=1.47]\n",
            "Epoch 2/12: 100%|████████████| 229/229 [03:39<00:00,  1.04it/s, train=0.424, val=1.86, val_ema=1.74]\n",
            "Epoch 3/12:  14%|█▊           | 32/229 [00:30<03:08,  1.05it/s, train=0.386, val=1.97, val_ema=1.87]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b25dce967f83>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mxvb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myvb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mval_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mxvb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myvb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mval_loss_ema\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mema_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mema_weight\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mval_loss_ema\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mval_loss_ema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_ema'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_ema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "val_every  = 1 # in steps, evaluate loss on val dataset\n",
        "cp_every   = 60 # in epochs, checkpoint every\n",
        "demo_every = 4 # in epochs, make a midi player demo\n",
        "\n",
        "best_loss = 999\n",
        "\n",
        "ema_weight, loss_ema, val_loss_ema = 0.95, None , None # exponential moving averages for loss reporting\n",
        "step = 0\n",
        "losses = {}\n",
        "epochs = config.epochs\n",
        "for epoch in range(1,epochs+1):\n",
        "    pbar = tqdm(total=len(train_dl), desc=f\"Epoch {epoch}/{config.epochs}\", dynamic_ncols=False, ncols=100) # progress bar, per epoch\n",
        "    for bi, batch in enumerate(train_dl):\n",
        "        step += 1\n",
        "        xb, yb = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "        logits, loss = model(xb, yb) # evaluate the loss\n",
        "        loss_ema = (1.0-ema_weight)*loss.item() + ema_weight*loss_ema if loss_ema is not None else loss.item()\n",
        "        losses['train'], losses['train_ema'] = loss.item(), loss_ema\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # status / diagnostics:\n",
        "        if (step % val_every == 0):\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                xvb, yvb = next(iter(val_dl))\n",
        "                val_logits, val_loss = model( xvb.to(device), yvb.to(device) )\n",
        "                val_loss_ema =  (1.0-ema_weight)*val_loss.item() + ema_weight*val_loss_ema if val_loss_ema is not None else val_loss.item()\n",
        "                losses['val'], losses['val_ema'] = val_loss.item(), val_loss_ema\n",
        "            model.train()\n",
        "\n",
        "        wbl_dict = {'step':step, 'epoch':epoch} | losses   # dict for logging losses, midi examples, etc to wandb\n",
        "        pbar.set_postfix( dict((k,losses[k]) for k in ['train', 'val','val_ema'])) # loss info for progress bar\n",
        "        pbar.update(1)\n",
        "        if use_wandb and wbl_dict != {}: wandb.log(wbl_dict)\n",
        "\n",
        "    #--- end of epoch ---\n",
        "\n",
        "    if losses['val_ema'] < best_loss: # Tracking best val_loss_ema for checkpointing purposes\n",
        "        best_loss = losses['val_ema']\n",
        "        pbar.set_postfix(dict( (k,losses[k]) for k in ['train', 'val']) | {'BEST val_ema':best_loss})\n",
        "        if (epoch) % cp_every==0:   # occasionally save a checkpoint of best model/optimizer states\n",
        "            cp_file = f\"musicbox-jsb\" #    -{step}\" # let's leave out step to avoid filling disk\n",
        "            print(f\"Saving a checkpoint to {cp_file}\")\n",
        "            torch.save({ 'step': step, 'model_state_dict': model.state_dict(), 'loss': loss,\n",
        "                         'optimizer_state_dict': optimizer.state_dict(),}, cp_file)\n",
        "    pbar.refresh()\n",
        "    pbar.close()\n",
        "\n",
        "    if (epoch % demo_every == 0) or (epoch==epochs):  # demo of midi generation\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            new_notes = decode( model.generate(prompt_tokens, max_new_tokens=demo_max_new_tokens, temperature=1)[0].cpu() )\n",
        "            p2 = midiplayer(new_notes,title=f\"Demo on val dataset, Epoch={epoch}\")\n",
        "            display(p2)\n",
        "            if use_wandb: wandb.log( {'step':step, 'player':wandb.Html(p2.html)} )\n",
        "        model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6TugZVD0pIER",
      "metadata": {
        "id": "6TugZVD0pIER",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294,
          "referenced_widgets": [
            "b39ca0166cad48cd8f075533052c7631",
            "a873448c1a2e4a81ba96e3cab8f3a55a",
            "79c7612db17342c1b93dd4dad65689e0",
            "3857ce08ab4a42d88949577b1eaae7b7",
            "305c4bb200cc48e8a2c5ca452495b665",
            "7c3473e8385e46c980548538f643fc6e",
            "ab469e349f1a42669819a9d2174e4e50",
            "0ff01aea880945a5a421e8dbb49192d7"
          ]
        },
        "outputId": "c77a31a1-2c67-4321-e083-9778f9f25d3f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.012 MB uploaded\\r'), FloatProgress(value=0.1000239100980314, max=1.0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b39ca0166cad48cd8f075533052c7631"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train</td><td>█▆▆▆▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_ema</td><td>█▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val</td><td>█▇▆▅▅▅▅▄▃▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▂▂▁</td></tr><tr><td>val_ema</td><td>█▇▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>step</td><td>382</td></tr><tr><td>train</td><td>0.66084</td></tr><tr><td>train_ema</td><td>0.71361</td></tr><tr><td>val</td><td>1.54811</td></tr><tr><td>val_ema</td><td>1.48589</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">firm-surf-46</strong> at: <a href='https://wandb.ai/drscotthawley/musicbox-jsb-tutorial/runs/l7mcznw1' target=\"_blank\">https://wandb.ai/drscotthawley/musicbox-jsb-tutorial/runs/l7mcznw1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20231231_185632-l7mcznw1/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "if use_wandb: wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f21425f9-75b4-4976-a9c5-c1bd1c0db970",
      "metadata": {
        "id": "f21425f9-75b4-4976-a9c5-c1bd1c0db970"
      },
      "source": [
        "## Evaluation Stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "659HS9LnpVPm",
      "metadata": {
        "id": "659HS9LnpVPm"
      },
      "outputs": [],
      "source": [
        "# gen samples\n",
        "\n",
        "file_ind = 1\n",
        "original = test_notes_tl[file_ind]\n",
        "num_tokens = len(original)\n",
        "display(midiplayer(original, title=f\"Full Evaluation Target, {num_tokens} notes long\"))\n",
        "\n",
        "\n",
        "prompt_len = 21\n",
        "prompt = original[:prompt_len]\n",
        "display(midiplayer(prompt, title=f\"Evaluation Prompt, {prompt_len} notes long\"))\n",
        "\n",
        "\n",
        "prompt_tokens = encode(prompt).unsqueeze(0).to(device)\n",
        "\n",
        "new_tokens = num_tokens - prompt_len\n",
        "\n",
        "for temperature in [ 0.7, 0.85, 0.92, 1.0, 1.2, 1.5]:\n",
        "    set_seeds(1337) # same temp for same seed will yield same output\n",
        "    notes = decode( model.generate(prompt_tokens, max_new_tokens=new_tokens, temperature=temperature)[0].cpu() )\n",
        "    display(midiplayer(notes, title=f\"Temperature = {temperature}\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd51564d-7095-44f0-bbad-ab27c288f05c",
      "metadata": {
        "id": "dd51564d-7095-44f0-bbad-ab27c288f05c"
      },
      "outputs": [],
      "source": [
        "# perplexity\n",
        "\n",
        "test_ds = NotesDataset(test_notes_tl, config.seq_length, len_mult=config.batch_size, pad=False)\n",
        "test_dl = DataLoader(train_ds, batch_size=config.batch_size, shuffle=False)\n",
        "\n",
        "total, batches = 0, 0\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for i, batch in enumerate(tqdm(test_dl, ncols=100)):\n",
        "        batches +=1\n",
        "        xb, yb = [q.to(device) for q in batch]\n",
        "        logits, loss = model(xb, yb)\n",
        "        total = total + loss.cpu().item()\n",
        "perplexity = np.exp(total/batches)\n",
        "print(f\"(Average) Perplexity = {perplexity:5.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aotxjItQpkOl",
      "metadata": {
        "id": "aotxjItQpkOl"
      },
      "outputs": [],
      "source": [
        "#stats\n",
        "\n",
        "all_test_notes = torch.vstack(test_notes_tl).type(torch.float32)\n",
        "all_test_notes = all_test_notes[:len(all_test_notes)//4]  # to save time, let's not do stats on ALL-all of them, just a quarter\n",
        "\n",
        "# generate a big batch of fake data.\n",
        "# WARNING: this is super slow because our generator only works for batches of one, oops\n",
        "prompt_len, max_new_tokens = 12, 54          # I just made these numbers up. not great science yet, sorry.\n",
        "new_notes_list = []\n",
        "print(\"Generating new notes...\")\n",
        "for i, notes in enumerate(tqdm(test_notes_tl, ncols=100)):\n",
        "    ptoks = encode(notes[:prompt_len]).unsqueeze(0).to(device)\n",
        "    # generate more notes, chop off prompt before adding to list of all generated notes\n",
        "    new_notes_list.append( decode( model.generate(ptoks, max_new_tokens=max_new_tokens)[0].cpu()[prompt_len:] ) )\n",
        "all_new_notes = torch.vstack(new_notes_list).type(torch.float32)\n",
        "\n",
        "print(f\"# of test notes = {len(all_test_notes)},  # of generated notes = {len(all_new_notes)} --- Close enough!\")\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "names = ['pitch', 'step', 'duration']\n",
        "for i in range(3):\n",
        "    bins = 32 if i !=0 else 0.5 + 2*np.arange(128//2)\n",
        "    ax[i].hist(all_test_notes[:,i].numpy(), bins=bins, label='test data', alpha=0.6)\n",
        "    ax[i].hist(all_new_notes[:,i].numpy(),  bins=bins, label='generated', alpha=0.6)\n",
        "    ax[i].set_title(names[i])\n",
        "    if i==2: ax[i].legend()\n",
        "    ax[i].set_xlabel('Value'+f'{\" (s)\" if i>0 else \"\"}')\n",
        "    if i==0: ax[i].set_ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7Ci2V5iqCJy",
      "metadata": {
        "id": "f7Ci2V5iqCJy"
      },
      "source": [
        "Appendix:\n",
        "\n",
        "If you want the Christmas stuff, just do \"show code\" in the blog post."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CgxtxxhbscLr",
      "metadata": {
        "id": "CgxtxxhbscLr"
      },
      "source": [
        "----\n",
        "This work is MIT-licensed by Scott H. Hawley. Feel free to use it as you like, but an attribution would be nice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JnUNZnelqHJE",
      "metadata": {
        "id": "JnUNZnelqHJE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b39ca0166cad48cd8f075533052c7631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a873448c1a2e4a81ba96e3cab8f3a55a",
              "IPY_MODEL_79c7612db17342c1b93dd4dad65689e0"
            ],
            "layout": "IPY_MODEL_3857ce08ab4a42d88949577b1eaae7b7"
          }
        },
        "a873448c1a2e4a81ba96e3cab8f3a55a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_305c4bb200cc48e8a2c5ca452495b665",
            "placeholder": "​",
            "style": "IPY_MODEL_7c3473e8385e46c980548538f643fc6e",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "79c7612db17342c1b93dd4dad65689e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab469e349f1a42669819a9d2174e4e50",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ff01aea880945a5a421e8dbb49192d7",
            "value": 1
          }
        },
        "3857ce08ab4a42d88949577b1eaae7b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "305c4bb200cc48e8a2c5ca452495b665": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c3473e8385e46c980548538f643fc6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab469e349f1a42669819a9d2174e4e50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ff01aea880945a5a421e8dbb49192d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
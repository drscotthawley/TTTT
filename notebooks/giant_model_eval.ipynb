{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT5iM-Dq6J1A"
   },
   "source": [
    "Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXLciiRULajm",
    "outputId": "59a38e26-a1ab-4c6d-f375-8345d1a3b0f1"
   },
   "outputs": [],
   "source": [
    "#!sudo apt install -qq -y fluidsynth >/dev/null;  # that's for Linux.  on Mac: \"brew install fluidsynth\"\n",
    "#!pip install -Uqq pyfluidsynth pretty_midi wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kjDFe6tNztJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from glob import glob\n",
    "import pathlib\n",
    "import pretty_midi\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import multiprocessing as mp\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtPj8fGF6I2V",
    "outputId": "d04cd8de-4ef4-4dff-ed22-1ac7e1ebf171"
   },
   "outputs": [],
   "source": [
    "#!wget -N https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip\n",
    "#!unzip -n -qq maestro-v2.0.0-midi.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJCJGzlOKycp",
    "outputId": "2b01ddbd-0019-47d1-c829-599222dfd09f"
   },
   "outputs": [],
   "source": [
    "#data_dir = pathlib.Path('maestro-v2.0.0')\n",
    "#filenames = glob(str(data_dir/'**/*.mid*'))\n",
    "#print('Number of files:', len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ex5X1GC6LN5A"
   },
   "outputs": [],
   "source": [
    "vocab_size=128 # number of midi notes\n",
    "n_contin = 2   # number of continuous variables besides pitches\n",
    "\n",
    "def midi_file_to_tensor(midi_file):\n",
    "    pm = pretty_midi.PrettyMIDI(midi_file) # read in the whole file. this is incredibly slow\n",
    "\n",
    "    # Sort the notes first by start time (then by pitch if two notes start at the same time)\n",
    "    sorted_notes = sorted(pm.instruments[0].notes, key=lambda note: (note.start, note.pitch))\n",
    "    notes = torch.empty( (len(sorted_notes), 3), dtype=torch.float32 ) # allocate storage\n",
    "\n",
    "    prev_start = sorted_notes[0].start\n",
    "    for i, note in enumerate(sorted_notes):\n",
    "        notes[i] = note.pitch\n",
    "        notes[i, 1] = note.start - prev_start  # step, i.e. time since last note started\n",
    "        notes[i, 2] = note.end - note.start    # duration\n",
    "        prev_start = note.start\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3xP4JuTOCkH",
    "outputId": "514fba3e-7b11-403e-b432-e769526b64fe"
   },
   "outputs": [],
   "source": [
    "#notes = midi_file_to_tensor(filenames[0])\n",
    "#pitches = notes[:,0].type(torch.long)  # just the pitch info\n",
    "#notes.shape, pitches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lUOaORcKTRW4"
   },
   "outputs": [],
   "source": [
    "# @title Tensor to MIDI Display Code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "\n",
    "def notes_arr_to_df(notes_arr) -> pd.DataFrame:\n",
    "    columns = ['pitch','step','duration']\n",
    "    df = pd.DataFrame(notes_arr, columns=columns)\n",
    "    df[\"start\"] = \"\"\n",
    "    df[\"end\"] = \"\"\n",
    "\n",
    "    prev_start = 0\n",
    "    #for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    for i, row in df.iterrows():\n",
    "        start = prev_start + float(row['step'])\n",
    "        df.at[i, 'start'] = start\n",
    "        df.at[i, 'end'] = start + float(row['duration'])\n",
    "        prev_start = start\n",
    "    return df\n",
    "\n",
    "def df_to_midi(\n",
    "        notes_df: pd.DataFrame,\n",
    "        out_file: str = '',  # output file to save to, if any\n",
    "        instrument_name: str = 'Acoustic Grand Piano', # whatever you want to call this instrument\n",
    "        velocity: int = 100,  # note loudness\n",
    "    ) -> pretty_midi.PrettyMIDI:\n",
    "    \"converts a dataframe to valid midi\"\n",
    "\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.Instrument(\n",
    "        program=pretty_midi.instrument_name_to_program(\n",
    "            instrument_name))\n",
    "\n",
    "    prev_start = 0\n",
    "    for i, note in notes_df.iterrows(): # this is a serial operation, not sure how to parallelize\n",
    "        start = float(prev_start + note['step'])\n",
    "        end = float(start + note['duration'])\n",
    "        note = pretty_midi.Note(\n",
    "            velocity=velocity,\n",
    "            pitch=int(note['pitch']),\n",
    "            start=start,\n",
    "            end=end,\n",
    "        )\n",
    "        instrument.notes.append(note)\n",
    "        prev_start = start\n",
    "\n",
    "    pm.instruments.append(instrument)\n",
    "    if out_file: pm.write(out_file)\n",
    "    return pm\n",
    "\n",
    "def plot_piano_roll(notes_df: pd.DataFrame, count: Optional[int] = None):\n",
    "    \"produce a piano roll plot\"\n",
    "    if count:\n",
    "        title = f'First {count} notes'\n",
    "    else:\n",
    "        title = f'Whole track'\n",
    "        count = len(notes_df['pitch'])\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plot_pitch = np.stack([notes_df['pitch'], notes_df['pitch']], axis=0)\n",
    "    plot_start_stop = np.stack([notes_df['start'], notes_df['end']], axis=0)\n",
    "    plt.plot(\n",
    "        plot_start_stop[:, :count], plot_pitch[:, :count], color=\"b\", marker=\".\")\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Pitch')\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim([0, vocab_size])\n",
    "    _ = plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def midi_to_audio(pm: pretty_midi.PrettyMIDI, seconds=30, sr=16000):\n",
    "  \"midi to audio, playable in notebook\"\n",
    "  waveform = pm.fluidsynth(fs=float(sr))\n",
    "  # Take a sample of the generated waveform to mitigate kernel resets\n",
    "  waveform_short = waveform[:seconds*sr]\n",
    "  return display(Audio(waveform_short, rate=sr))\n",
    "\n",
    "def pitches_to_midi(pitch_list, seconds=30):\n",
    "    notes_tensor = torch.zeros((len(pitch_list), 3)) + 0.25\n",
    "    for i, p in enumerate(pitch_list):\n",
    "        notes_tensor[i,0] = p\n",
    "    notes_df = notes_arr_to_df(notes_tensor.cpu().detach().numpy())\n",
    "    midi = df_to_midi(notes_df)\n",
    "    plot_piano_roll(notes_df)\n",
    "    audio_display = midi_to_audio(midi, seconds=seconds)\n",
    "    return audio_display\n",
    "\n",
    "def notes_to_midi(notes_tensor, seconds=30):\n",
    "    #notes_tensor = notes_tensor * (notes_tensor>0)  # negative numbers clipped to zero\n",
    "    if notes_tensor.min() < 0.0:\n",
    "      print(\"WARNING: You have negative pitches, steps or durations. Setting them to zero\")\n",
    "      notes_tensor = notes_tensor * (notes_tensor >= 0)\n",
    "    notes_df = notes_arr_to_df(notes_tensor.cpu().detach().numpy())\n",
    "    midi = df_to_midi(notes_df)\n",
    "    plot_piano_roll(notes_df)\n",
    "    audio_display = midi_to_audio(midi, seconds=seconds)\n",
    "    return audio_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzE1MiIUbroR"
   },
   "outputs": [],
   "source": [
    "def old_files_to_tensor_list(filenames, n_files=None): # takes ~2.5 seconds per file\n",
    "  tensor_list = []\n",
    "  if n_files is None: n_files = len(filenames)\n",
    "  for f in tqdm(filenames[:n_files]):\n",
    "    tensor_list.append(midi_file_to_tensor(f))\n",
    "  return tensor_list\n",
    "\n",
    "\n",
    "def files_to_tensor_list(filenames): # reads files in parallel! Take 2 mins 30s intstead of 20 mins\n",
    "    tensor_list = process_map(midi_file_to_tensor, filenames, max_workers=mp.cpu_count(), chunksize=1)\n",
    "    return tensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "Gz--Hv1inCIg",
    "outputId": "2082c7e3-a493-45dd-ec8a-77357b86e2d4"
   },
   "outputs": [],
   "source": [
    "#notes_list = files_to_tensor_list(filenames, n_files=len(filenames))\n",
    "#print(f\"\\n{len(notes_list)} files read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXjJjXiNC0ov"
   },
   "outputs": [],
   "source": [
    "notes_list = torch.load('maestro3_tensor_list.pt')\n",
    "len(notes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "UhrtxPKjXeTv",
    "outputId": "7178904f-f7bb-41ce-e5b0-2fdff6f2887b"
   },
   "outputs": [],
   "source": [
    "use_wandb = True\n",
    "if use_wandb:\n",
    "    import wandb\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(\"device =\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iReoycw0b07C"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y06bDCiX6R0C"
   },
   "outputs": [],
   "source": [
    "\n",
    "# hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 128 # what is the maximum context length for predictions?\n",
    "n_embd = 256# 64  # dimensions of embeddings\n",
    "n_head = 16# 4\n",
    "n_layer = 16# 4\n",
    "dropout = 0.1 #0.1\n",
    "\n",
    "max_iters = 100000\n",
    "eval_interval = 100\n",
    "eval_iters = 200\n",
    "# ------------\n",
    "\n",
    "config = {\n",
    "  'learning_rate': learning_rate,\n",
    "  'batch_size': batch_size,\n",
    "  'block_size': block_size,\n",
    "  'n_embd': n_embd,\n",
    "  'n_head': n_head,\n",
    "  'n_layer': n_layer,\n",
    "  'dropout': dropout\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QlkM3xo6WhL"
   },
   "source": [
    "Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaAMdat36Vj4",
    "outputId": "483d96da-120b-4bd3-a85b-a463eeeaba04"
   },
   "outputs": [],
   "source": [
    "def tl_to_notes(tensor_list, shuffle=False, delimit=True):\n",
    "  \"list of tensors (of arbitrary length, for each song) converted to one big long tensor of notes all running togehter\"\n",
    "  if shuffle:random.shuffle(tensor_list)\n",
    "  if delimit:\n",
    "    delimiter = torch.zeros(3)  # use all zeros to show ends of songs\n",
    "    tensor_list = [element for item in tensor_list for element in (item, delimiter)]\n",
    "  return torch.vstack(tensor_list)\n",
    "\n",
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "all_notes = tl_to_notes(notes_list, shuffle=True) # just grab one file, for testing overfitting\n",
    "#all_notes = tl_to_notes(notes_list, shuffle=True)     # all the songs you read in earlier\n",
    "all_pitches = all_notes[:,0].type(torch.long)  # just the pitch info\n",
    "print(\"all_notes.shape =\",all_notes.shape)\n",
    "\n",
    "data = all_notes # all_notes\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(\"len(train_data), len(val_data) =\",len(train_data), len(val_data) )\n",
    "#assert n_contin == train_data.shape[-1]-1  # data consists of pitches plus continuous variables\n",
    "\n",
    "\n",
    "def augment_data(x,y):\n",
    "    pitch_shift = torch.randint(low=-12, high=12, size=(1,1,1), dtype=torch.long).item()\n",
    "    time_scale = 0.5+torch.rand((1,1,1),dtype=torch.float32).item()  # [0.5 .. 1.5]\n",
    "\n",
    "    x[:,:,0] = torch.clamp( x[:,:,0]+pitch_shift, min=0, max=vocab_size)\n",
    "    y[:,:,0] = torch.clamp( y[:,:,0]+pitch_shift, min=0, max=vocab_size)\n",
    "    x[:,:,1:] = x[:,:,1:] * time_scale\n",
    "    y[:,:,1:] = y[:,:,1:] * time_scale\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_batch(split, debug=False):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.clone(), y.clone()  # just to avoid overwrites\n",
    "    if split == 'train': # data augmentation\n",
    "        x, y = augment_data(x,y)\n",
    "    x[:,0,1] = x[:,0,1] * 0  # set first step of every x sequence to 0 (this shouldn't make a difference though)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        pitch_losses = torch.zeros(eval_iters)\n",
    "        contin_losses = torch.zeros(eval_iters)\n",
    "\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, contin_vars, loss, sublosses = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "            pitch_losses[k] = sublosses['pitch']\n",
    "            contin_losses[k] = sublosses['contin']\n",
    "        out[split] = losses.mean()\n",
    "        out['pitch'] = pitch_losses.mean()\n",
    "        out['contin'] = contin_losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9PfGh_57IY0",
    "outputId": "46fccc5f-943d-46df-db5a-33ddae856374"
   },
   "outputs": [],
   "source": [
    "x,y = get_batch('train',debug=True)\n",
    "print(f\"B, T = {batch_size}, {block_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Kr70iKZ7pPd"
   },
   "source": [
    "x is a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "ZBwJTuc17kxe",
    "outputId": "e493eae3-ac40-4335-c6b4-14653df7c302"
   },
   "outputs": [],
   "source": [
    "notes_to_midi(x[0])\n",
    "x[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WP9U8aIm7qh3"
   },
   "source": [
    "y is x shifted back by one and including new data.\n",
    "in this sense only y[:,-1] is the \"next token\" being predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "5Wjb4QMS7lub",
    "outputId": "54fe6210-448d-42ea-8d92-38d80d4cd9e9"
   },
   "outputs": [],
   "source": [
    "notes_to_midi(y[0])\n",
    "y[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CO7ZJhu-6dvk"
   },
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOeDesr4348z"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def mse_with_positive_pressure(y_true: torch.Tensor, y_pred: torch.Tensor):\n",
    "  # from Magenta example, converted to pytorch\n",
    "  se = (y_true - y_pred) ** 2\n",
    "  positive_pressure = 10 * torch.clamp(-y_pred, min=0) # ten times the negative values made positive\n",
    "  return (se + positive_pressure).mean()\n",
    "\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd-n_contin)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        self.contin_head = nn.Linear(n_embd, n_contin, bias=False)\n",
    "        \n",
    "    def forward(self, inputs, targets=None):\n",
    "        idx = inputs[:,:,0].type(torch.long).to(inputs.device)  # idx are the pitch indices\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C-2) # embed the pitches\n",
    "        tok_emb = torch.cat((tok_emb, inputs[:,:,1:]), dim=-1)   # concat the continuous variables\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x)   # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "        contin_vars = self.contin_head(x) # (B,T,2)\n",
    "        #contin_vars = F.softplus(contin_vars) # just to prevent negative values\n",
    "        time_step = F.leaky_relu(contin_vars[:,:,0]).unsqueeze(-1)  # encourage non-negative steps, but allow zero and small neg values\n",
    "        time_dur = F.softplus(contin_vars[:,:,1]).unsqueeze(-1)     # force only positive durations.\n",
    "        contin_vars = torch.cat( (time_step, time_dur), dim=-1 )\n",
    "        \n",
    "        # we can also compute losses if targets are included in .forward call\n",
    "        if targets is None:\n",
    "            return logits, contin_vars, None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            pitch_targets = (targets[:,:,0]).type(torch.long).to(logits.device).view(B*T)\n",
    "            \n",
    "            lambda_pitch, lambda_contin = 0.5, 10.0  # values from Magenta example, rescaled\n",
    "            #lambda_pitch, lambda_contin = 0.25, 20.0  # half pitch, double contin\n",
    "            pitch_loss = lambda_pitch * F.cross_entropy(logits, pitch_targets)\n",
    "\n",
    "            # contin variables, use mse with postive pressure\n",
    "            contin_loss = lambda_contin * F.mse_loss(contin_vars, targets[:,:,-n_contin:] )\n",
    "            #contin_loss = lambda_contin * mse_with_positive_pressure(contin_vars, targets[:,:,-n_contin:] )\n",
    "            sublosses = {'pitch':pitch_loss.item(), 'contin':contin_loss.item() } \n",
    "\n",
    "            loss = pitch_loss + contin_loss\n",
    "            return logits, contin_vars, loss, sublosses\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        # idx is (B, T, 1+contin_vars) array of values in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, contin_vars, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits, contin_vars = logits[:, -1, :], contin_vars[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits/temperature, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # concat continuous variables\n",
    "            idx_next = torch.cat((idx_next, contin_vars), dim=-1).unsqueeze(0) # unsqueeze to add dummy batch dim\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx.cpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSUO8GJx6tpw"
   },
   "source": [
    "Instantiate and get ready to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3yYAD0t6tOK",
    "outputId": "d1c3bad1-0b15-474c-aa27-7ca578411511"
   },
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate/50)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, 0.01, total_steps=max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(step, model, optimizer,loss, name):\n",
    "    name = name + f'_{step}.pt'\n",
    "    print(\"Saving checkpoint to\",name)\n",
    "    torch.save({\n",
    "            'step': step,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, name)\n",
    "\n",
    "def load_checkpoint(name, model, optimizer):\n",
    "    checkpoint = torch.load('augemented_cuda_checkpoint.pt')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    m = model.to(device)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    step = checkpoint['step']\n",
    "    loss = checkpoint['loss']\n",
    "    return step, m, optimizer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_the_checkpoint = True\n",
    "\n",
    "if load_the_checkpoint:\n",
    "    step, model, optimizer, loss = load_checkpoint('giant_model_lrelu_checkpoint_24000.pt', model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "eh9Neb2tYwsR",
    "outputId": "1e143ce1-caad-4968-a673-92c3892e31c8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_wandb: wandb.init(project='musicbox', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mO4XTmlj58La"
   },
   "source": [
    "Do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YCWAxp3I57eO",
    "outputId": "a93ab45a-6317-45db-d56f-8140b377fce9"
   },
   "outputs": [],
   "source": [
    "loss_hist = []\n",
    "checkpoint_every = 1000\n",
    "for step in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if step % eval_interval == 0 or step == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: losses: train {losses['train']:.4f}, val {losses['val']:.4f}, pitch {losses['pitch']:.4f}, contin {losses['contin']:.4f}\")\n",
    "        if use_wandb: wandb.log(losses | {'step':(step+19000)//eval_interval})\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, contin_vars, loss, sublosses = model(xb, yb)\n",
    "\n",
    "    if (step>0) and (step % checkpoint_every==0):\n",
    "        save_checkpoint(step, model, optimizer, loss, \"giant_model_lrelu_checkpoint\")\n",
    "        \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save({\n",
    "#            'step': 10000,\n",
    "#            'model_state_dict': model.state_dict(),\n",
    "#            'optimizer_state_dict': optimizer.state_dict(),\n",
    "#            'loss': loss,\n",
    "#            }, f'giant_model_lrelu_checkpoint_{step}.pt')\n",
    "\n",
    "save_checkpoint(step, model,optimizer,loss, \"giant_model_lrelu_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290,
     "referenced_widgets": [
      "1681d2d473564f4e85e82c41daa06607",
      "62fb22845f01443eba2b42ec7f3674cb",
      "d5cb1a9239174a218c9440a96120b4ba",
      "54c97a105c9f4750a0ba7f80bb30fecc",
      "707d8dce5c9e4a34bb5e1dd32e649f18",
      "909ae16959dc472e8e6b7657ed27b31d",
      "f75e27a45d5d4b15ad9bb6419f53710c",
      "9bbf5bc473e047f68909695b6360bbbc"
     ]
    },
    "id": "rD-eaZO7ZhH0",
    "outputId": "e97a230f-d490-4492-d19a-2765b221daaa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_wandb: wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHDfFkUyRsEB"
   },
   "source": [
    "# Evaluate Generative Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "NulK0v0Ivy4e",
    "outputId": "906d6cc6-6abb-4972-cb14-cfad57087d7f"
   },
   "outputs": [],
   "source": [
    "# input example (from training dataset)\n",
    "input_example = notes_list[0]\n",
    "print(\"len(input_example) =\", len(input_example))\n",
    "notes_to_midi(input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLrWvnt35lgf"
   },
   "outputs": [],
   "source": [
    "n_starting_notes = 24  # how many real notes to start with\n",
    "starting_notes = input_example[:n_starting_notes]\n",
    "#notes_to_midi(starting_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMVARJ7_58_U"
   },
   "source": [
    "Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 961
    },
    "id": "K4z_uVprWeSE",
    "outputId": "edfb46e4-a77e-4ca0-c093-7400cd82566e"
   },
   "outputs": [],
   "source": [
    "# generate output variations\n",
    "context = starting_notes.detach().to(device=device).unsqueeze(0)\n",
    "\n",
    "for variation in range(1):\n",
    "    notes_gen = m.generate(context, max_new_tokens=80, temperature=1)[0]\n",
    "    print(len(input_example), len(notes_gen))\n",
    "    notes_to_midi(notes_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCDYye2K5feF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1ppXg5iCuYr"
   },
   "source": [
    "Links:\n",
    "\n",
    "https://archives.ismir.net/ismir2021/latebreaking/000005.pdf\n",
    "\n",
    "https://dl.acm.org/doi/10.1145/3394171.3413671\n",
    "\n",
    "https://miditok.readthedocs.io/en/latest/index.html\n",
    "\n",
    "https://github.com/lucasnfe/adl-piano-midi\n",
    "\n",
    "https://medium.com/mlearning-ai/generating-music-with-gpt-b0f4ab738b58\n",
    "\n",
    "https://arxiv.org/pdf/1809.04281.pdf\n",
    "\n",
    "https://magenta.tensorflow.org/music-transformer\n",
    "\n",
    "https://colab.research.google.com/notebooks/magenta/piano_transformer/piano_transformer.ipynb\n",
    "\n",
    "https://github.com/gwinndr/MusicTransformer-Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Data Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_list[0][:,2].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(1) == int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30NfldFuDHpn"
   },
   "outputs": [],
   "source": [
    "bins=400\n",
    "\n",
    "def plot_hist(songs, bins=None, max_time=None):\n",
    "    fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "    ax[0].set_ylabel(\"Count\")\n",
    "    if type(songs[0]) == int: \n",
    "        songs = [notes_list[s] for s in songs]\n",
    "    for s_ind, song in enumerate(songs):\n",
    "        for i in range(len(ax)):\n",
    "            ax[i].set_xlabel(f\"{['Notes','Steps','Durations'][i]}\")\n",
    "            data = song[:,i]\n",
    "            minval, maxval =  data.min().item(), data.max().item()\n",
    "            if i > 0 and max_time is not None: maxval = max_time\n",
    "            bins = int(len(data))//4 if bins is None else bins\n",
    "            hist = np.array(torch.histc(data, bins=bins, min=minval, max=maxval).tolist())\n",
    "            t = np.linspace(minval, maxval, num=bins)\n",
    "            modes = np.argpartition(hist, -2)[-2:]\n",
    "            mode = modes[(i+1) % 2]\n",
    "            mode = minval + mode*(maxval-minval)/bins\n",
    "            \n",
    "            print(\"s_ind, i, minval, maxval, bins, mode = \",s_ind, i, minval, maxval, bins, mode)\n",
    "\n",
    "            #print(\"i, maxval, bins = \",i, maxval, bins)\n",
    "            ax[i].bar(t, hist, align='center', width=(maxval-minval)/bins, alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "for i in [0,1,2,3]:\n",
    "    notes_to_midi(notes_list[i])\n",
    "    plot_hist([i], bins=32, max_time=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_to_midi(input_example)\n",
    "plot_hist([input_example], bins=32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_to_midi(notes_gen)\n",
    "plot_hist([notes_gen], bins=32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist([all_notes], bins=32, max_time=2.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pitches.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1681d2d473564f4e85e82c41daa06607": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_62fb22845f01443eba2b42ec7f3674cb",
       "IPY_MODEL_d5cb1a9239174a218c9440a96120b4ba"
      ],
      "layout": "IPY_MODEL_54c97a105c9f4750a0ba7f80bb30fecc"
     }
    },
    "54c97a105c9f4750a0ba7f80bb30fecc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62fb22845f01443eba2b42ec7f3674cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_707d8dce5c9e4a34bb5e1dd32e649f18",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_909ae16959dc472e8e6b7657ed27b31d",
      "value": "0.012 MB of 0.012 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "707d8dce5c9e4a34bb5e1dd32e649f18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "909ae16959dc472e8e6b7657ed27b31d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9bbf5bc473e047f68909695b6360bbbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d5cb1a9239174a218c9440a96120b4ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f75e27a45d5d4b15ad9bb6419f53710c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9bbf5bc473e047f68909695b6360bbbc",
      "value": 0.9867716279824704
     }
    },
    "f75e27a45d5d4b15ad9bb6419f53710c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

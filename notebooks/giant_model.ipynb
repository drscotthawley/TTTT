{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT5iM-Dq6J1A"
   },
   "source": [
    "Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXLciiRULajm",
    "outputId": "59a38e26-a1ab-4c6d-f375-8345d1a3b0f1"
   },
   "outputs": [],
   "source": [
    "#!sudo apt install -qq -y fluidsynth >/dev/null;  # that's for Linux.  on Mac: \"brew install fluidsynth\"\n",
    "#!pip install -Uqq pyfluidsynth pretty_midi wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kjDFe6tNztJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "import mido \n",
    "import pathlib\n",
    "import pretty_midi\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import multiprocessing as mp\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "import random\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtPj8fGF6I2V",
    "outputId": "d04cd8de-4ef4-4dff-ed22-1ac7e1ebf171"
   },
   "outputs": [],
   "source": [
    "\n",
    "source_dataset = 'jsb'\n",
    "\n",
    "!rm -rf midi_files\n",
    "if source_dataset == 'groove':\n",
    "    !wget -N https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip\n",
    "    !unzip -n -qq groove-v1.0.0-midionly.zip\n",
    "    !ln -s groove midi_files\n",
    "elif source_dataset == 'maestro':\n",
    "    !wget -N https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip\n",
    "    !unzip -n -qq maestro-v3.0.0-midi.zip\n",
    "    !ln -s maestro-v3.0.0 midi_files\n",
    "elif source_dataset == 'js-fakes':\n",
    "    !ln -s js-fakes midi_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJCJGzlOKycp",
    "outputId": "2b01ddbd-0019-47d1-c829-599222dfd09f"
   },
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('midi_files')\n",
    "filenames = glob(str(data_dir/'**/*.mid*'), recursive=True)\n",
    "print('Number of files:', len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ex5X1GC6LN5A"
   },
   "outputs": [],
   "source": [
    "vocab_size=128 # number of midi notes\n",
    "n_contin = 2   # number of continuous variables besides pitches\n",
    "\n",
    "\n",
    "def time_convert(time_s, bpm, pm, time_units='ticks'):\n",
    "    if time_units == 'beats':\n",
    "        bps = bpm/60\n",
    "        beats = time_s * bps\n",
    "        #print(\"time_s, bps, beats = \",time_s, bps, beats)\n",
    "        return beats\n",
    "    elif time_units == 'ticks':  # 500000 ticks per beat\n",
    "        return pm.time_to_tick(time_s)\n",
    "    return time_s  # leave it in seconds\n",
    "\n",
    "\n",
    "def midi_file_to_tensor_old(midi_file):\n",
    "    pm = pretty_midi.PrettyMIDI(midi_file) # read in the whole file. this is incredibly slow\n",
    "\n",
    "    # Sort the notes first by start time (then by pitch if two notes start at the same time)\n",
    "    sorted_notes = sorted(pm.instruments[0].notes, key=lambda note: (note.start, note.pitch))\n",
    "    notes = torch.empty( (len(sorted_notes), 3), dtype=torch.float32 ) # allocate storage\n",
    "\n",
    "    prev_start = sorted_notes[0].start\n",
    "    for i, note in enumerate(sorted_notes):\n",
    "        notes[i] = note.pitch\n",
    "        notes[i, 1] = note.start - prev_start  # step, i.e. time since last note started\n",
    "        notes[i, 2] = note.end - note.start    # duration\n",
    "        prev_start = note.start\n",
    "\n",
    "    return notes\n",
    "\n",
    "\n",
    "def midi_file_to_tensor(midi_file,\n",
    "                        time_units='ticks', # beats, ticks, s\n",
    "                        info=False,  # return info about the track\n",
    "                       ):\n",
    "    pm = pretty_midi.PrettyMIDI(midi_file) # read in the whole file. this is incredibly slow\n",
    "    bpm = pm.estimate_tempo()\n",
    "    mid = mido.MidiFile(midi_file)\n",
    "    tpb = mid.ticks_per_beat\n",
    "    tps = 60000.0 / (bpm * tpb) \n",
    "    spt = mido.tick2second(1, tpb, 500000 )\n",
    "    # Sort the notes first by start time (then by pitch if two notes start at the same time)\n",
    "    sorted_notes = sorted(pm.instruments[0].notes, key=lambda note: (note.start, note.pitch))\n",
    "    notes = torch.empty( (len(sorted_notes), 3), dtype=torch.float32 ) # allocate storage\n",
    "    \n",
    "    prev_start = sorted_notes[0].start\n",
    "    for i, note in enumerate(sorted_notes):\n",
    "        notes[i] = note.pitch\n",
    "        notes[i, 1] = note.start - prev_start  # step, time since last note\n",
    "        notes[i, 2] = note.end - note.start    # duration\n",
    "        prev_start = note.start\n",
    "\n",
    "        #notes[i, 1] = time_convert(notes[i, 1], bpm, pm, time_units=time_units) # don't rescale any timing yet\n",
    "        #notes[i, 2] = time_convert(notes[i, 2], bpm, pm, time_units=time_units)\n",
    "\n",
    "    #notes[:,1:] = notes[:,1:]//(tpb/16) # don't quantize in time yet\n",
    "    if info:\n",
    "        return notes, {'bpm': bpm, 'ticks_per_beat':tpb, 'seconds_per_tick':spt}\n",
    "    else:\n",
    "        return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3xP4JuTOCkH",
    "outputId": "514fba3e-7b11-403e-b432-e769526b64fe"
   },
   "outputs": [],
   "source": [
    "#notes, info = midi_file_to_tensor(filenames[0], info=True)\n",
    "#print(info)\n",
    "#pitches = notes[:,0].type(torch.long)  # just the pitch info\n",
    "#notes.shape, pitches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lUOaORcKTRW4"
   },
   "outputs": [],
   "source": [
    "# @title Tensor to MIDI Display Code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def notes_arr_to_df(notes_arr) -> pd.DataFrame:\n",
    "    columns = ['pitch','step','duration']\n",
    "    df = pd.DataFrame(notes_arr, columns=columns)\n",
    "    df[\"start\"] = \"\"\n",
    "    df[\"end\"] = \"\"\n",
    "\n",
    "    prev_start = 0\n",
    "    #for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    for i, row in df.iterrows():\n",
    "        start = prev_start + float(row['step'])\n",
    "        df.at[i, 'start'] = start\n",
    "        df.at[i, 'end'] = start + float(row['duration'])\n",
    "        prev_start = start\n",
    "    return df\n",
    "\n",
    "def df_to_midi(\n",
    "        notes_df: pd.DataFrame,\n",
    "        out_file: str = '',  # output file to save to, if any\n",
    "        instrument_name: str = 'Acoustic Grand Piano', # whatever you want to call this instrument\n",
    "        velocity: int = 100,  # note loudness\n",
    "    ) -> pretty_midi.PrettyMIDI:\n",
    "    \"converts a dataframe to valid midi\"\n",
    "\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.Instrument(\n",
    "        program=pretty_midi.instrument_name_to_program(\n",
    "            instrument_name))\n",
    "\n",
    "    prev_start = 0\n",
    "    for i, note in notes_df.iterrows(): # this is a serial operation, not sure how to parallelize\n",
    "        start = float(prev_start + note['step'])\n",
    "        end = float(start + note['duration'])\n",
    "        note = pretty_midi.Note(\n",
    "            velocity=velocity,\n",
    "            pitch=int(note['pitch']),\n",
    "            start=start,\n",
    "            end=end,\n",
    "        )\n",
    "        instrument.notes.append(note)\n",
    "        prev_start = start\n",
    "\n",
    "    pm.instruments.append(instrument)\n",
    "    if out_file: pm.write(out_file)\n",
    "    return pm\n",
    "\n",
    "def plot_piano_roll(notes_df: pd.DataFrame, count: Optional[int] = None):\n",
    "    \"produce a piano roll plot\"\n",
    "    if count:\n",
    "        title = f'First {count} notes'\n",
    "    else:\n",
    "        title = f'Whole track'\n",
    "        count = len(notes_df['pitch'])\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plot_pitch = np.stack([notes_df['pitch'], notes_df['pitch']], axis=0)\n",
    "    plot_start_stop = np.stack([notes_df['start'], notes_df['end']], axis=0)\n",
    "    plt.plot(\n",
    "        plot_start_stop[:, :count], plot_pitch[:, :count], color=\"b\", marker=\".\")\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Pitch')\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim([0, vocab_size])\n",
    "    _ = plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def midi_to_audio(pm: pretty_midi.PrettyMIDI, seconds=30, sr=16000):\n",
    "  \"midi to audio, playable in notebook\"\n",
    "  waveform = pm.fluidsynth(fs=float(sr))\n",
    "  # Take a sample of the generated waveform to mitigate kernel resets\n",
    "  waveform_short = waveform[:seconds*sr]\n",
    "  return display(Audio(waveform_short, rate=sr))\n",
    "\n",
    "def pitches_to_midi(pitch_list, seconds=30):\n",
    "    notes_tensor = torch.zeros((len(pitch_list), 3)) + 0.25\n",
    "    for i, p in enumerate(pitch_list):\n",
    "        notes_tensor[i,0] = p\n",
    "    notes_df = notes_arr_to_df(notes_tensor.cpu().detach().numpy())\n",
    "    midi = df_to_midi(notes_df)\n",
    "    plot_piano_roll(notes_df)\n",
    "    audio_display = midi_to_audio(midi, seconds=seconds)\n",
    "    return audio_display\n",
    "\n",
    "def notes_to_midi(notes_tensor, seconds=30, time_rescale=None):\n",
    "    notes_tensor = notes_tensor.clone() # just to avoid weird overwrites of memory\n",
    "    #notes_tensor = notes_tensor * (notes_tensor>0)  # negative numbers clipped to zero\n",
    "    if notes_tensor.min() < 0.0:\n",
    "      print(\"WARNING: You have negative pitches, steps or durations. Setting them to zero\")\n",
    "      notes_tensor = notes_tensor * (notes_tensor >= 0)\n",
    "    if time_rescale is not None :\n",
    "        notes_tensor[:,1:] = notes_tensor[:,1:] *time_rescale # no quantization, just rescaling time\n",
    "    notes_df = notes_arr_to_df(notes_tensor.cpu().detach().numpy())\n",
    "    midi = df_to_midi(notes_df)\n",
    "    plot_piano_roll(notes_df)\n",
    "    audio_display = midi_to_audio(midi, seconds=seconds)\n",
    "    return audio_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzE1MiIUbroR"
   },
   "outputs": [],
   "source": [
    "def files_to_tensor_list(filenames): \n",
    "    tensor_list = process_map(midi_file_to_tensor, filenames, max_workers=mp.cpu_count(), chunksize=1)\n",
    "    return tensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "Gz--Hv1inCIg",
    "outputId": "2082c7e3-a493-45dd-ec8a-77357b86e2d4"
   },
   "outputs": [],
   "source": [
    "#uncomment if needed\n",
    "#notes_list = files_to_tensor_list(filenames)\n",
    "#print(f\"\\n{len(notes_list)} files read\")\n",
    "#torch.save(notes_list, 'maestro3_tensor_list.pt') # save for next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXjJjXiNC0ov"
   },
   "outputs": [],
   "source": [
    "notes_list = torch.load('maestro3_tensor_list.pt')  # load from previous computation\n",
    "#notes_list = torch.load('rastro-120bpm_16th_tensor_list.pt')  # load from previous computation\n",
    "len(notes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantize in time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_quantize(notes_tensor,  # a single song\n",
    "                  time_res=0.008, # resolution in seconds.  8ms is from Google \"This Time With Feeling\" paper\n",
    "                  t_max=1.0, # again, from Google paper. This will give us from 0 to 1 second. Anything beyond that gets clipped\n",
    "                  use_buckets=True,\n",
    "                 ):\n",
    "    nt2 = notes_tensor.contiguous().clone()\n",
    "    if use_buckets:\n",
    "        bucket_vals = torch.arange(0, t_max, time_res)\n",
    "        boundaries = torch.arange(time_res/2, t_max - time_res/2, time_res)\n",
    "        inds = torch.bucketize(nt2[:,1:].contiguous(), boundaries)\n",
    "        #nt2[:,1:] = bucket_vals[inds]\n",
    "        nt2[:,1:] = inds  # time is now in divisions with resolution time_res \n",
    "    else:\n",
    "        nt2[:,1:] = torch.clamp(torch.floor(nt2[:,1:]/time_res)*time_res, 0.0, t_max)\n",
    "    return nt2\n",
    "\n",
    "quant_notes_list = [time_quantize(q) for q in notes_list]\n",
    "torch.save([q.type(torch.int16) for q in quant_notes_list], 'maestro3_tensor_list_quant.pt') # all integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_list = quant_notes_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "UhrtxPKjXeTv",
    "outputId": "7178904f-f7bb-41ce-e5b0-2fdff6f2887b"
   },
   "outputs": [],
   "source": [
    "use_wandb = True\n",
    "if use_wandb:\n",
    "    import wandb\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(\"device =\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QlkM3xo6WhL"
   },
   "source": [
    "# Dataset creation\n",
    "\n",
    "RASTRO: \"Reduced Accessible Songs for Training, Rhythmically Oversimplified\"\n",
    "\n",
    "**TODO:** change dataset from one big string of integers to reading from individual songs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaAMdat36Vj4",
    "outputId": "483d96da-120b-4bd3-a85b-a463eeeaba04"
   },
   "outputs": [],
   "source": [
    "def tl_to_notes(tensor_list, shuffle=False, delimit=True):\n",
    "  \"list of tensors (of arbitrary length, for each song) converted to one big long tensor of notes all running togehter\"\n",
    "  if shuffle:random.shuffle(tensor_list)\n",
    "  if delimit:\n",
    "    delimiter = torch.zeros(3)  # use all zeros to show ends of songs\n",
    "    tensor_list = [element for item in tensor_list for element in (item, delimiter)]\n",
    "  return torch.vstack(tensor_list).type(torch.float32)  # return one big tensor of floats\n",
    "\n",
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "all_notes = tl_to_notes(notes_list, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsb\n",
    "if source_dataset == 'jsb': \n",
    "    all_notes = torch.load('jsb_tensor_rests.pt').type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaAMdat36Vj4",
    "outputId": "483d96da-120b-4bd3-a85b-a463eeeaba04"
   },
   "outputs": [],
   "source": [
    "all_pitches = all_notes[:,0].type(torch.long)  # just the pitch info\n",
    "\n",
    "#info about our data\n",
    "print(\"all_notes.shape =\",all_notes.shape)\n",
    "print(\"steps: min max = \",all_notes[:,1].min(), all_notes[:,1].max())\n",
    "print(\"dur: min max =\",all_notes[:,2].min(), all_notes[:,2].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(torch.unique(all_notes[:,1])), len(torch.unique(all_notes[:,2])),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_notes[0:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_notes[:,1:] = torch.clamp(all_notes[:,1:], 0, 5.76) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallest_time = all_notes[:,1:][all_notes[:,1:]>0].min()\n",
    "print(\"smallest_time = \",smallest_time)\n",
    "all_notes[:,1:] = all_notes[:,1:]/smallest_time  #integer times, need to convert later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outbound_t_rescale(notes_tensor, time_min=1.0):\n",
    "    nt2 = notes_tensor.contiguous().clone()\n",
    "    nt2[:,1:] = nt2[:,1:]*time_min\n",
    "    return nt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=128\n",
    "#time_rescale = 0.008 # per Magenta specs\n",
    "time_rescale = 1 # for my JSB retread\n",
    "notes_to_midi(outbound_t_rescale(all_notes[0:120],time_min=smallest_time), time_rescale=time_rescale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = all_notes # all_notes\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(\"len(train_data), len(val_data) =\",len(train_data), len(val_data) )\n",
    "#assert n_contin == train_data.shape[-1]-1  # data consists of pitches plus continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_notes[:,1].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_notes[:,2].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_notes[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: need to do proper vocab mapping for pitches, steps and durations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 128 \n",
    "print(\"pitch spread = \",len(all_notes[:,0].unique()))\n",
    "pitch_vocab_size = vocab_size # \n",
    "#steps_size = len(all_notes[:,1].unique())\n",
    "#durs_size = len(all_notes[:,2].unique())\n",
    "step_vocab_size = int(all_notes[:,1].max()+1)\n",
    "dur_vocab_size = int(all_notes[:,2].max()+1)\n",
    "\n",
    "pitch_vocab_size, step_vocab_size, dur_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iReoycw0b07C"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y06bDCiX6R0C"
   },
   "outputs": [],
   "source": [
    "\n",
    "# hyperparameters\n",
    "learning_rate = 1e-3\n",
    "#learning_rate = 5e-5  # for onecycle schedule\n",
    "\n",
    "'''\n",
    "# Overfit parameters:\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 128 # what is the maximum context length for predictions?\n",
    "n_embd = 384 #256# 64  # dimensions of embeddings\n",
    "n_head = 16# 4\n",
    "n_layer = 16# 4\n",
    "dropout = 0.1 #0.1\n",
    "\n",
    "max_iters = 25000\n",
    "eval_interval = 100\n",
    "eval_iters = 100\n",
    "use_alibi = False\n",
    "'''\n",
    "\n",
    "'''\n",
    "Underfit parameters:\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 128 # what is the maximum context length for predictions?\n",
    "n_embd = 64 #256# 64  # dimensions of embeddings\n",
    "n_head = 8# 4\n",
    "n_layer = 6# 4\n",
    "dropout = 0.1 #0.1\n",
    "'''\n",
    "\n",
    "'''\n",
    "# large sequence length\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "n_embd = 256 # 64  # dimensions of embeddings\n",
    "n_head = 16 # 4\n",
    "n_layer = 8 # # 4\n",
    "dropout = 0.1 #0.1\n",
    "'''\n",
    "\n",
    "# jsb\n",
    "batch_size = 128 # how many independent sequences will we process in parallel?\n",
    "block_size = 128 # what is the maximum context length for predictions?\n",
    "n_embd = 64 # 64  # dimensions of embeddings\n",
    "n_head = 8 # 4\n",
    "n_layer = 4 # # 4\n",
    "dropout = 0.1 #0.1\n",
    "\n",
    "\n",
    "max_iters = 25000\n",
    "eval_interval = 100\n",
    "eval_iters = 100\n",
    "use_alibi = True\n",
    "\n",
    "\n",
    "# ------------\n",
    "\n",
    "config = {\n",
    "  'learning_rate': learning_rate,\n",
    "  'batch_size': batch_size,\n",
    "  'block_size': block_size,\n",
    "  'n_embd': n_embd,\n",
    "  'n_head': n_head,\n",
    "  'n_layer': n_layer,\n",
    "  'dropout': dropout,\n",
    "  'use_alibi': use_alibi\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"removing short songs, for block_size = {block_size}. Current len(notes_list) = {len(notes_list)}\")\n",
    "notes_list = [q for q in notes_list if q.shape[0] > block_size+1 ]\n",
    "print(\"new len(notes_list) =\",len(notes_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(db, pb=12): # db = datablock - a seqeunce , likely of length 1+block_size \n",
    "    db = db.clone()  # avoid in-place alterations of data\n",
    "    #minpitch, maxpitch = int(db[::,0].min().item()),  int(db[::,0].max().item())\n",
    "    pitch_shift = torch.randint(low=-pb, high=pb, size=(1,1,1), dtype=torch.long).item()\n",
    "    db[:,0] = torch.where( db[:,0] < 127,  db[:,0]+pitch_shift,  db[:,0])  \n",
    "    #time_scale = torch.randint(1, 2, (1,)) \n",
    "    #db[::,0] = torch.clamp(db[::,0] + pitch_shift, 0, pitch_vocab_size-1) \n",
    "    #db[:,:,1] = torch.clamp(db[:,:,1] * time_scale,  0, step_vocab_size-1)   \n",
    "    #db[:,:,2] = torch.clamp(db[:,:,2] * time_scale,  0, dur_vocab_size-1)  \n",
    "    return db\n",
    "\n",
    "class NotesDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 notes, # notes is either a list of tensors (i.e. songs) or a tensor (all notes together)\n",
    "                 block_size, \n",
    "                 augment=False):\n",
    "        super().__init__()\n",
    "        self.notes = notes\n",
    "        self.augment = augment   # set to True for training \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.notes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.notes[idx] if type(self.notes) is list else self.notes \n",
    "        try:\n",
    "            i = torch.randint(0, data.shape[0]-1 - block_size, (1,)) \n",
    "        except: \n",
    "            assert False, f\"data.shape[0] = {data.shape[0]}, block_size = {block_size}. type(self.notes) = {type(self.notes)}\"\n",
    "        data_block = data[i:i+block_size+1]  # grab input sequence plus the next note\n",
    "        if self.augment:\n",
    "            data_block = augment_data( data_block )\n",
    "        x, y = data_block[:-1], data_block[1:] \n",
    "        return x, y                   \n",
    "        \n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "use_song_list = False\n",
    "notes = notes_list if use_song_list else all_notes \n",
    "n_split = int(0.8*len(notes))\n",
    "train_ds = NotesDataset(notes[:n_split], block_size, augment=True)\n",
    "val_ds   = NotesDataset(notes[n_split:], block_size, augment=False)\n",
    "\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=12)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "train_dl_iter = cycle(train_dl)\n",
    "val_dl_iter = cycle(val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaAMdat36Vj4",
    "outputId": "483d96da-120b-4bd3-a85b-a463eeeaba04"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_batch(split, debug=False):\n",
    "    '''\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.clone(), y.clone()  # just to avoid overwrites\n",
    "    if split == 'train': # data augmentation\n",
    "        x, y = augment_data(x,y)\n",
    "    x[:,0,1] = x[:,0,1] * 0  # set first step of every x sequence to 0 (this shouldn't make a difference though)\n",
    "    '''\n",
    "    x, y = next(train_dl_iter) if split == 'train' else next(val_dl_iter)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        pitch_losses = torch.zeros(eval_iters)\n",
    "        step_losses = torch.zeros(eval_iters)\n",
    "        dur_losses = torch.zeros(eval_iters)\n",
    "\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            pitch_logits, step_logits, dur_logits, loss, sublosses = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "            pitch_losses[k] = sublosses['pitch_loss']\n",
    "            step_losses[k] = sublosses['step_loss']\n",
    "            dur_losses[k] = sublosses['dur_loss']\n",
    "        out[split] = losses.mean()\n",
    "        out['pitch_loss'] = pitch_losses.mean()\n",
    "        out['step_loss'] = step_losses.mean()\n",
    "        out['dur_loss'] = dur_losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9PfGh_57IY0",
    "outputId": "46fccc5f-943d-46df-db5a-33ddae856374"
   },
   "outputs": [],
   "source": [
    "x,y = get_batch('val',debug=True)\n",
    "print(\"x.shape = \",x.shape)\n",
    "print(f\"B, T = {batch_size}, {block_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Kr70iKZ7pPd"
   },
   "source": [
    "x is a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "ZBwJTuc17kxe",
    "outputId": "e493eae3-ac40-4335-c6b4-14653df7c302"
   },
   "outputs": [],
   "source": [
    "#time_rescale = info['seconds_per_tick']*grid_resolution\n",
    "ind = 2\n",
    "notes_to_midi(x[ind], time_rescale=time_rescale)\n",
    "x[ind][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WP9U8aIm7qh3"
   },
   "source": [
    "y is x shifted back by one and including new data.\n",
    "in this sense only y[:,-1] is the \"next token\" being predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "5Wjb4QMS7lub",
    "outputId": "54fe6210-448d-42ea-8d92-38d80d4cd9e9"
   },
   "outputs": [],
   "source": [
    "notes_to_midi(y[ind], time_rescale=time_rescale)\n",
    "y[ind][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CO7ZJhu-6dvk"
   },
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOeDesr4348z"
   },
   "outputs": [],
   "source": [
    "def create_alibi_mask(n):\n",
    "    # Create an n x n matrix filled with zeros\n",
    "    mat = torch.zeros((n, n))\n",
    "    \n",
    "    # Iterate over the lower triangular indices (excluding the diagonal)\n",
    "    for i in range(1, n):\n",
    "        for j in range(i):\n",
    "            mat[i, j] = -(i - j)\n",
    "    \n",
    "    # Create a mask for the upper triangular part, excluding the diagonal\n",
    "    mask = torch.triu(torch.ones_like(mat), diagonal=1)\n",
    "    \n",
    "    # Apply the mask and set the upper triangular part to -inf\n",
    "    mat[mask.bool()] = float('-inf')\n",
    "    \n",
    "    return mat\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.use_alibi = use_alibi\n",
    "        if use_alibi:\n",
    "            self.register_buffer('alibi', create_alibi_mask(block_size))\n",
    "        else:\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        if self.use_alibi:\n",
    "            wei = wei + self.alibi[:T,:T]\n",
    "        else:\n",
    "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def mse_with_positive_pressure(y_true: torch.Tensor, y_pred: torch.Tensor):\n",
    "  # from Magenta example, converted to pytorch\n",
    "  se = (y_true - y_pred) ** 2\n",
    "  positive_pressure = 10 * torch.clamp(-y_pred, min=0) # ten times the negative values made positive\n",
    "  return (se + positive_pressure).mean()\n",
    "\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_embeddings = False\n",
    "        if self.add_embeddings: \n",
    "            pitch_embd, step_emb, dur_embd = [n_embd]*3\n",
    "        else: \n",
    "            pitch_embd, step_emb, dur_embd = int(n_embd*0.5), int(n_embd*0.25), int(n_embd*0.25) \n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.pitch_embedding_table = nn.Embedding(pitch_vocab_size, pitch_embd)\n",
    "        self.step_embedding_table = nn.Embedding(step_vocab_size, step_emb)\n",
    "        self.dur_embedding_table = nn.Embedding(dur_vocab_size, dur_embd)\n",
    "\n",
    "        if not use_alibi: self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, pitch_vocab_size, bias=False)\n",
    "        self.step_head = nn.Linear(n_embd, step_vocab_size, bias=False)\n",
    "        self.dur_head = nn.Linear(n_embd, dur_vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, inputs, targets=None):\n",
    "        device = inputs.device\n",
    "        pitch_idx, step_idx, dur_idx = [inputs[:,:,i].type(torch.long).to(device) for i in [0,1,2]]  # indices\n",
    "        B, T = pitch_idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        pitch_tok_emb = self.pitch_embedding_table(pitch_idx) # (B,T,C) # embed the pitches\n",
    "        step_tok_emb  = self.step_embedding_table(step_idx)   # (B,T,C) # embed the steps\n",
    "        dur_tok_emb   = self.dur_embedding_table(dur_idx)     # (B,T,C) # embed the durations\n",
    "\n",
    "        if self.add_embeddings:\n",
    "            x = pitch_tok_emb + step_tok_emb + dur_tok_emb  # try just adding them? \n",
    "        else:\n",
    "            x = torch.cat((pitch_tok_emb, step_tok_emb, dur_tok_emb), dim=-1)   # concat the separate embeddings\n",
    "\n",
    "        if not use_alibi:\n",
    "            pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "            x = x + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x)   # (B,T,C)\n",
    "        pitch_logits = self.lm_head(x)    # (B,T,pitch_vocab_size)\n",
    "        step_logits  = self.step_head(x)  # (B,T,step_vocab_size)\n",
    "        dur_logits   = self.dur_head(x)   # (B,T,dur_vocab_size)\n",
    "               \n",
    "        # we can also compute losses if targets are included in .forward call\n",
    "        if targets is None:\n",
    "            return pitch_logits, step_logits, dur_logits, None\n",
    "        else:\n",
    "            B, T, C = pitch_logits.shape\n",
    "            pitch_logits, step_logits, dur_logits = [q.view(B*T,-1) for q in [pitch_logits, step_logits, dur_logits]]\n",
    "            pitch_targets, step_targets, dur_targets = [targets[:,:,i].type(torch.long).to(device).view(B*T) for i in [0,1,2]]\n",
    "            \n",
    "            lambda_pitch, lambda_step, lambda_dur = 0.5, 0.5, 0.5  # scale relative losses\n",
    "            #lambda_pitch, lambda_step, lambda_dur = 0.5, 0.5, 0.1  # scale relative losses\n",
    "            #lambda_pitch, lambda_step, lambda_dur = 0.5, 0.1666, 0.2  # scale relative losses\n",
    "\n",
    "            pitch_loss = lambda_pitch * F.cross_entropy(pitch_logits, pitch_targets)\n",
    "            step_loss  = lambda_step  * F.cross_entropy(step_logits,  step_targets)\n",
    "            dur_loss   = lambda_dur   * F.cross_entropy(dur_logits,   dur_targets)\n",
    "\n",
    "            sublosses = {'pitch_loss':pitch_loss.item(), 'step_loss':step_loss.item(), 'dur_loss':dur_loss.item() } \n",
    "\n",
    "            loss = pitch_loss + step_loss + dur_loss\n",
    "            return pitch_logits, step_logits, dur_logits, loss, sublosses\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        # idx is (B, T, 1+contin_vars) array of values in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            pitch_logits, step_logits, dur_logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            pitch_logits, step_logits, dur_logits = [q[:, -1, :] for q in [pitch_logits, step_logits, dur_logits]] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            pitch_probs = F.softmax(pitch_logits/temperature, dim=-1) # (B, C)\n",
    "            step_probs  = F.softmax(step_logits /temperature, dim=-1) # (B, C)\n",
    "            dur_probs   = F.softmax(dur_logits  /temperature, dim=-1) # (B, C)\n",
    "\n",
    "            # sample from the distribution\n",
    "            pitch_idx_next = torch.multinomial(pitch_probs, num_samples=1) # (B, 1)\n",
    "            step_idx_next  = torch.multinomial(step_probs,  num_samples=1) # (B, 1)\n",
    "            dur_idx_next   = torch.multinomial(dur_probs,   num_samples=1) # (B, 1)\n",
    "\n",
    "            # concat continuous variables\n",
    "            idx_next = torch.cat((pitch_idx_next, step_idx_next, dur_idx_next), dim=-1).unsqueeze(0) # unsqueeze to add dummy batch dim\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx.cpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSUO8GJx6tpw"
   },
   "source": [
    "Instantiate and get ready to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3yYAD0t6tOK",
    "outputId": "d1c3bad1-0b15-474c-aa27-7ca578411511"
   },
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "#scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, 0.002, total_steps=max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(step, model, optimizer,loss, name):\n",
    "    name = name + f'_{step}.pt'\n",
    "    print(\"Saving checkpoint to\",name)\n",
    "    torch.save({\n",
    "            'step': step,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, name)\n",
    "\n",
    "def load_checkpoint(name, model, optimizer):\n",
    "    checkpoint = torch.load(name)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    m = model.to(device)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    step = checkpoint['step']\n",
    "    loss = checkpoint['loss']\n",
    "    return step, m, optimizer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_the_checkpoint = False\n",
    "\n",
    "if load_the_checkpoint:\n",
    "    step, model, optimizer, loss = load_checkpoint('augemented_cuda_checkpoint.pt', model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "eh9Neb2tYwsR",
    "outputId": "1e143ce1-caad-4968-a673-92c3892e31c8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_wandb = True\n",
    "if use_wandb: wandb.init(project='musicbox-jsb', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mO4XTmlj58La"
   },
   "source": [
    "# Do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YCWAxp3I57eO",
    "outputId": "a93ab45a-6317-45db-d56f-8140b377fce9"
   },
   "outputs": [],
   "source": [
    "loss_hist = []\n",
    "checkpoint_every = 1000\n",
    "for iteri in range(max_iters): # \"iter\" already means something in python, so \"iteri\" here\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iteri % eval_interval == 0 or iteri == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"iter {iteri}: losses: train {losses['train']:.4f}, val {losses['val']:.4f}, pitch {losses['pitch_loss']:.4f},\",\n",
    "              f\"step_loss {losses['step_loss']:.4f} dur_loss {losses['dur_loss']:.4f}\")\n",
    "        if use_wandb: wandb.log(losses | {'iter':(iteri)//eval_interval,  'lr':optimizer.param_groups[0]['lr'] })\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    pitch_logits, step_logits, dur_logits, loss, sublosses = model(xb, yb)\n",
    "\n",
    "    if (iteri>0) and (iteri % checkpoint_every==0):\n",
    "        save_checkpoint(iteri, model, optimizer, loss, \"giant_model_quantized_checkpoint-noalibi\")\n",
    "        \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(iteri, model, optimizer,loss, f\"giant_model_quantized-{'no' if use_alibi==False else ''}alibi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290,
     "referenced_widgets": [
      "1681d2d473564f4e85e82c41daa06607",
      "62fb22845f01443eba2b42ec7f3674cb",
      "d5cb1a9239174a218c9440a96120b4ba",
      "54c97a105c9f4750a0ba7f80bb30fecc",
      "707d8dce5c9e4a34bb5e1dd32e649f18",
      "909ae16959dc472e8e6b7657ed27b31d",
      "f75e27a45d5d4b15ad9bb6419f53710c",
      "9bbf5bc473e047f68909695b6360bbbc"
     ]
    },
    "id": "rD-eaZO7ZhH0",
    "outputId": "e97a230f-d490-4492-d19a-2765b221daaa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_wandb: wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHDfFkUyRsEB"
   },
   "source": [
    "# Evaluate Generative Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteri, model, optimizer, loss = load_checkpoint('giant_model_quantized.pt', model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "NulK0v0Ivy4e",
    "outputId": "906d6cc6-6abb-4972-cb14-cfad57087d7f"
   },
   "outputs": [],
   "source": [
    "# input examplze (from training dataset)\n",
    "time_rescale=0.008 if source_dataset != 'jsb' else .12\n",
    "input_example, _  = next(train_dl_iter)\n",
    "input_example = input_example[1]\n",
    "print(\"len(input_example) =\", len(input_example))\n",
    "notes_to_midi(input_example, time_rescale=time_rescale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLrWvnt35lgf"
   },
   "outputs": [],
   "source": [
    "n_starting_notes = 32  # how many real notes to start with\n",
    "starting_notes = input_example[:n_starting_notes]\n",
    "notes_to_midi(starting_notes, time_rescale=time_rescale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMVARJ7_58_U"
   },
   "source": [
    "Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 961
    },
    "id": "K4z_uVprWeSE",
    "outputId": "edfb46e4-a77e-4ca0-c093-7400cd82566e"
   },
   "outputs": [],
   "source": [
    "# generate output variations\n",
    "context = starting_notes.detach().to(device=device).unsqueeze(0)\n",
    "\n",
    "for variation in range(4):\n",
    "  notes = m.generate(context, max_new_tokens=128, temperature=1.0)[0]\n",
    "  notes_to_midi(notes, time_rescale=time_rescale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCDYye2K5feF"
   },
   "outputs": [],
   "source": [
    "notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generator(model:nn.Module, idx, max_new_tokens=10, temperature=1.0):\n",
    "    # idx is (B, T, 1+contin_vars) array of values in the current context\n",
    "    if len(idx.shape) < 3: idx = idx.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            pitch_logits, step_logits, dur_logits, loss = model(idx_cond.to(device))\n",
    "            print(f\"p s d shapes: {[q.shape for q in [pitch_logits, step_logits, dur_logits]]}\") \n",
    "            # focus only on the last time step\n",
    "            pitch_logits, step_logits, dur_logits = [q[:, -1, :].to(device) for q in [pitch_logits, step_logits, dur_logits]] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            pitch_probs = F.softmax(pitch_logits/temperature, dim=-1) # (B, C)\n",
    "            step_probs  = F.softmax(step_logits /temperature, dim=-1) # (B, C)\n",
    "            dur_probs   = F.softmax(dur_logits  /temperature, dim=-1) # (B, C)\n",
    "    \n",
    "            # sample from the distribution\n",
    "            pitch_idx_next = torch.multinomial(pitch_probs, num_samples=1) # (B, 1)\n",
    "            step_idx_next  = torch.multinomial(step_probs,  num_samples=1) # (B, 1)\n",
    "            dur_idx_next   = torch.multinomial(dur_probs,   num_samples=1) # (B, 1)\n",
    "    \n",
    "            # concat continuous variables\n",
    "            idx_next = torch.cat((pitch_idx_next, step_idx_next, dur_idx_next), dim=-1).unsqueeze(0) # unsqueeze to add dummy batch dim\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx.to(device), idx_next.to(device)), dim=1) # (B, T+1)\n",
    "    return idx.cpu()\n",
    "\n",
    "sn = starting_notes[:5]\n",
    "notes_pred = generator(model, sn, max_new_tokens=5)\n",
    "print(\"starting notes =\\n\",sn)\n",
    "print(\"notes_pred =\\n\",notes_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1ppXg5iCuYr"
   },
   "source": [
    "Links:\n",
    "\n",
    "https://archives.ismir.net/ismir2021/latebreaking/000005.pdf\n",
    "\n",
    "https://dl.acm.org/doi/10.1145/3394171.3413671\n",
    "\n",
    "https://miditok.readthedocs.io/en/latest/index.html\n",
    "\n",
    "https://github.com/lucasnfe/adl-piano-midi\n",
    "\n",
    "https://medium.com/mlearning-ai/generating-music-with-gpt-b0f4ab738b58\n",
    "\n",
    "https://arxiv.org/pdf/1809.04281.pdf\n",
    "\n",
    "https://magenta.tensorflow.org/music-transformer\n",
    "\n",
    "https://colab.research.google.com/notebooks/magenta/piano_transformer/piano_transformer.ipynb\n",
    "\n",
    "https://github.com/gwinndr/MusicTransformer-Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30NfldFuDHpn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1681d2d473564f4e85e82c41daa06607": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_62fb22845f01443eba2b42ec7f3674cb",
       "IPY_MODEL_d5cb1a9239174a218c9440a96120b4ba"
      ],
      "layout": "IPY_MODEL_54c97a105c9f4750a0ba7f80bb30fecc"
     }
    },
    "54c97a105c9f4750a0ba7f80bb30fecc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62fb22845f01443eba2b42ec7f3674cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_707d8dce5c9e4a34bb5e1dd32e649f18",
      "placeholder": "​",
      "style": "IPY_MODEL_909ae16959dc472e8e6b7657ed27b31d",
      "value": "0.012 MB of 0.012 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "707d8dce5c9e4a34bb5e1dd32e649f18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "909ae16959dc472e8e6b7657ed27b31d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9bbf5bc473e047f68909695b6360bbbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d5cb1a9239174a218c9440a96120b4ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f75e27a45d5d4b15ad9bb6419f53710c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9bbf5bc473e047f68909695b6360bbbc",
      "value": 0.9867716279824704
     }
    },
    "f75e27a45d5d4b15ad9bb6419f53710c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

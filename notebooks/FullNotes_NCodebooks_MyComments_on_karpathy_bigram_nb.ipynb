{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Karpathy video: https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "* Karpathy repo: https://github.com/karpathy/nanoGPT\n",
    "* Colab for video: https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT5iM-Dq6J1A"
   },
   "source": [
    "# Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = \"jsb\"\n",
    "\n",
    "! rm -f midi_files\n",
    "if data_source == \"shakespeare\":\n",
    "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "elif data_source == \"maestro\":\n",
    "    !wget -N https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip\n",
    "    !unzip -n -qq maestro-v3.0.0-midi.zip\n",
    "    !ln -s maestro-v3.0.0 midi_files \n",
    "elif data_source == \"jsb\":\n",
    "    !ln -s jsb_chorale_midi midi_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtPj8fGF6I2V",
    "outputId": "81443c6d-b3c5-43a2-f60f-d43d4f418b54"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from glob import glob\n",
    "import pathlib\n",
    "import pretty_midi\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random \n",
    "from midi_player import MIDIPlayer\n",
    "from midi_player.stylers import basic, cifka_advanced, dark\n",
    "import multiprocessing as mp\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdrscotthawley\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "use_wandb = True\n",
    "if use_wandb:\n",
    "    import wandb\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "y06bDCiX6R0C"
   },
   "outputs": [],
   "source": [
    "# Karpathy's # hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "weight_decay = 1e-2 # pytorch default added here\n",
    "# ------------\n",
    "\n",
    "\n",
    "# Mine for Pitches\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embd = 128 # 64\n",
    "n_head = 8 # 4\n",
    "n_layer = 4 * 4\n",
    "dropout = 0.1\n",
    "weight_decay = 1e-2 # pytorch default added here\n",
    "\n",
    "\n",
    "# Mine for Full notes? \n",
    "# hyperparameters\n",
    "batch_size = 96 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "#learning_rate = 1e-3\n",
    "learning_rate = 3e-3\n",
    "eval_iters = 200\n",
    "n_embd = 128 # 64\n",
    "n_head = 8 # 4\n",
    "n_layer = 4 * 4\n",
    "dropout = 0.1\n",
    "weight_decay = 1e-2 # pytorch default added here\n",
    "\n",
    "# for comparison with giant_model code\n",
    "batch_size = 128 # how many independent sequences will we process in parallel?\n",
    "block_size = 128 # what is the maximum context length for predictions?\n",
    "n_embd = 64 # 64  # dimensions of embeddings\n",
    "n_head = 8 # 4\n",
    "n_layer = 4 # # 4\n",
    "dropout = 0.1 #0.1\n",
    "max_iters = 25000\n",
    "eval_interval = 100\n",
    "eval_iters = 100\n",
    "#use_alibi = True # n/a here\n",
    "weight_decay = 1e-2 # pytorch default added here\n",
    "\n",
    "\n",
    "# for jsb testing\n",
    "batch_size = 128 # how many independent sequences will we process in parallel?\n",
    "block_size = 128 # what is the maximum context length for predictions?\n",
    "learning_rate = 1e-3\n",
    "n_embd = 256\n",
    "n_head = 16\n",
    "n_layer = 8\n",
    "dropout = 0.7 # drop that shit\n",
    "weight_decay =  1e-2 # 1e-2 is pytorch default\n",
    "# ------------\n",
    "\n",
    "\n",
    "#\"small model\" but larger batch:\n",
    "batch_size = 32\n",
    "block_size =64  \n",
    "learning_rate = 0.001\n",
    "n_embd = 128\n",
    "n_head = 8\n",
    "n_layer = 4\n",
    "dropout = 0.1\n",
    "\n",
    "max_iters = 11000\n",
    "eval_interval = 100\n",
    "eval_iters = 200\n",
    "\n",
    "\n",
    "config = {\n",
    "    'learning_rate': learning_rate,\n",
    "    'batch_size': batch_size,\n",
    "    'block_size': block_size,\n",
    "    'n_embd': n_embd,\n",
    "    'n_head': n_head,\n",
    "    'n_layer': n_layer,\n",
    "    'dropout': dropout,\n",
    "    'weight_decay': weight_decay,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('device =',device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QlkM3xo6WhL"
   },
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility.\n",
    "def set_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) \n",
    "        \n",
    "set_seeds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 382\n"
     ]
    }
   ],
   "source": [
    "data_dir = pathlib.Path('midi_files')\n",
    "filenames = glob(str(data_dir/'**/*.mid*'), recursive=True)\n",
    "print('Number of files:', len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_file_to_tensor(midi_file,\n",
    "                        time_units='ticks', # beats, ticks, s\n",
    "                        info=False,  # return info about the track\n",
    "                       ):\n",
    "    pm = pretty_midi.PrettyMIDI(midi_file) # read in the whole file. this is very slow for long midi files (e.g. in MAESTRO)\n",
    "    # Sort the notes first by start time (then by pitch if two notes start at the same time)\n",
    "    sorted_notes = sorted(pm.instruments[0].notes, key=lambda note: (note.start, note.pitch))\n",
    "    notes = torch.empty( (len(sorted_notes), 5), dtype=torch.float32 ) # allocate storage\n",
    "    \n",
    "    prev_start = sorted_notes[0].start\n",
    "    for i, note in enumerate(sorted_notes):\n",
    "        notes[i] = note.pitch\n",
    "        notes[i, 1] = note.start - prev_start  # step, time since last note\n",
    "        notes[i, 2] = note.end - note.start    # duration\n",
    "        # let's keep the start & end to make life easier elsewhere. we can always chop these off.\n",
    "        notes[i, 3], notes[i, 4] = note.start, note.end \n",
    "        prev_start = note.start\n",
    "\n",
    "    return notes\n",
    "\n",
    "\n",
    "def files_to_tensor_list(filenames): \n",
    "    \"runs in parallel so should be reasonably fast\"\n",
    "    tensor_list = process_map(midi_file_to_tensor, filenames, max_workers=mp.cpu_count(), chunksize=1)\n",
    "    return tensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([432, 5]), torch.Size([432]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in one song\n",
    "notes = midi_file_to_tensor(filenames[0])\n",
    "pitches = notes[:,0].type(torch.long)  # just the pitch info\n",
    "notes.shape, pitches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37811f05b79a4a07aa22b228e129e279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "len(notes_list) = 382\n"
     ]
    }
   ],
   "source": [
    "# here we read midi files into a list of tensors\n",
    "read_all_midi_files = True # sometime you don't wanna re-do this\n",
    "\n",
    "if read_all_midi_files:\n",
    "    notes_list = files_to_tensor_list(filenames)\n",
    "    print(f\"\\nlen(notes_list) = {len(notes_list)}\")\n",
    "    torch.save(notes_list, f'{data_source}_tensor_list.pt') # save for next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382, torch.Size([432, 5]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load from previous save\n",
    "notes_tensor_list = torch.load(f'{data_source}_tensor_list.pt')  # load from previous computation\n",
    "#notes_list = torch.load('rastro-120bpm_16th_tensor_list.pt')  # load from previous computation\n",
    "len(notes_tensor_list), notes_tensor_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([78799, 3]), torch.Size([78799]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_from_last(ts:torch.Tensor):\n",
    "    \"returns how far the next song should step from the last-struck note of the current song\"\n",
    "    if ts.shape[-1] < 5: return None # too much of a pain to re-integrate start & end times\n",
    "    last_struck_note = ts.shape[0]-1  # the final midi note event\n",
    "    last_held_note = ts[:,4].argmax() # the note with the final end time\n",
    "    # time diff between (start of last-struck note) and (end of last-held note)\n",
    "    return ts[last_held_note,4] - ts[last_struck_note,3]\n",
    "\n",
    "\n",
    "def tl_to_notes(tensor_list, \n",
    "                shuffle=False, \n",
    "                delimit=True, # leave this on\n",
    "                rest_pitch=127, # some datasets already prefer 127 or -1. you should check\n",
    "                rest_dur=0.96,  # value used by jsb chorales iirc. in seconds\n",
    "                ):\n",
    "    \"\"\"Takes list of tensors (of arbitrary length, for each song).\n",
    "    converts to one big long tensor of notes all running togehter\"\"\"\n",
    "    if shuffle: random.shuffle(tensor_list)  # shuffle order of songs\n",
    "    # writing the following as a loop first so i get it right\n",
    "    out_tl = []\n",
    "    for si, ts in enumerate(tensor_list):  # ts = \"tensor song\" lol\n",
    "        if si == 0 :  \n",
    "            out_tl.append(ts)  # no work to do\n",
    "        else:\n",
    "            sfl = step_from_last(tensor_list[-1]) \n",
    "            add_rest = torch.tensor((rest_pitch, sfl, rest_dur, 0, rest_dur))  # jsb chorales used this method, so good enough for me\n",
    "            out_tl.append(add_rest)\n",
    "            ts[0,1] = rest_dur  # step of not of new song should be dur of rest that precedes it.\n",
    "            out_tl.append(ts)\n",
    "    # remember to strip out start & end values\n",
    "    out = torch.vstack(out_tl).type(torch.float32)  # return one big tensor of floats\n",
    "    out = out[:,:3]   #  leave only pitch, step, duration\n",
    "    return out\n",
    "\n",
    "set_seeds(0)\n",
    "all_notes = tl_to_notes(notes_tensor_list, shuffle=True) \n",
    "notes_tensor_list = [q[:,:3] for q in notes_tensor_list] # make sure we chopped off any any extra values\n",
    "all_pitches = all_notes[:,0].type(torch.long)  # just the pitch info\n",
    "all_notes.shape, all_pitches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all_notes for inspection & later use\n",
    "torch.save(all_notes, f'{data_source}_all_notes_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notes_arr_to_df(notes_arr) -> pd.DataFrame:\n",
    "    columns = ['pitch','step','duration']\n",
    "    df = pd.DataFrame(notes_arr, columns=columns)\n",
    "    df[\"start\"], df[\"end\"] = \"\",  \"\"\n",
    "    prev_start = 0\n",
    "    for i, row in df.iterrows():\n",
    "        start = prev_start + float(row['step'])\n",
    "        df.at[i, 'start'] = start\n",
    "        df.at[i, 'end']   = start + float(row['duration'])\n",
    "        prev_start = start\n",
    "    return df\n",
    "\n",
    "def df_to_midi(\n",
    "        notes_df: pd.DataFrame,\n",
    "        out_file: str = '',  # output file to save to, if any\n",
    "        instrument_name: str = 'Acoustic Grand Piano', # whatever you want to call this instrument\n",
    "        velocity: int = 100,  # note loudness\n",
    "    ) -> pretty_midi.PrettyMIDI:\n",
    "    \"converts a dataframe to valid midi\"\n",
    "\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    instrument = pretty_midi.Instrument(\n",
    "        program=pretty_midi.instrument_name_to_program(\n",
    "            instrument_name))\n",
    "\n",
    "    prev_start = 0\n",
    "    for i, note in notes_df.iterrows(): # this is a serial operation, not sure how to parallelize\n",
    "        start = float(prev_start + note['step'])\n",
    "        end = float(start + note['duration'])\n",
    "        note = pretty_midi.Note( velocity=velocity, pitch=int(note['pitch']), start=start,  end=end, )\n",
    "        instrument.notes.append(note)\n",
    "        prev_start = start\n",
    "\n",
    "    pm.instruments.append(instrument)\n",
    "    if out_file: pm.write(out_file)\n",
    "    return pm\n",
    "\n",
    "def notes_to_midi(notes_tensor, time_rescale=None, out_file: str = '') -> pretty_midi.PrettyMIDI:\n",
    "    notes_tensor = notes_tensor.clone() # just to avoid weird overwrites of memory\n",
    "    #notes_tensor = notes_tensor * (notes_tensor>0)  # negative numbers clipped to zero\n",
    "    if notes_tensor.min() < 0.0:\n",
    "      print(\"WARNING: You have negative pitches, steps or durations. Setting them to zero\")\n",
    "      notes_tensor = notes_tensor * (notes_tensor >= 0)\n",
    "    if time_rescale is not None :\n",
    "        notes_tensor[:,1:] = notes_tensor[:,1:] *time_rescale # no quantization, just rescaling time\n",
    "    notes_df = notes_arr_to_df(notes_tensor.cpu().detach().numpy())\n",
    "    return df_to_midi(notes_df, out_file=out_file)\n",
    "\n",
    "def midiplayer(notes_tensor, height=400, time_rescale=None, midi_file=\"/tmp/tmp.mid\"):\n",
    "    pm = notes_to_midi(notes_tensor, time_rescale=time_rescale, out_file=midi_file)\n",
    "    return MIDIPlayer(midi_file, height, styler=dark, dl=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"&lt;script src=&quot;https://cdn.jsdelivr.net/combine/npm/tone@14.7.58,npm/@magenta/music@1.23.1/es6/core.js,npm/focus-visible@5,npm/html-midi-player@1.5.0&quot;&gt;&lt;/script&gt;\n",
       "\n",
       "&lt;style&gt;\n",
       "/* Custom player style */\n",
       "#section306 midi-player {\n",
       "  display: block;\n",
       "  width: inherit;\n",
       "  margin: 4px;\n",
       "  margin-bottom: 0;\n",
       "  color: #d4d4d4; /* Lighter text color for better readability */\n",
       "}\n",
       "#section306 midi-player::part(control-panel) {\n",
       "  background: #222; /* Dark background */\n",
       "  border: 2px solid #888; /* Lightened border color for contrast */\n",
       "  border-radius: 10px 10px 0 0;\n",
       "}\n",
       "#section306 midi-player::part(play-button) {\n",
       "  color: #ffffff; /* White text for visibility */\n",
       "  border: 2px solid currentColor;\n",
       "  background-color: #6c7a89; /* Lighter green for visibility */\n",
       "  border-radius: 20px;\n",
       "  transition: all 0.2s;\n",
       "  content: &#x27;hello&#x27;;\n",
       "}\n",
       "#section306 midi-player::part(play-button):hover {\n",
       "  color: #0a0; /* Green text on hover */\n",
       "  background-color: #7fc97f; /* Brighter green background on hover */\n",
       "  border-radius: 10px;\n",
       "}\n",
       "#section306 midi-player::part(time) {\n",
       "  font-family: monospace; /* Monospace font for time */\n",
       "}\n",
       "\n",
       "/* Custom visualizer style */\n",
       "#section306 midi-visualizer .piano-roll-visualizer {\n",
       "  background: #333; /* Dark background for visualizer */\n",
       "  border: 2px solid #505050; /* Dark border for subtle appearance */\n",
       "  border-top: none;\n",
       "  border-radius: 0 0 10px 10px;\n",
       "  margin: 4px;\n",
       "  width: inherit;\n",
       "  margin-top: 0;\n",
       "  overflow: auto;\n",
       "}\n",
       "#section306 midi-visualizer svg rect.note {\n",
       "  opacity: 0.9; \n",
       "  stroke-width: 1; /* Stroke width for note clarity */\n",
       "}\n",
       "\n",
       "/* Different instrument colors */\n",
       "#section306 midi-visualizer svg rect.note[data-instrument=&quot;0&quot;]{\n",
       "  fill: #7aa6ed; /*  blue for Instrument 0 */\n",
       "  stroke: #444; \n",
       "}\n",
       "#section306 midi-visualizer svg rect.note[data-instrument=&quot;2&quot;]{\n",
       "  fill: #d586d0; /* purple for Instrument 2 for consistency */\n",
       "  stroke: #444; /* White stroke for visibility */\n",
       "}\n",
       "#section306 midi-visualizer svg rect.note[data-is-drum=&quot;true&quot;]{\n",
       "  fill: brightorange; \n",
       "  stroke: #bbb;\n",
       "}\n",
       "#section306 midi-visualizer svg rect.note.active {\n",
       "  opacity: 0.9; /* Highlight active notes */\n",
       "  stroke: #ddd; /* White stroke for maximum contrast */\n",
       "  stroke-width: 2; /* Thicker stroke for active notes */\n",
       "}\n",
       "&lt;/style&gt;\n",
       "\n",
       "          &lt;section id=&quot;section306&quot;&gt;\n",
       "          &lt;a href=&quot;data:audio/midi;base64,TVRoZAAAAAYAAQACANxNVHJrAAAAEwD/UQMHoSAA/1gEBAIYCAH/LwBNVHJrAAADqADAAACQM2QAQmQARWQAR2SBaDMAADRkAEBkAEIAAENkAEUAgWg0AAA1ZAA+ZABAAABDAABEZIFpNQAANmQAPWQAPgAAQmQARAAARmQARwCBaDpkAEYAgWk2AAA4ZAA6AAA7ZAA9AABAZABCAABHZIFoOAAAOmQAOwAAQmQARwAASWSBaDoAADtkAD5kAEAAAEkAAEpkdD4AAEBkdUAAAEdkgWg6ZAA7AABCAABDZABHAABJZABKAABMZIFoOgAAO2QAQmQAQwAAR2QASQAASmQATACBaDZkADsAAEZkAEcAAElkAEoAgWg6ZABGAIFqNgAAOgAAO2QASQAASmSBaDlkADsAAD1kAEIAAEVkAEoAAExkgWgyZAA5AAA9AAA+ZABMAABOZHQ9ZAA+AABMZABOAHQyAAA3ZAA7ZAA9AABFAABHZABKZABMAHQ2ZAA3AABFZABHAHQ0ZAA2AABDZABFAABKAABPZHU0AAA2ZABDAABFZHQ2AAA3ZAA7AABAZABFAABHZHQ3AAA5ZHQ5AAA7ZAA+ZABAAABOZABPAHQ9ZAA+AABMZABOAHQ5ZAA7AAA9AAA+ZABMAABOZHU4ZAA5AHM4AAA5ZABAZABHAABMZABOAHVAAABCZHQ9ZAA+AABCAABDZIFpMmQAOQAAPQAAPmQAQmQAQwAASmQATACBaDZkAD4AAEIAAEdkgWgyAAA0ZAA2AAA4ZABAZABHAABJZABKAHU0AAA2ZAA4AAA6ZHM2AAA3ZAA6AAA7ZAA+ZABAAABHZABJAIFoMWQANGQANwAAOwAAPgAAQGQARWQARwB1Q2QARQB0MQAAMmQANAAAOWQAPmQAQAAAQmQAQwB0PgAAQGQAQgAAQ2R0MgAAOQAAPmQAQAAAQmQAQwAARWR0PGR1O2QAPAB0OWQAOwB0N2QAOQAAQgAAQ2QARQAAR2R0NmQANwAAPgAAQGR0NGQANgAAQAAAQmQAQwAARWQARwB0M2QANAB1MwAANGQAO2QAQ2QARQBzP2QAQgB1PwAAQGSBaC9kADQAAD9kAEAAAEJkAEMAgWg7AAA+ZAA/AABCAABHZABOZIFoLwAAMWQAPWQAPgAATGQATgB1RmQARwB1MQAAMmQANmQAPQAARgAAR2QASmQATABzMgAANGR1NAAAQmQASWQASgBzRGQARwB1RAAARmQASQB0QGQAQgB0L2QANgAAP2QAQAAAQmQARgAAR2SHIS8AAD8AAEIAAEcAAf8vAA==&quot; target=&quot;_blank&quot;&gt;Download MIDI&lt;/a&gt;&lt;br&gt;\n",
       "          &lt;midi-player src=data:audio/midi;base64,TVRoZAAAAAYAAQACANxNVHJrAAAAEwD/UQMHoSAA/1gEBAIYCAH/LwBNVHJrAAADqADAAACQM2QAQmQARWQAR2SBaDMAADRkAEBkAEIAAENkAEUAgWg0AAA1ZAA+ZABAAABDAABEZIFpNQAANmQAPWQAPgAAQmQARAAARmQARwCBaDpkAEYAgWk2AAA4ZAA6AAA7ZAA9AABAZABCAABHZIFoOAAAOmQAOwAAQmQARwAASWSBaDoAADtkAD5kAEAAAEkAAEpkdD4AAEBkdUAAAEdkgWg6ZAA7AABCAABDZABHAABJZABKAABMZIFoOgAAO2QAQmQAQwAAR2QASQAASmQATACBaDZkADsAAEZkAEcAAElkAEoAgWg6ZABGAIFqNgAAOgAAO2QASQAASmSBaDlkADsAAD1kAEIAAEVkAEoAAExkgWgyZAA5AAA9AAA+ZABMAABOZHQ9ZAA+AABMZABOAHQyAAA3ZAA7ZAA9AABFAABHZABKZABMAHQ2ZAA3AABFZABHAHQ0ZAA2AABDZABFAABKAABPZHU0AAA2ZABDAABFZHQ2AAA3ZAA7AABAZABFAABHZHQ3AAA5ZHQ5AAA7ZAA+ZABAAABOZABPAHQ9ZAA+AABMZABOAHQ5ZAA7AAA9AAA+ZABMAABOZHU4ZAA5AHM4AAA5ZABAZABHAABMZABOAHVAAABCZHQ9ZAA+AABCAABDZIFpMmQAOQAAPQAAPmQAQmQAQwAASmQATACBaDZkAD4AAEIAAEdkgWgyAAA0ZAA2AAA4ZABAZABHAABJZABKAHU0AAA2ZAA4AAA6ZHM2AAA3ZAA6AAA7ZAA+ZABAAABHZABJAIFoMWQANGQANwAAOwAAPgAAQGQARWQARwB1Q2QARQB0MQAAMmQANAAAOWQAPmQAQAAAQmQAQwB0PgAAQGQAQgAAQ2R0MgAAOQAAPmQAQAAAQmQAQwAARWR0PGR1O2QAPAB0OWQAOwB0N2QAOQAAQgAAQ2QARQAAR2R0NmQANwAAPgAAQGR0NGQANgAAQAAAQmQAQwAARWQARwB0M2QANAB1MwAANGQAO2QAQ2QARQBzP2QAQgB1PwAAQGSBaC9kADQAAD9kAEAAAEJkAEMAgWg7AAA+ZAA/AABCAABHZABOZIFoLwAAMWQAPWQAPgAATGQATgB1RmQARwB1MQAAMmQANmQAPQAARgAAR2QASmQATABzMgAANGR1NAAAQmQASWQASgBzRGQARwB1RAAARmQASQB0QGQAQgB0L2QANgAAP2QAQAAAQmQARgAAR2SHIS8AAD8AAEIAAEcAAf8vAA== sound-font visualizer=&quot;#section306 midi-visualizer&quot;&gt;&lt;/midi-player&gt;\n",
       "          &lt;midi-visualizer src=data:audio/midi;base64,TVRoZAAAAAYAAQACANxNVHJrAAAAEwD/UQMHoSAA/1gEBAIYCAH/LwBNVHJrAAADqADAAACQM2QAQmQARWQAR2SBaDMAADRkAEBkAEIAAENkAEUAgWg0AAA1ZAA+ZABAAABDAABEZIFpNQAANmQAPWQAPgAAQmQARAAARmQARwCBaDpkAEYAgWk2AAA4ZAA6AAA7ZAA9AABAZABCAABHZIFoOAAAOmQAOwAAQmQARwAASWSBaDoAADtkAD5kAEAAAEkAAEpkdD4AAEBkdUAAAEdkgWg6ZAA7AABCAABDZABHAABJZABKAABMZIFoOgAAO2QAQmQAQwAAR2QASQAASmQATACBaDZkADsAAEZkAEcAAElkAEoAgWg6ZABGAIFqNgAAOgAAO2QASQAASmSBaDlkADsAAD1kAEIAAEVkAEoAAExkgWgyZAA5AAA9AAA+ZABMAABOZHQ9ZAA+AABMZABOAHQyAAA3ZAA7ZAA9AABFAABHZABKZABMAHQ2ZAA3AABFZABHAHQ0ZAA2AABDZABFAABKAABPZHU0AAA2ZABDAABFZHQ2AAA3ZAA7AABAZABFAABHZHQ3AAA5ZHQ5AAA7ZAA+ZABAAABOZABPAHQ9ZAA+AABMZABOAHQ5ZAA7AAA9AAA+ZABMAABOZHU4ZAA5AHM4AAA5ZABAZABHAABMZABOAHVAAABCZHQ9ZAA+AABCAABDZIFpMmQAOQAAPQAAPmQAQmQAQwAASmQATACBaDZkAD4AAEIAAEdkgWgyAAA0ZAA2AAA4ZABAZABHAABJZABKAHU0AAA2ZAA4AAA6ZHM2AAA3ZAA6AAA7ZAA+ZABAAABHZABJAIFoMWQANGQANwAAOwAAPgAAQGQARWQARwB1Q2QARQB0MQAAMmQANAAAOWQAPmQAQAAAQmQAQwB0PgAAQGQAQgAAQ2R0MgAAOQAAPmQAQAAAQmQAQwAARWR0PGR1O2QAPAB0OWQAOwB0N2QAOQAAQgAAQ2QARQAAR2R0NmQANwAAPgAAQGR0NGQANgAAQAAAQmQAQwAARWQARwB0M2QANAB1MwAANGQAO2QAQ2QARQBzP2QAQgB1PwAAQGSBaC9kADQAAD9kAEAAAEJkAEMAgWg7AAA+ZAA/AABCAABHZABOZIFoLwAAMWQAPWQAPgAATGQATgB1RmQARwB1MQAAMmQANmQAPQAARgAAR2QASmQATABzMgAANGR1NAAAQmQASWQASgBzRGQARwB1RAAARmQASQB0QGQAQgB0L2QANgAAP2QAQAAAQmQARgAAR2SHIS8AAD8AAEIAAEcAAf8vAA== type=piano-roll&gt;&lt;/midi-visualizer&gt;\n",
       "          &lt;/section&gt;\n",
       "          \" width=\"100%\" height=\"400\"\n",
       "            style=\"border:none !important;\"\n",
       "            \"allowfullscreen\" \"webkitallowfullscreen\" \"mozallowfullscreen\">'\n",
       "            </iframe>"
      ],
      "text/plain": [
       "<midi_player.midi_player.MIDIPlayer at 0x7f4a6b4f6b30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = midiplayer(notes_tensor_list[0], time_rescale=1.1)\n",
    "p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codebook creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_data.shape = torch.Size([78799, 3])\n",
      "n_codebooks = 3\n",
      "\n",
      "---\n",
      "cb 0: cb_vals = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])\n",
      " cb 0: cb keys = dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127])\n",
      "\n",
      "---\n",
      "cb 1: cb_vals = tensor([0.0000, 0.1182, 0.1205, 0.2386, 0.2409, 0.3591, 0.3614, 0.4795, 0.4818,\n",
      "        0.7182, 0.7205, 0.9591, 0.9600, 0.9614, 0.9614, 1.2000, 1.4386, 1.4409,\n",
      "        1.6795, 1.6818, 1.9182, 1.9205, 2.4000, 2.8795, 2.8818, 3.8386, 3.8409,\n",
      "        4.0000])\n",
      " cb 1: cb keys = dict_keys([0.0, 0.11818181723356247, 0.12045454233884811, 0.23863635957241058, 0.24090908467769623, 0.3590908944606781, 0.36136364936828613, 0.4795454442501068, 0.48181816935539246, 0.7181817889213562, 0.7204545736312866, 0.9590908885002136, 0.9599999785423279, 0.9613636136054993, 0.96136474609375, 1.2000000476837158, 1.4386364221572876, 1.4409091472625732, 1.6795454025268555, 1.6818181276321411, 1.9181817770004272, 1.920454502105713, 2.4000000953674316, 2.8795454502105713, 2.8818182945251465, 3.8386363983154297, 3.840909004211426, 4.0])\n",
      "\n",
      "---\n",
      "cb 2: cb_vals = tensor([0.1182, 0.1205, 0.2386, 0.2409, 0.3591, 0.3614, 0.4795, 0.4818, 0.6000,\n",
      "        0.7182, 0.7205, 0.8386, 0.8409, 0.9591, 0.9600, 0.9614, 1.0795, 1.0818,\n",
      "        1.2000, 1.3205, 1.4386, 1.4409, 1.5591, 1.6795, 1.6818, 1.8000, 1.9182,\n",
      "        1.9205, 2.0409, 2.1591, 2.1614, 2.4000, 2.5205, 2.6386, 2.6409, 2.8795,\n",
      "        2.8818, 3.1182, 3.1205, 3.2386, 3.3591, 3.3614, 3.6000, 3.8386, 3.8409,\n",
      "        4.0000])\n",
      " cb 2: cb keys = dict_keys([0.11818181723356247, 0.12045454233884811, 0.23863635957241058, 0.24090908467769623, 0.3590908944606781, 0.36136364936828613, 0.4795454442501068, 0.48181816935539246, 0.6000000238418579, 0.7181817889213562, 0.7204545736312866, 0.8386363387107849, 0.8409090638160706, 0.9590908885002136, 0.9599999785423279, 0.9613636136054993, 1.079545497894287, 1.0818182229995728, 1.2000000476837158, 1.3204545974731445, 1.4386364221572876, 1.4409091472625732, 1.5590908527374268, 1.6795454025268555, 1.6818181276321411, 1.7999999523162842, 1.9181817770004272, 1.920454502105713, 2.0409090518951416, 2.159090995788574, 2.1613636016845703, 2.4000000953674316, 2.5204546451568604, 2.638636350631714, 2.640909194946289, 2.8795454502105713, 2.8818182945251465, 3.1181817054748535, 3.1204545497894287, 3.2386362552642822, 3.359090805053711, 3.361363649368286, 3.5999999046325684, 3.8386363983154297, 3.840909004211426, 4.0])\n",
      "vocab_sizes =  [128, 28, 46]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Housekeeping: clamp any step or dur values that are really long at, say 4 seconds\n",
    "all_notes[:,1] = torch.clamp(all_notes[:,1], 0, 4.0) # step\n",
    "all_notes[:,2] = torch.clamp(all_notes[:,2], 0, 4.0) # dur\n",
    "\n",
    "raw_data = all_notes\n",
    "print(\"raw_data.shape =\",raw_data.shape)\n",
    "n_codebooks = raw_data.shape[-1]\n",
    "print(\"n_codebooks =\",n_codebooks)\n",
    "\n",
    "\n",
    "codebooks = []\n",
    "for i in range(n_codebooks): \n",
    "    if i==0:\n",
    "        cb_vals = torch.arange(128)  # use all possible pitches \n",
    "    else:\n",
    "        cb_vals = raw_data[:,i].unique().sort()[0]  # sorted(set(raw_data[:,i]))\n",
    "    print(f\"\\n---\\ncb {i}: cb_vals = {cb_vals}\")\n",
    "    codebooks.append({'encode':{k.item(): int(v) for v, k in enumerate(cb_vals)}, \n",
    "                      'decode':{int(v): k for v, k in enumerate(cb_vals)}})\n",
    "    print(f\" cb {i}: cb keys = {codebooks[-1]['encode'].keys()}\")\n",
    "vocab_sizes = [len(cb['encode']) for cb in codebooks]\n",
    "print(\"vocab_sizes = \",vocab_sizes)\n",
    "vocab_size = vocab_sizes[0]\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before encoding, test_str =\n",
      " tensor([[51.0000,  0.0000,  0.4795],\n",
      "        [66.0000,  0.0000,  0.4795],\n",
      "        [69.0000,  0.0000,  0.4795],\n",
      "        [71.0000,  0.0000,  1.4409],\n",
      "        [52.0000,  0.4795,  0.4795],\n",
      "        [64.0000,  0.0000,  0.4795]])\n",
      "After encoding, ilist =\n",
      " tensor([[51,  0,  6],\n",
      "        [66,  0,  6],\n",
      "        [69,  0,  6],\n",
      "        [71,  0, 21],\n",
      "        [52,  7,  6],\n",
      "        [64,  0,  6]])\n",
      "After decoding, ret_str =\n",
      " tensor([[51.0000,  0.0000,  0.4795],\n",
      "        [66.0000,  0.0000,  0.4795],\n",
      "        [69.0000,  0.0000,  0.4795],\n",
      "        [71.0000,  0.0000,  1.4409],\n",
      "        [52.0000,  0.4795,  0.4795],\n",
      "        [64.0000,  0.0000,  0.4795]])\n",
      "Checks pass! :-)\n"
     ]
    }
   ],
   "source": [
    "#these are slow and could be improved a lot by doing all cbs at once\n",
    "#encode = lambda s: torch.tensor( [[codebooks[cb]['encode'][note[cb].item()] for cb in range(n_codebooks)] for note in s], dtype=torch.long).to(s.device)\n",
    "        \n",
    "#decode = lambda si: torch.tensor([[codebooks[cb]['decode'][i[cb].item()]  for cb in range(n_codebooks) ] for i in si]).to(si.device)\n",
    "\n",
    "def remap_vals(s, direction, dtype=torch.long):\n",
    "    out = torch.zeros_like(s, dtype=dtype)\n",
    "    for cb in range(s.shape[-1]): \n",
    "        dict_map = codebooks[cb][direction]\n",
    "        out[:,cb] = torch.tensor([dict_map[x.item()] for x in s[:,cb]], dtype=dtype)\n",
    "    return out \n",
    "\n",
    "encode = lambda s: remap_vals(s, 'encode')\n",
    "decode = lambda s: remap_vals(s, 'decode', dtype=all_notes.dtype)\n",
    "\n",
    "test_str = all_notes[0:6]\n",
    "print(\"Before encoding, test_str =\\n\",test_str)\n",
    "ilist = encode(test_str)\n",
    "print(\"After encoding, ilist =\\n\",ilist)\n",
    "ret_str = decode(ilist)\n",
    "print(\"After decoding, ret_str =\\n\",ret_str)\n",
    "assert torch.equal(test_str, ret_str), f\"Oops. test_str={test_str}, but ret_str={ret_str}\"\n",
    "print(\"Checks pass! :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape, val_data.shape = torch.Size([70919, 3]) torch.Size([7880, 3])\n"
     ]
    }
   ],
   "source": [
    "data = encode(all_notes) # this takes a while\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(\"train_data.shape, val_data.shape =\",train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaAMdat36Vj4",
    "outputId": "3e0c13ca-4a17-48e3-dcd0-58eda669cd36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db.shape =  torch.Size([1, 5, 3])\n",
      "original db = \n",
      " tensor([[[ 54,  12,   6],\n",
      "         [ 61,   0, 124],\n",
      "         [127,  31,   4],\n",
      "         [ 86,   0,  12],\n",
      "         [126,   7,  12]]])\n",
      "augmented db = \n",
      " tensor([[[ 62,  12,   6],\n",
      "         [ 69,   0, 124],\n",
      "         [127,  31,   4],\n",
      "         [ 94,   0,  12],\n",
      "         [127,   7,  12]]])\n",
      "Data aug looks good!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def augment_data_db(db, pitch_shift=12, debug=True):\n",
    "    dbnew = db.clone()\n",
    "    change = torch.randint(-pitch_shift, pitch_shift, (1,))\n",
    "    dbnew[ dbnew[:,:,0] < 127 ] += torch.tensor((change, 0, 0))\n",
    "    dbnew[:,:,0] = torch.clamp(dbnew[:,:,0], 0, 127)\n",
    "    return dbnew\n",
    "\n",
    "\n",
    "# test data augmentation\n",
    "db = torch.tensor([[54,12,6],[61,0,124],[127,31,4],[86,0,12],[126,7,12]]).unsqueeze(0)\n",
    "print(\"db.shape = \",db.shape)\n",
    "print(\"original db = \\n\",db)\n",
    "ret = augment_data_db(db) # make sure the note with the \"127\" rest is left completely alone\n",
    "print(\"augmented db = \\n\",ret)\n",
    "\n",
    "assert torch.equal(db[0,2,:], ret[0,2,:]),f\"Those lines don't match but they should: {db[0,2,:]} and {ret[0,2,:]}\"\n",
    "for i in [0,1,3,4]:\n",
    "    assert not torch.equal(db[0,i,:], ret[0,i,:]), f\"row {i}'s do match but probably shouldn't: {db[0,i,:]} and {ret[0,i,:]}\"\n",
    "\n",
    "print(\"Data aug looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaAMdat36Vj4",
    "outputId": "3e0c13ca-4a17-48e3-dcd0-58eda669cd36"
   },
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split, debug=False):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # batch_size number of indices to grab from\n",
    "    data_block = torch.stack([data[i:i+block_size+1] for i in ix])\n",
    "    if split=='train': \n",
    "        data_block = augment_data_db(data_block)\n",
    "    x, y = data_block[:,:-1,:], data_block[:,1:,:]\n",
    "    #x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    #y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    #x, y = augment_data(x,y)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    if debug: print(f\"get_batch: x.shape = {x.shape}, y.shape = {y.shape}\")\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9PfGh_57IY0",
    "outputId": "814b53f0-d196-455c-9ff5-cef74e1b8a8a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_batch: x.shape = torch.Size([32, 64, 3]), y.shape = torch.Size([32, 64, 3])\n",
      "B, T = 32, 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[64,  0, 18],\n",
       "        [68,  0,  6],\n",
       "        [49,  4,  2],\n",
       "        [47,  3,  3],\n",
       "        [61,  0,  6],\n",
       "        [69,  0,  6],\n",
       "        [45,  4,  2],\n",
       "        [44,  3,  3],\n",
       "        [59,  0,  3],\n",
       "        [71,  0,  7]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = get_batch('train',debug=True) # first time is slow\n",
    "print(f\"B, T = {batch_size}, {block_size}\")\n",
    "x[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WP9U8aIm7qh3"
   },
   "source": [
    "y is x ahead back by one and including new data.\n",
    "in this sense only y[:,-1] is the \"next token\" being predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Wjb4QMS7lub",
    "outputId": "edb23f78-ccac-4d36-f6dc-2ed41e52444e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[68,  0,  6],\n",
       "        [49,  4,  2],\n",
       "        [47,  3,  3],\n",
       "        [61,  0,  6],\n",
       "        [69,  0,  6],\n",
       "        [45,  4,  2],\n",
       "        [44,  3,  3],\n",
       "        [59,  0,  3],\n",
       "        [71,  0,  7],\n",
       "        [42,  4,  3]], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"&lt;script src=&quot;https://cdn.jsdelivr.net/combine/npm/tone@14.7.58,npm/@magenta/music@1.23.1/es6/core.js,npm/focus-visible@5,npm/html-midi-player@1.5.0&quot;&gt;&lt;/script&gt;\n",
       "\n",
       "&lt;style&gt;\n",
       "/* Custom player style */\n",
       "#section864 midi-player {\n",
       "  display: block;\n",
       "  width: inherit;\n",
       "  margin: 4px;\n",
       "  margin-bottom: 0;\n",
       "  color: #d4d4d4; /* Lighter text color for better readability */\n",
       "}\n",
       "#section864 midi-player::part(control-panel) {\n",
       "  background: #222; /* Dark background */\n",
       "  border: 2px solid #888; /* Lightened border color for contrast */\n",
       "  border-radius: 10px 10px 0 0;\n",
       "}\n",
       "#section864 midi-player::part(play-button) {\n",
       "  color: #ffffff; /* White text for visibility */\n",
       "  border: 2px solid currentColor;\n",
       "  background-color: #6c7a89; /* Lighter green for visibility */\n",
       "  border-radius: 20px;\n",
       "  transition: all 0.2s;\n",
       "  content: &#x27;hello&#x27;;\n",
       "}\n",
       "#section864 midi-player::part(play-button):hover {\n",
       "  color: #0a0; /* Green text on hover */\n",
       "  background-color: #7fc97f; /* Brighter green background on hover */\n",
       "  border-radius: 10px;\n",
       "}\n",
       "#section864 midi-player::part(time) {\n",
       "  font-family: monospace; /* Monospace font for time */\n",
       "}\n",
       "\n",
       "/* Custom visualizer style */\n",
       "#section864 midi-visualizer .piano-roll-visualizer {\n",
       "  background: #333; /* Dark background for visualizer */\n",
       "  border: 2px solid #505050; /* Dark border for subtle appearance */\n",
       "  border-top: none;\n",
       "  border-radius: 0 0 10px 10px;\n",
       "  margin: 4px;\n",
       "  width: inherit;\n",
       "  margin-top: 0;\n",
       "  overflow: auto;\n",
       "}\n",
       "#section864 midi-visualizer svg rect.note {\n",
       "  opacity: 0.9; \n",
       "  stroke-width: 1; /* Stroke width for note clarity */\n",
       "}\n",
       "\n",
       "/* Different instrument colors */\n",
       "#section864 midi-visualizer svg rect.note[data-instrument=&quot;0&quot;]{\n",
       "  fill: #7aa6ed; /*  blue for Instrument 0 */\n",
       "  stroke: #444; \n",
       "}\n",
       "#section864 midi-visualizer svg rect.note[data-instrument=&quot;2&quot;]{\n",
       "  fill: #d586d0; /* purple for Instrument 2 for consistency */\n",
       "  stroke: #444; /* White stroke for visibility */\n",
       "}\n",
       "#section864 midi-visualizer svg rect.note[data-is-drum=&quot;true&quot;]{\n",
       "  fill: brightorange; \n",
       "  stroke: #bbb;\n",
       "}\n",
       "#section864 midi-visualizer svg rect.note.active {\n",
       "  opacity: 0.9; /* Highlight active notes */\n",
       "  stroke: #ddd; /* White stroke for maximum contrast */\n",
       "  stroke-width: 2; /* Thicker stroke for active notes */\n",
       "}\n",
       "&lt;/style&gt;\n",
       "\n",
       "          &lt;section id=&quot;section864&quot;&gt;\n",
       "          &lt;a href=&quot;data:audio/midi;base64,TVRoZAAAAAYAAQACANxNVHJrAAAAEwD/UQMHoSAA/1gEBAIYCAH/LwBNVHJrAAABjADAAACQQGQARGRqMWRpL2QAMQAAPWQARAAARWRqLWQALwBpLGQALQAAO2QAPQAARQAAR2RqKmQALAAAOWQAOwAAP2QAQABqKgAALGQAOQAAO2QAPwAAQGQARwBpLAAALWQAOwAAPWQAQmRqLQAAL2QAO2QAPQAAQgAARGSBUz9kAEAAAEJkAEQAajlkADsAaShkAC8AADhkADkAADtkAD8AAEBkAEIAgycoAAA4AAA7AABAAAB/ZIMmOmQAPWQAQWQARmQAfwBqOGQAOgBpNmQAOAAAPGQAPQAAP2QAQQBqOmQAPABpNWQANgAAOgAAPGQAPwAAQWQARGQARgBqM2QANQBqMWQAMwAAOmQAPAAARAAASWRpLmQAMQAAQQAAQmQ1QgAARGQ1LgAAM2QAQmQARAAASGQASQBpMWQAMwAAQWQAQgAARmQASABqMQAANWQAP2QAQQAARgAASGRqOWQAOgBpLmQAOQAAPWQAPwAARmQASACDJzUAAEFkaT0AAEYAai4AaUEAAf8vAA==&quot; target=&quot;_blank&quot;&gt;Download MIDI&lt;/a&gt;&lt;br&gt;\n",
       "          &lt;midi-player src=data:audio/midi;base64,TVRoZAAAAAYAAQACANxNVHJrAAAAEwD/UQMHoSAA/1gEBAIYCAH/LwBNVHJrAAABjADAAACQQGQARGRqMWRpL2QAMQAAPWQARAAARWRqLWQALwBpLGQALQAAO2QAPQAARQAAR2RqKmQALAAAOWQAOwAAP2QAQABqKgAALGQAOQAAO2QAPwAAQGQARwBpLAAALWQAOwAAPWQAQmRqLQAAL2QAO2QAPQAAQgAARGSBUz9kAEAAAEJkAEQAajlkADsAaShkAC8AADhkADkAADtkAD8AAEBkAEIAgycoAAA4AAA7AABAAAB/ZIMmOmQAPWQAQWQARmQAfwBqOGQAOgBpNmQAOAAAPGQAPQAAP2QAQQBqOmQAPABpNWQANgAAOgAAPGQAPwAAQWQARGQARgBqM2QANQBqMWQAMwAAOmQAPAAARAAASWRpLmQAMQAAQQAAQmQ1QgAARGQ1LgAAM2QAQmQARAAASGQASQBpMWQAMwAAQWQAQgAARmQASABqMQAANWQAP2QAQQAARgAASGRqOWQAOgBpLmQAOQAAPWQAPwAARmQASACDJzUAAEFkaT0AAEYAai4AaUEAAf8vAA== sound-font visualizer=&quot;#section864 midi-visualizer&quot;&gt;&lt;/midi-player&gt;\n",
       "          &lt;midi-visualizer src=data:audio/midi;base64,TVRoZAAAAAYAAQACANxNVHJrAAAAEwD/UQMHoSAA/1gEBAIYCAH/LwBNVHJrAAABjADAAACQQGQARGRqMWRpL2QAMQAAPWQARAAARWRqLWQALwBpLGQALQAAO2QAPQAARQAAR2RqKmQALAAAOWQAOwAAP2QAQABqKgAALGQAOQAAO2QAPwAAQGQARwBpLAAALWQAOwAAPWQAQmRqLQAAL2QAO2QAPQAAQgAARGSBUz9kAEAAAEJkAEQAajlkADsAaShkAC8AADhkADkAADtkAD8AAEBkAEIAgycoAAA4AAA7AABAAAB/ZIMmOmQAPWQAQWQARmQAfwBqOGQAOgBpNmQAOAAAPGQAPQAAP2QAQQBqOmQAPABpNWQANgAAOgAAPGQAPwAAQWQARGQARgBqM2QANQBqMWQAMwAAOmQAPAAARAAASWRpLmQAMQAAQQAAQmQ1QgAARGQ1LgAAM2QAQmQARAAASGQASQBpMWQAMwAAQWQAQgAARmQASABqMQAANWQAP2QAQQAARgAASGRqOWQAOgBpLmQAOQAAPWQAPwAARmQASACDJzUAAEFkaT0AAEYAai4AaUEAAf8vAA== type=piano-roll&gt;&lt;/midi-visualizer&gt;\n",
       "          &lt;/section&gt;\n",
       "          \" width=\"100%\" height=\"400\"\n",
       "            style=\"border:none !important;\"\n",
       "            \"allowfullscreen\" \"webkitallowfullscreen\" \"mozallowfullscreen\">'\n",
       "            </iframe>"
      ],
      "text/plain": [
       "<midi_player.midi_player.MIDIPlayer at 0x7f4a6b493ee0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midiplayer(decode(x[0]), time_rescale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CO7ZJhu-6dvk"
   },
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "zOeDesr4348z"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, debug=False):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        print(\"vocab sizes = \",[vocab_sizes[cb] for cb in range(n_codebooks)])\n",
    "        self.token_embedding_tables = nn.ModuleList([nn.Embedding(vocab_sizes[cb], n_embd) for cb in range(n_codebooks)])\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_heads = nn.ModuleList([nn.Linear(n_embd, vocab_sizes[cb]) for cb in range(n_codebooks)])\n",
    "        self.debug=False\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T, CBS = idx.shape\n",
    "        tok_emb = 0\n",
    "        for cb in range(CBS): # just sum codebook reps\n",
    "            if self.debug: \n",
    "                print(f\"cb = {cb}, idx.shape = {idx.shape}, idx[:,:,cb].shape = {idx[:,:,cb].shape}, idx[:,:,cb] =\\n\",idx[:,:,cb])\n",
    "                print(f\"   idx[:,:,{cb}].min(), idx[:,:,{cb}].max() = \",idx[:,:,cb].min(), idx[:,:,cb].max())\n",
    "            tok_emb = tok_emb + self.token_embedding_tables[cb](idx[:,:,cb])\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,E)\n",
    "\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits_list = [head(x) for head in self.lm_heads]  # list of (B,T,vocab_sizes) tensors, CBS long\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            lambdas = [0.5]*CBS\n",
    "            #lambdas =  [1.0, 0.25, 0.25]+[0.5]*(CBS-3)  # relative \"weights\" to pitch, step, dur losses. anything added later gets 0.5\n",
    "            loss = 0.0\n",
    "            for cb in range(CBS):  # loop over codebooks\n",
    "                logits = logits_list[cb]  \n",
    "                B, T, V = logits.shape   # V = vocab size\n",
    "                targ = targets[:,:,cb]   # B, T \n",
    "                logits = logits.view(B*T, V)\n",
    "                targ = targ.reshape(B*T)\n",
    "                loss = loss + lambdas[cb]*F.cross_entropy(logits, targ)\n",
    "        if self.debug: print(\"loss = \",loss.item()) \n",
    "        return logits_list, loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        # idx is (B, T, CBS) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits_list, loss = self(idx_cond)\n",
    "            idx_next_list = []\n",
    "            for cb in range(idx_cond.shape[-1]):\n",
    "                # focus only on the last time step\n",
    "                logits = logits_list[cb]  # B, T, V  where V = vocab/embedding size \n",
    "                logits = logits[:, -1, :] # get last time.  becomes (B, V)\n",
    "                # apply softmax to get probabilities\n",
    "                probs = F.softmax(logits/temperature, dim=-1) # (B, V)\n",
    "                # sample from the distribution\n",
    "                idx_next_list.append(torch.multinomial(probs, num_samples=1)) # (B, 1)\n",
    "                \n",
    "            idx_next = torch.tensor(idx_next_list).unsqueeze(0).unsqueeze(0).to(idx.device)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSUO8GJx6tpw"
   },
   "source": [
    "Instantiate and get ready to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3yYAD0t6tOK",
    "outputId": "8b8a7edb-80c2-4efc-9f48-86ee16f0809e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab sizes =  [128, 28, 46]\n",
      "0.851914 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(step, model, optimizer,loss, name):\n",
    "    name = name + f'_{step}.pt' if step is not None else name+\".pt\"\n",
    "    print(\"Saving checkpoint to\",name)\n",
    "    torch.save({\n",
    "            'step': step,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, name)\n",
    "\n",
    "def load_checkpoint(name, model, optimizer):\n",
    "    checkpoint = torch.load(name)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    m = model.to(device)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    step = checkpoint['step']\n",
    "    loss = checkpoint['loss']\n",
    "    return step, m, optimizer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_from_checkpoint = False\n",
    "\n",
    "if start_from_checkpoint:\n",
    "    step, model, optimizer, loss = load_checkpoint('fullnotes_jsb_best_cp_do0.7.pt', model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mO4XTmlj58La"
   },
   "source": [
    "# Do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f229fd551c4446815485a4451e6e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112386788914188, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shawley/gpt-midi/wandb/run-20231128_231358-k9giq4tk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/drscotthawley/testvis-jsb/runs/k9giq4tk' target=\"_blank\">azure-elevator-50</a></strong> to <a href='https://wandb.ai/drscotthawley/testvis-jsb' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/drscotthawley/testvis-jsb' target=\"_blank\">https://wandb.ai/drscotthawley/testvis-jsb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/drscotthawley/testvis-jsb/runs/k9giq4tk' target=\"_blank\">https://wandb.ai/drscotthawley/testvis-jsb/runs/k9giq4tk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if use_wandb: wandb.init(project='testvis-jsb', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YCWAxp3I57eO",
    "outputId": "3e053b8a-798c-4198-fcb5-4268afa009f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop...\n",
      "step 100: train loss 3.3224, val loss 3.2836\n",
      "step 200: train loss 2.9632, val loss 2.9451\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 300: train loss 2.8119, val loss 2.7761\n",
      "step 400: train loss 2.7092, val loss 2.6804\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 500: train loss 2.5537, val loss 2.5406\n",
      "step 600: train loss 2.4535, val loss 2.4346\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 700: train loss 2.3512, val loss 2.3711\n",
      "step 800: train loss 2.2738, val loss 2.2752\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 900: train loss 2.2089, val loss 2.2131\n",
      "step 1000: train loss 2.1368, val loss 2.1392\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 1100: train loss 2.0822, val loss 2.0886\n",
      "step 1200: train loss 2.0212, val loss 2.0299\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 1300: train loss 2.0095, val loss 2.0204\n",
      "step 1400: train loss 1.9474, val loss 1.9632\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 1500: train loss 1.9209, val loss 1.9218\n",
      "step 1600: train loss 1.8748, val loss 1.9081\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 1700: train loss 1.8344, val loss 1.8739\n",
      "step 1800: train loss 1.8414, val loss 1.8828\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 1900: train loss 1.8168, val loss 1.8517\n",
      "step 2000: train loss 1.7690, val loss 1.8232\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 2100: train loss 1.7509, val loss 1.8257\n",
      "step 2200: train loss 1.7087, val loss 1.7504\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 2300: train loss 1.7218, val loss 1.7632\n",
      "step 2400: train loss 1.6782, val loss 1.7225\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 2500: train loss 1.6614, val loss 1.7127\n",
      "step 2600: train loss 1.6286, val loss 1.6937\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 2700: train loss 1.6221, val loss 1.6750\n",
      "step 2800: train loss 1.6260, val loss 1.6742\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 2900: train loss 1.5980, val loss 1.6577\n",
      "step 3000: train loss 1.5764, val loss 1.6584\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 3100: train loss 1.5655, val loss 1.6457\n",
      "step 3200: train loss 1.5461, val loss 1.6275\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 3300: train loss 1.6254, val loss 1.7475\n",
      "step 3400: train loss 1.5406, val loss 1.6201\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 3500: train loss 1.5331, val loss 1.6138\n",
      "step 3600: train loss 1.4994, val loss 1.5927\n",
      "   Making demo... ...Finished demo\n",
      "   New best val loss: Saving checkpoint to fullnotes_jsb_best_cp.pt\n",
      "step 3700: train loss 1.5305, val loss 1.6218\n",
      "step 3800: train loss 1.4914, val loss 1.6001\n",
      "   Making demo... ...Finished demo\n",
      "step 3900: train loss 1.4835, val loss 1.5931\n",
      "step 4000: train loss 1.5812, val loss 1.6362\n",
      "   Making demo... ...Finished demo\n"
     ]
    }
   ],
   "source": [
    "demo_every = eval_interval*2\n",
    "cp_every = eval_interval*2\n",
    "prompt_length = 24\n",
    "val_int = 4*((n+1)//4) # try to get at the start of 4 notes in a chord\n",
    "context = data[val_int:val_int+prompt_length].unsqueeze(0).to(device) # same starting point from somewhere in validation set\n",
    "best_loss = 999\n",
    "\n",
    "print(\"Starting training loop...\")\n",
    "for iter in range(max_iters):\n",
    "    wbl_dict = {} # wandb log dict\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if (iter % eval_interval == 0 or iter == max_iters - 1) and iter>0:\n",
    "        #print(\"      Estimating loss\")\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        wbl_dict = wbl_dict | losses  | {'step':iter//eval_interval} \n",
    "\n",
    "    if use_wandb and iter % demo_every == 0 and iter>0:\n",
    "        print(\"   Making demo...\",end=\"\",flush=True)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            new_notes = decode( model.generate(context, max_new_tokens=64, temperature=1)[0].cpu() )\n",
    "            p2 = midiplayer(new_notes, time_rescale=1)\n",
    "            wbl_dict = wbl_dict | {'player':wandb.Html(p2.html)}\n",
    "        model.train()\n",
    "        print(\" ...Finished demo\")\n",
    "        \n",
    "    if use_wandb and wbl_dict != {}: wandb.log(wbl_dict)\n",
    "    \n",
    "    if iter % cp_every==0 and iter>0: \n",
    "        if losses['val'] < best_loss:\n",
    "            print(\"   New best val loss: \",end=\"\",flush=True)\n",
    "            best_loss = losses['val']\n",
    "            save_checkpoint(None, model, optimizer,loss, f\"fullnotes_jsb_best_cp\")   \n",
    "        \n",
    "    \n",
    "    xb, yb = get_batch('train') # sample a batch of data\n",
    "    logits, loss = model(xb, yb) # evaluate the loss\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_checkpoint(max_iters, model, optimizer,loss, f\"fullnotees_jsb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_wandb: wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMVARJ7_58_U"
   },
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best model from our run \n",
    "step, model, optimizer, loss = load_checkpoint('fullnotes_jsb_best_cp.pt', model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefining model.generate  for customization\n",
    "from tqdm.notebook import tqdm as pbar\n",
    "\n",
    "@torch.no_grad()\n",
    "def generator(model, idx, max_new_tokens, temperature=1.0):\n",
    "    # idx is (B, T, CBS) array of indices in the current context\n",
    "    for _ in pbar(range(max_new_tokens), leave=False):\n",
    "        # crop idx to the last block_size tokens\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        # get the predictions\n",
    "        logits_list, loss = model(idx_cond)\n",
    "        idx_next_list = []\n",
    "        for cb in range(idx_cond.shape[-1]):\n",
    "            # focus only on the last time step\n",
    "            logits = logits_list[cb][:, -1, :] # becomes (B, V)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits/temperature, dim=-1) # (B, V)\n",
    "            # sample from the distribution\n",
    "            idx_next_list.append(torch.multinomial(probs, num_samples=1)) # (B, 1)\n",
    "        idx_next = torch.tensor(idx_next_list).unsqueeze(0).unsqueeze(0).to(idx.device)\n",
    "        # append sampled index to the running sequence\n",
    "        \n",
    "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind, num_tokens = 0, 150\n",
    "original = val_data[ind:ind+num_tokens].to(device) \n",
    "print(\"Original:\")\n",
    "display(midiplayer(decode(original), time_rescale=1.1))\n",
    "\n",
    "prompt_len = 21\n",
    "prompt = original[0:prompt_len] # grab from validation dataset\n",
    "print(\"prompt.shape =\",prompt.shape)\n",
    "print(\"Prompt:\")\n",
    "midiplayer(decode(prompt), time_rescale=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = num_tokens - prompt_len\n",
    "\n",
    "for temperature in [0.7, 0.7, 0.85, 0.92, 1.0, 1.2, 1.5]:\n",
    "    set_seeds(1337) # same temp for same seed will yield same output\n",
    "    notes = decode( generator(model, prompt.unsqueeze(0), max_new_tokens=new_tokens, temperature=temperature)[0].cpu() )\n",
    "    print(f\"temperature = {temperature}:\")\n",
    "    display(midiplayer(notes, time_rescale=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

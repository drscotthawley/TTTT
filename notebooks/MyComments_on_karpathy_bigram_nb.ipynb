{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT5iM-Dq6J1A"
   },
   "source": [
    "Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtPj8fGF6I2V",
    "outputId": "81443c6d-b3c5-43a2-f60f-d43d4f418b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-07 12:17:53--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2023-11-07 12:17:53 (27.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdrscotthawley\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "use_wandb = True\n",
    "if use_wandb:\n",
    "    import wandb\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "y06bDCiX6R0C"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "batch_size = 128 # how many independent sequences will we process in parallel?\n",
    "block_size = 64 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 128\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "dropout = 0.1\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n"
     ]
    }
   ],
   "source": [
    "print('device =',device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QlkM3xo6WhL"
   },
   "source": [
    "Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaAMdat36Vj4",
    "outputId": "3e0c13ca-4a17-48e3-dcd0-58eda669cd36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape, val_data.shape = torch.Size([1003854]) torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(\"train_data.shape, val_data.shape =\",train_data.shape, val_data.shape)\n",
    "\n",
    "# data loading\n",
    "def get_batch(split, debug=False):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    if debug: print(f\"get_batch: x.shape = {x.shape}, y.shape = {y.shape}\")\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9PfGh_57IY0",
    "outputId": "814b53f0-d196-455c-9ff5-cef74e1b8a8a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_batch: x.shape = torch.Size([128, 64]), y.shape = torch.Size([128, 64])\n",
      "B, T = 128, 64\n"
     ]
    }
   ],
   "source": [
    "x,y = get_batch('train',debug=True)\n",
    "print(f\"B, T = {batch_size}, {block_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Kr70iKZ7pPd"
   },
   "source": [
    "x is a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBwJTuc17kxe",
    "outputId": "d0e6b84d-5ef6-471f-ff39-b11a29f80ccf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50, 43,  8,  1, 32, 46, 47, 52, 49,  1, 61, 47, 58, 46,  1, 58, 46, 63,\n",
       "        57, 43, 50, 44,  0, 20, 53, 61,  1, 51, 53, 56, 43,  1, 59, 52, 44, 53,\n",
       "        56, 58, 59, 52, 39, 58, 43,  1, 58, 46, 39, 52,  1, 39, 50, 50,  1, 50,\n",
       "        47, 60, 47, 52, 45,  1, 61, 53, 51, 43], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WP9U8aIm7qh3"
   },
   "source": [
    "y is x shifted back by one and including new data.\n",
    "in this sense only y[:,-1] is the \"next token\" being predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Wjb4QMS7lub",
    "outputId": "edb23f78-ccac-4d36-f6dc-2ed41e52444e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([43,  8,  1, 32, 46, 47, 52, 49,  1, 61, 47, 58, 46,  1, 58, 46, 63, 57,\n",
       "        43, 50, 44,  0, 20, 53, 61,  1, 51, 53, 56, 43,  1, 59, 52, 44, 53, 56,\n",
       "        58, 59, 52, 39, 58, 43,  1, 58, 46, 39, 52,  1, 39, 50, 50,  1, 50, 47,\n",
       "        60, 47, 52, 45,  1, 61, 53, 51, 43, 52], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CO7ZJhu-6dvk"
   },
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "zOeDesr4348z"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            #print(\"logits.shape =\",logits.shape,\", targets.shape =\",targets.shape)\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSUO8GJx6tpw"
   },
   "source": [
    "Instantiate and get ready to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3yYAD0t6tOK",
    "outputId": "8b8a7edb-80c2-4efc-9f48-86ee16f0809e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.212481 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shawley/gpt-midi/wandb/run-20231107_122538-d50qdjkh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/drscotthawley/karpathy-gpt-mini/runs/d50qdjkh' target=\"_blank\">hardy-snow-3</a></strong> to <a href='https://wandb.ai/drscotthawley/karpathy-gpt-mini' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/drscotthawley/karpathy-gpt-mini' target=\"_blank\">https://wandb.ai/drscotthawley/karpathy-gpt-mini</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/drscotthawley/karpathy-gpt-mini/runs/d50qdjkh' target=\"_blank\">https://wandb.ai/drscotthawley/karpathy-gpt-mini/runs/d50qdjkh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if use_wandb: wandb.init(project='karpathy-gpt-mini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mO4XTmlj58La"
   },
   "source": [
    "Do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YCWAxp3I57eO",
    "outputId": "3e053b8a-798c-4198-fcb5-4268afa009f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2835, val loss 4.2866\n",
      "step 100: train loss 2.4290, val loss 2.4453\n",
      "step 200: train loss 2.2344, val loss 2.2527\n",
      "step 300: train loss 2.0351, val loss 2.0877\n",
      "step 400: train loss 1.8951, val loss 1.9849\n",
      "step 500: train loss 1.7932, val loss 1.9147\n",
      "step 600: train loss 1.7124, val loss 1.8540\n",
      "step 700: train loss 1.6518, val loss 1.8159\n",
      "step 800: train loss 1.6091, val loss 1.7699\n",
      "step 900: train loss 1.5732, val loss 1.7387\n",
      "step 1000: train loss 1.5428, val loss 1.7221\n",
      "step 1100: train loss 1.5169, val loss 1.7061\n",
      "step 1200: train loss 1.4951, val loss 1.6863\n",
      "step 1300: train loss 1.4776, val loss 1.6657\n",
      "step 1400: train loss 1.4606, val loss 1.6538\n",
      "step 1500: train loss 1.4471, val loss 1.6528\n",
      "step 1600: train loss 1.4377, val loss 1.6367\n",
      "step 1700: train loss 1.4197, val loss 1.6167\n",
      "step 1800: train loss 1.4082, val loss 1.6129\n",
      "step 1900: train loss 1.3982, val loss 1.6041\n",
      "step 2000: train loss 1.3905, val loss 1.5988\n",
      "step 2100: train loss 1.3857, val loss 1.6008\n",
      "step 2200: train loss 1.3717, val loss 1.5881\n",
      "step 2300: train loss 1.3658, val loss 1.5805\n",
      "step 2400: train loss 1.3585, val loss 1.5690\n",
      "step 2500: train loss 1.3547, val loss 1.5626\n",
      "step 2600: train loss 1.3458, val loss 1.5752\n",
      "step 2700: train loss 1.3413, val loss 1.5709\n",
      "step 2800: train loss 1.3409, val loss 1.5661\n",
      "step 2900: train loss 1.3317, val loss 1.5659\n",
      "step 3000: train loss 1.3278, val loss 1.5509\n",
      "step 3100: train loss 1.3210, val loss 1.5594\n",
      "step 3200: train loss 1.3150, val loss 1.5524\n",
      "step 3300: train loss 1.3125, val loss 1.5454\n",
      "step 3400: train loss 1.3104, val loss 1.5459\n",
      "step 3500: train loss 1.3052, val loss 1.5463\n",
      "step 3600: train loss 1.3015, val loss 1.5374\n",
      "step 3700: train loss 1.2988, val loss 1.5426\n",
      "step 3800: train loss 1.2922, val loss 1.5421\n",
      "step 3900: train loss 1.2908, val loss 1.5343\n",
      "step 4000: train loss 1.2900, val loss 1.5241\n",
      "step 4100: train loss 1.2846, val loss 1.5413\n",
      "step 4200: train loss 1.2822, val loss 1.5315\n",
      "step 4300: train loss 1.2797, val loss 1.5370\n",
      "step 4400: train loss 1.2749, val loss 1.5306\n",
      "step 4500: train loss 1.2711, val loss 1.5266\n",
      "step 4600: train loss 1.2702, val loss 1.5199\n",
      "step 4700: train loss 1.2658, val loss 1.5251\n",
      "step 4800: train loss 1.2623, val loss 1.5303\n",
      "step 4900: train loss 1.2641, val loss 1.5274\n",
      "step 4999: train loss 1.2580, val loss 1.5225\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if use_wandb: wandb.log(losses | {'step':iter//eval_interval})\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171dbcc768504219ab9640b2e61392e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train</td><td>█▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val</td><td>█▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>49</td></tr><tr><td>train</td><td>1.25797</td></tr><tr><td>val</td><td>1.52246</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hardy-snow-3</strong> at: <a href='https://wandb.ai/drscotthawley/karpathy-gpt-mini/runs/d50qdjkh' target=\"_blank\">https://wandb.ai/drscotthawley/karpathy-gpt-mini/runs/d50qdjkh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231107_122538-d50qdjkh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if use_wandb: wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMVARJ7_58_U"
   },
   "source": [
    "Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TwLRHwwh54ez",
    "outputId": "87615cfa-8d7d-4d19-e9b5-a92ac5f79cb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JULIET:\n",
      "Some eyes, she is mine alone excemember.\n",
      "\n",
      "GREMIO:\n",
      "A troth. This in Bolingham, they are they committed,\n",
      "Is sinced but that news of bloody?\n",
      "\n",
      "PAGE:\n",
      "You think it is made, more, my life grood night be an hundred\n",
      "your enemies, married and faults are far;\n",
      "The Lord on, for hour hence string crafts,\n",
      "And ill-wander sufficious 'sharden\n",
      "That war homselfhootables, that not infirmation,\n",
      "Opefition from them to know. This father yields his death?\n",
      "Come, be have the glast douth it write,\n",
      "And yet that be men's as\n",
      "unwilly that golden king, and begin;\n",
      "Sir lought believe thee as here is but not the king,\n",
      "well set against you to Clifford's castle,\n",
      "But so of your kingren as find, shall us it.\n",
      "\n",
      "MENENIUS:\n",
      "I will not do.\n",
      "\n",
      "ANGELO:\n",
      "I wot have sortly belief; thou art thy about this,\n",
      "Take my admis-swort, they are upon my badd,\n",
      "And that's all eather wan like to my lodge,\n",
      "Blost which you were send that I parrs too cause.\n",
      "\n",
      "CAPULETESTER:\n",
      "As snacrentled luckshing point, abow!\n",
      "\n",
      "CORIOLANUS:\n",
      "So Citizen:\n",
      "I will go to you: not do Saught alriage my ears:\n",
      "I'll not shall be the sension will be in hear\n",
      "Worships a spirit mell at all, what like concer,\n",
      "It severbs him hence to lod, my son-peace.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "And in my tribes; that's not I'll kneeless be again,\n",
      "Therefore fiery themselves this only paw,\n",
      "Receitanted, nights have will not murder in the back.\n",
      "\n",
      "JOHN ONTUS:\n",
      "I linds go: Barnart by thine to be go:\n",
      "O, I peace must be runly father than my busing\n",
      "Worthy hair you, they will speak a child,\n",
      "Your Roman,--but God's sentence coure; no more solex\n",
      "Of good up sheeus onest.\n",
      "\n",
      "First Senators.\n",
      "\n",
      "ESCALUS:\n",
      "What, madam, your honour to service the stood,\n",
      "Well the country dukes much as the pleasure\n",
      "Is said with his majesty, nor sleep-half tooth,\n",
      "Musicians to be even: it is notor my oates,\n",
      "We'll present the loving: styrn not I am for any\n",
      "To Bastista's.\n",
      "A Princle hold, and kindusing! I will conted in\n",
      "I hear brutut him\n",
      "it, trainess 'twas but feast.\n",
      "\n",
      "SOMERD:\n",
      "And that you hither but the rest, and do remain\n",
      "The ironly ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
